
.ad l

.TH lsb.queues 5 "July 2021" "" ""
.ll 72

.ce 1000
\fBlsb.queues \fR
.ce 0

.sp 2
The lsb.queues file defines batch queues. Numerous controls are
available at the queue level to allow cluster administrators to
customize site policies.
.sp 2

.sp 2
This file is optional. If no queues are configured, LSF creates a
queue that is named \fRdefault\fR, with all parameters set to
default values.
.sp 2
This file is installed by default in
LSB_CONFDIR/\fIcluster_name\fR/configdir.
.SH Changing lsb.queues configuration

.sp 2
After you change lsb.queues, run badmin reconfig to reconfigure
mbatchd.
.sp 2
Some parameters, such as run window and runtime limit, do not
take effect immediately for running jobs unless you run mbatchd
restart or sbatchd restart on the job execution host.
.SH lsb.queues structure

.sp 2
Each queue definition begins with the line Begin Queue and ends
with the line End Queue. The queue name must be specified; all
other parameters are optional.
.sp 2
Parent topic: Configuration files
.sp 2

.ce 1000
\fB#INCLUDE\fR
.ce 0

.sp 2

.SH Syntax

.sp 2
\fR#INCLUDE\fR \fR"path-to-file"\fR
.SH Description

.sp 2
Inserts a configuration setting from another file to the current
location. Use this directive to dedicate control of a portion of
the configuration to other users or user groups by providing
write access for the included file to specific users or user
groups, and to ensure consistency of configuration file settings
in different clusters (if you are using the LSF multicluster
capability).
.sp 2
See more information on shared configuration file content in
Administering IBM Spectrum LSF
.sp 2
\fR#INCLUDE\fR can be inserted anywhere in the local
configuration file.
.SH Default

.sp 2
Not defined.
.sp 2

.ce 1000
\fBADMINISTRATORS\fR
.ce 0

.sp 2
Specifies a space-separated list of queue administrators. Queue
administrators can operate on any user job in the queue, and on
the queue itself.
.sp 2

.SH Syntax

.sp 2
\fBADMINISTRATORS=\fRuser_name | user_group ...
.SH Description

.sp 2
To specify a Windows user account or user group, include the
domain name in uppercase letters
(\fIDOMAIN_NAME\fR\\\fIuser_name\fR or
\fIDOMAIN_NAME\fR\\\fIuser_group\fR).
.SH Default

.sp 2
Not defined. You must be a cluster administrator to operate on
this queue.
.sp 2

.ce 1000
\fBAPS_PRIORITY\fR
.ce 0

.sp 2
Specifies calculation factors for absolute priority scheduling
(APS). Pending jobs in the queue are ordered according to the
calculated APS value.
.sp 2

.SH Syntax

.sp 2
\fRAPS_PRIORITY=WEIGHT[\fR[\fIfactor\fR\fR, \fR\fIvalue\fR]
[\fIsubfactor\fR\fR, \fR\fIvalue\fR]...]...\fR]
LIMIT[\fR[\fIfactor\fR\fR, \fR\fIvalue\fR] [\fIsubfactor\fR\fR,
\fR\fIvalue\fR]...]...\fR] GRACE_PERIOD[\fR[\fIfactor\fR\fR,
\fR\fIvalue\fR] [\fIsubfactor\fR\fR,
\fR\fIvalue\fR]...]...\fR]\fR
.SH Description

.sp 2
If weight of a subfactor is defined, but the weight of parent
factor is not defined, the parent factor weight is set as 1.
.sp 2
The WEIGHT and LIMIT factors are floating-point values. Specify a
\fIvalue\fR for GRACE_PERIOD in seconds (\fIvalue\fRs), minutes
(\fIvalue\fRm), or hours (\fIvalue\fRh).
.sp 2
The default unit for grace period is hours.
.sp 2
The following are the names of the factors and subfactors to
specify:
.sp 2

.sp 2
+--------------------+--------------------+--------------------+
| Factors            | Subfactors         | Metric             |
+--------------------+--------------------+--------------------+
| FS (user-based     | The existing       | The fairshare      |
| fairshare factor)  | fairshare feature  | factor             |
|                    | tunes the dynamic  | automatically      |
|                    | user priority      | adjusts the APS    |
|                    |                    | value based on     |
|                    |                    | dynamic user       |
|                    |                    | priority.          |
|                    |                    | The FAIRSHARE      |
|                    |                    | parameter must be  |
|                    |                    | defined in the     |
|                    |                    | queue. The FS      |
|                    |                    | factor is ignored  |
|                    |                    | for non-fairshare  |
|                    |                    | queues.            |
|                    |                    | The FS factor is   |
|                    |                    | influenced by the  |
|                    |                    | following          |
|                    |                    | fairshare          |
|                    |                    | parameters that    |
|                    |                    | are defined in the |
|                    |                    | lsb.queues or      |
|                    |                    | lsb.params file:   |
|                    |                    |  * CPU_TIME_FACTOR |
|                    |                    |  * FWD_JOB_FACTOR  |
|                    |                    |  * RUN_TIME_FACTOR |
|                    |                    |  * RUN_JOB_FACTOR  |
|                    |                    |  * HIST_HOURS      |
+--------------------+--------------------+--------------------+
| RSRC (resource     | PROC               | Requested tasks    |
| factors)           |                    | are the maximum of |
|                    |                    | bsub -n min_task , |
|                    |                    | max_task , the min |
|                    |                    | of bsub -n min ,   |
|                    |                    | or the value of    |
|                    |                    | the TASKLIMIT      |
|                    |                    | parameter in the   |
|                    |                    | lsb.queues file.   |
|                    |--------------------|--------------------|
|                    | MEM                | Total real memory  |
|                    |                    | requested (in MB   |
|                    |                    | or in units set in |
|                    |                    | the                |
|                    |                    | LSF_UNIT_FOR_LIMIT |
|                    |                    | S parameter in the |
|                    |                    | lsf.conf file).    |
|                    |                    | Memory requests    |
|                    |                    | appearing to the   |
|                    |                    | right of a ||      |
|                    |                    | symbol in a usage  |
|                    |                    | string are ignored |
|                    |                    | in the APS         |
|                    |                    | calculation.       |
|                    |                    | For multi-phase    |
|                    |                    | memory             |
|                    |                    | reservation, the   |
|                    |                    | APS value is based |
|                    |                    | on the first phase |
|                    |                    | of reserved        |
|                    |                    | memory.            |
|                    |--------------------|--------------------|
|                    | SWAP               | Total swap space   |
|                    |                    | requested (in MB   |
|                    |                    | or in units set in |
|                    |                    | the                |
|                    |                    | LSF_UNIT_FOR_LIMIT |
|                    |                    | S parameter in the |
|                    |                    | lsf.conf file).    |
|                    |                    | As with MEM , swap |
|                    |                    | space requests     |
|                    |                    | appearing to the   |
|                    |                    | right of a ||      |
|                    |                    | symbol in a usage  |
|                    |                    | string are         |
|                    |                    | ignored.           |
+--------------------+--------------------+--------------------+
| WORK (job          | JPRIORITY          | The job priority   |
| attributes)        |                    | that is specified  |
|                    |                    | by:                |
|                    |                    |  * Default that is |
|                    |                    | specified by half  |
|                    |                    | of the value of    |
|                    |                    | the                |
|                    |                    | MAX_USER_PRIORITY  |
|                    |                    | parameter in the   |
|                    |                    | lsb.params file    |
|                    |                    |  * Users with bsub |
|                    |                    | -sp or bmod -sp    |
|                    |                    |  * Automatic       |
|                    |                    | priority           |
|                    |                    | escalation with    |
|                    |                    | the                |
|                    |                    | JOB_PRIORITY_OVER_ |
|                    |                    | TIME parameter in  |
|                    |                    | the lsb.params     |
|                    |                    | file               |
|                    |                    | If the             |
|                    |                    | TRACK_ELIGIBLE_PEN |
|                    |                    | DINFO parameter in |
|                    |                    | the lsb.params     |
|                    |                    | file is set to Y   |
|                    |                    | or y, LSF          |
|                    |                    | increases the job  |
|                    |                    | priority for       |
|                    |                    | pending jobs as    |
|                    |                    | long as it is      |
|                    |                    | eligible for       |
|                    |                    | scheduling. LSF    |
|                    |                    | does not increase  |
|                    |                    | the job priority   |
|                    |                    | for ineligible     |
|                    |                    | pending jobs.      |
|                    |--------------------|--------------------|
|                    | QPRIORITY          | The priority of    |
|                    |                    | the submission     |
|                    |                    | queue.             |
+--------------------+--------------------+--------------------+
| APP                |                    | Set the priority   |
|                    |                    | factor at the      |
|                    |                    | application        |
|                    |                    | profile level by   |
|                    |                    | specifying the     |
|                    |                    | PRIORITY parameter |
|                    |                    | in the             |
|                    |                    | lsb.applications   |
|                    |                    | file. The          |
|                    |                    | APP_PRIORITY       |
|                    |                    | factor is added to |
|                    |                    | the calculated APS |
|                    |                    | value to change    |
|                    |                    | the factor value.  |
|                    |                    | The APP_PRIORITY   |
|                    |                    | factor applies to  |
|                    |                    | the entire job.    |
+--------------------+--------------------+--------------------+
| USER               |                    | Set the priority   |
|                    |                    | factor for users   |
|                    |                    | by specifying the  |
|                    |                    | PRIORITY parameter |
|                    |                    | in the User        |
|                    |                    | section of the     |
|                    |                    | lsb.users file.    |
|                    |                    | The USER_PRIORITY  |
|                    |                    | factor is added to |
|                    |                    | the calculated APS |
|                    |                    | value to change    |
|                    |                    | the factor value.  |
|                    |                    | The USER_PRIORITY  |
|                    |                    | factor applies to  |
|                    |                    | the entire job.    |
+--------------------+--------------------+--------------------+
| UG                 |                    | Set the priority   |
|                    |                    | factor for user    |
|                    |                    | groups by          |
|                    |                    | specifying the     |
|                    |                    | PRIORITY parameter |
|                    |                    | in the UserGroup   |
|                    |                    | section of the     |
|                    |                    | lsb.users file.    |
|                    |                    | The UG_PRIORITY    |
|                    |                    | factor is added to |
|                    |                    | the calculated APS |
|                    |                    | value to change    |
|                    |                    | the factor value.  |
|                    |                    | The UG_PRIORITY    |
|                    |                    | factor applies to  |
|                    |                    | the entire job.    |
|                    |                    | LSF uses the       |
|                    |                    | priority of the    |
|                    |                    | user group as      |
|                    |                    | specified in the   |
|                    |                    | bsub -G option.    |
+--------------------+--------------------+--------------------+
| ADMIN              |                    | Administrators use |
|                    |                    | bmod -aps to set   |
|                    |                    | this subfactor     |
|                    |                    | value for each     |
|                    |                    | job. A positive    |
|                    |                    | value increases    |
|                    |                    | the APS. A         |
|                    |                    | negative value     |
|                    |                    | decreases the APS. |
|                    |                    | The ADMIN factor   |
|                    |                    | is added to the    |
|                    |                    | calculated APS     |
|                    |                    | value to change    |
|                    |                    | the factor value.  |
|                    |                    | The ADMIN factor   |
|                    |                    | applies to the     |
|                    |                    | entire job. You    |
|                    |                    | cannot configure   |
|                    |                    | separate weight,   |
|                    |                    | limit, or grace    |
|                    |                    | period factors.    |
|                    |                    | The ADMIN factor   |
|                    |                    | takes effect as    |
|                    |                    | soon as it is set. |
+--------------------+--------------------+--------------------+
.sp 2
For example, the following sets a grace period of 10 hours for
the \fRMEM\fR factor, 10 minutes for the \fRJPRIORITY\fR factor,
10 seconds for the \fRQPRIORITY\fR factor, and 10 hours (default)
for the \fRRSRC\fR factor:
.sp 2
GRACE_PERIOD[[MEM,10h] [JPRIORITY, 10m] [QPRIORITY,10s] [RSRC, 10]]
.br

.sp 2
You cannot specify \fR0\fR (zero) for the \fRWEIGHT\fR,
\fRLIMIT\fR, and \fRGRACE_PERIOD\fR of any factor or subfactor.
.sp 2
APS queues cannot configure cross-queue fairshare
(\fBFAIRSHARE_QUEUES\fR). The \fBQUEUE_GROUP\fR parameter
replaces FAIRSHARE_QUEUES, which is obsolete in LSF 7.0.
.sp 2
Suspended (bstop) jobs and migrated jobs (bmig) are always
scheduled before pending jobs. For migrated jobs, LSF keeps the
existing job priority information.
.sp 2
If LSB_REQUEUE_TO_BOTTOM and LSB_MIG2PEND are configured in
lsf.conf, the migrated jobs keep their APS information. When
LSB_REQUEUE_TO_BOTTOM and LSB_MIG2PEND are configured, the
migrated jobs need to compete with other pending jobs based on
the APS value. To reset the APS value, use brequeue, not bmig.
.SH Default

.sp 2
Not defined
.sp 2

.ce 1000
\fBBACKFILL\fR
.ce 0

.sp 2
Enables backfill scheduling for the queue.
.sp 2

.SH Syntax

.sp 2
\fRBACKFILL=Y\fR | \fRN\fR
.SH Description

.sp 2
Set this parameter as Y to enable backfill scheduling for the
queue.
.sp 2
A possible conflict exists if \fBBACKFILL\fR and \fBPREEMPTION\fR
are specified together. If \fBPREEMPT_JOBTYPE\fR = \fBBACKFILL\fR
is set in the lsb.params file, a backfill queue can be
preemptable. Otherwise, a backfill queue cannot be preemptable.
If \fBBACKFILL\fR is enabled, do not also specify
\fBPREEMPTION\fR = \fBPREEMPTABLE\fR.
.sp 2
\fBBACKFILL\fR is required for interruptible backfill queues
(\fBINTERRUPTIBLE_BACKFILL\fR=\fIseconds\fR).
.sp 2
When \fBMAX_SLOTS_IN_POOL\fR, \fBSLOT_RESERVE\fR, and
\fBBACKFILL\fR are defined for the same queue, jobs in the queue
cannot backfill with slots that are reserved by other jobs in the
same queue.
.SH Default

.sp 2
Not defined. No backfilling.
.sp 2

.ce 1000
\fBCHKPNT\fR
.ce 0

.sp 2
Enables automatic checkpointing for the queue. All jobs that are
submitted to the queue are checkpointable.
.sp 2

.SH Syntax

.sp 2
\fRCHKPNT=\fR\fIchkpnt_dir \fR[\fIchkpnt_period\fR]
.SH Description

.sp 2
The checkpoint directory is the directory where the checkpoint
files are created. Specify an absolute path or a path relative to
CWD, do not use environment variables.
.sp 2
Specify the optional checkpoint period in minutes.
.sp 2
You can checkpoint only running members of a chunk job.
.sp 2
If checkpoint-related configuration is specified in both the
queue and an application profile, the application profile setting
overrides queue level configuration.
.sp 2
Checkpoint-related configuration that is specified in the queue,
application profile, and at job level has the following effect:
.sp 2
*  Application-level and job-level parameters are merged. If the
   same parameter is defined at both job-level and in the
   application profile, the job-level value overrides the
   application profile value.
.sp 2
*  The merged result of job-level and application profile
   settings override queue-level configuration.
.sp 2
To enable checkpointing of MultiCluster jobs, define a checkpoint
directory in both the send-jobs and receive-jobs queues (CHKPNT
in lsb.queues), or in an application profile (CHKPNT_DIR,
CHKPNT_PERIOD, CHKPNT_INITPERIOD, CHKPNT_METHOD in
lsb.applications) of both submission cluster and execution
cluster. LSF uses the directory that is specified in the
execution cluster.
.sp 2
To make a MultiCluster job checkpointable, both submission and
execution queues must enable checkpointing, and the application
profile or queue setting on the execution cluster determines the
checkpoint directory. Checkpointing is not supported if a job
runs on a leased host.
.sp 2
The file path of the checkpoint directory can contain up to 4000
characters for UNIX and Linux, or up to 255 characters for
Windows, including the directory and file name.
.SH Default

.sp 2
Not defined
.sp 2

.ce 1000
\fBCHUNK_JOB_SIZE\fR
.ce 0

.sp 2
Enables job chunking and specifies the maximum number of jobs
that are allowed to be dispatched together in a chunk.
.sp 2

.SH Syntax

.sp 2
\fRCHUNK_JOB_SIZE=\fR\fIinteger\fR
.SH Description

.sp 2
\fBNote: \fRThis parameter is deprecated and might be removed in
a future version of LSF.
.sp 2
Specify a positive integer greater than 1.
.sp 2
The ideal candidates for job chunking are jobs that have the same
host and resource requirements and typically take 1 - 2 minutes
to run.
.sp 2
Job chunking can have the following advantages:
.sp 2
*  Reduces communication between sbatchd and mbatchd and reduces
   scheduling performance in mbschd.
.sp 2
*  Increases job throughput in mbatchd and CPU usage on the
   execution hosts.
.sp 2
However, throughput can deteriorate if the chunk job size is too
large. Performance might decrease on queues with CHUNK_JOB_SIZE
greater than 30. Evaluate the chunk job size on your own systems
for best performance.
.sp 2
With MultiCluster job forwarding model, this parameter does not
affect MultiCluster jobs that are forwarded to a remote cluster.
.SH Compatibility

.sp 2
This parameter is ignored in interactive queues
(\fBINTERACTIVE=ONLY\fR parameter).
.sp 2
*  Interactive (\fBINTERACTIVE=ONLY\fR parameter)
.sp 2
If \fBCHUNK_JOB_DURATION\fR is set in lsb.params, chunk jobs are
accepted as long as the CPULIMIT, RUNLIMIT, or \fBRUNTIME\fR
values do not exceed the \fBCHUNK_JOB_DURATION\fR.
.SH Example

.sp 2
The following example configures a queue that is named
\fRchunk\fR, which dispatches up to four jobs in a chunk:
.sp 2
Begin Queue
.br
QUEUE_NAME     = chunk 
.br
PRIORITY       = 50 
.br
CHUNK_JOB_SIZE = 4 
.br
End Queue
.br

.SH Default

.sp 2
Not defined
.sp 2

.ce 1000
\fBCOMMITTED_RUN_TIME_FACTOR\fR
.ce 0

.sp 2
Specifies the committed runtime weighting factor. Used only with
fairshare scheduling.
.sp 2

.SH Syntax

.sp 2
\fRCOMMITTED_RUN_TIME_FACTOR=\fR\fInumber\fR
.SH Description

.sp 2
In the calculation of a user dynamic priority, this factor
determines the relative importance of the committed run time in
the calculation. If the -W option of bsub is not specified at job
submission and a \fBRUNLIMIT\fR is not set for the queue, the
committed run time is not considered.
.sp 2
If undefined, the cluster-wide value from the lsb.params
parameter of the same name is used.
.SH Valid values

.sp 2
Any positive number between 0.0 and 1.0
.SH Default

.sp 2
Not defined.
.sp 2

.ce 1000
\fBCONTAINER\fR
.ce 0

.sp 2

.SH Syntax

.sp 2
\fRCONTAINER=docker[image(\fR\fIimage_name\fR\fR)
options(\fR\fIdocker_run_options\fR\fR)]\fR
.sp 2
\fRCONTAINER=nvidia-docker[image(\fR\fIimage_name\fR\fR)
options(\fR\fIdocker_run_options\fR\fR)]\fR
.sp 2
\fRCONTAINER=shifter[image(\fR\fIimage_name\fR\fR)
options(\fR\fIcontainer_options\fR\fR)]\fR
.sp 2
\fRCONTAINER=singularity[image(\fR\fIimage_name\fR\fR)
options(\fR\fIcontainer_options\fR\fR)]\fR
.sp 2
\fRCONTAINER=enroot[image(\fR\fIimage_name\fR\fR)
options(\fR\fIenroot_start_options\fR\fR)]\fR
.SH Description

.sp 2
Enables LSF to use a supported container for jobs that are
submitted to this queue. This parameter uses the following
keywords:
.sp 2
\fBdocker | nvidia-docker | shifter | singularity | enroot\fR
.br
         Required. Use one of these keywords to specify the type
         of container to use for jobs that are submitted to this
         queue. Use \fRdocker\fR if you are running Podman
         containers.
.sp 2
\fBimage\fR
.br
         Required. This keyword specifies the image name that is
         used in running jobs.
.sp 2
         For Docker, NVIDIA Docker, Podman, and Enroot jobs, use
         the \fB$LSB_CONTAINER_IMAGE\fR environment variable to
         allow users to specify the image name for the container
         jobs at job submission time. At job submission time,
         users can specify a specific image name that is in the
         specified repository server by specifying a value for
         the \fB$LSB_CONTAINER_IMAGE\fR environment variable.
.sp 2
\fBoptions\fR
.br
         Optional. This keyword specifies the job run options for
         the container.
.sp 2
         To enable a pre-execution script to run, specify an at
         sign (\fR@\fR) and a full file path to the script, which
         the execution host must be able to access. Before the
         container job runs, LSF runs this script with LSF
         administrator privileges. While the script is running,
         the jobs\(aq environment variables are passed to the
         script. When the script finishes running, the output is
         used as container startup options. The script must
         provide this output on one line. The method of
         processing the container job depends on the result of
         the pre-execution script:
.sp 2
         *  If the pre-execution script failed, the container job
            exits with the same exit code from the script. In
            addition, an external status message is sent to
            inform the user that the job exited because of script
            execution failure.
.sp 2
         *  If the execution of the script is successful but the
            output contains more than 512 options, LSF only keeps
            the first 512 options, and the remaining options are
            ignored.
.sp 2
         *  If the execution of the script is successful and the
            output is valid, the output is part of the container
            job running options. The position of the output from
            the script in the options is exactly where the user
            configured the script in the options field.
.sp 2
         For Docker and NVIDIA Docker containers, this keyword
         specifies the Docker job run options for the docker run
         command, which are passed to the job container.
.sp 2
         \fBNote: \fR
.sp 2
         *  Before you specify the Docker job run options, make
            sure that these options work with the docker run
            command in the command line.
.sp 2
         *  The --cgroup-parent and --user (or -u) options are
            reserved for LSF internal use. Do not use these
            options in the options keyword configuration,
            otherwise the job fails.
.sp 2
            If you specified a pre-execution script and the
            output of this script contains --cgroup-parent,
            --user, or -u, the container job also fails.
.sp 2
         *  The -w and --ulimit options are automatically set for
            LSF. Do not use these options in the options keyword
            configuration because these specifications override
            the LSF settings.
.sp 2
         *  The -v option is automatically used by LSF to mount
            the working directories that LSF requires, such as
            the current working directory, job spool directory,
            destination file for the bsub -f command, tmp
            directory, the LSF_TOP, and the checkpoint directory
            on demand.
.sp 2
         *  You can configure the --rm option in the options
            keyword configuration to automatically remove
            containers after the job is done.
.sp 2
         *  You can enable LSF to automatically assign a name to
            a Docker container when it creates the Docker
            container. To enable this feature, set the
            \fBENABLE_CONTAINER_NAME\fR parameter to \fBTrue\fR
            in the lsfdockerlib.py file.
.sp 2
            The container name uses the following naming
            convention:
.sp 2
            *  Normal jobs and blaunch parallel job containers:
               \fI<cluster_name>\fR.job.\fI<job_id>\fR
.sp 2
            *  Array jobs and array blaunch parallel job
               containers:
               \fI<cluster_name>\fR.job.\fI<job_id>\fR.\fI<job_index>\fR
.sp 2
            *  blaunch parallel job task containers:
               \fI<cluster_name>\fR.job.\fI<job_id>\fR.task.\fI<task_id>\fR
.sp 2
            *  Array blaunch parallel job task containers:
               \fI<cluster_name>\fR.job.\fI<job_id>\fR.\fI<job_index>\fR.task.\fI<task_id>\fR
.sp 2
         *  Limitation: If you use the -d option, LSF incorrectly
            gets the status of the Docker jobs as \fRDONE\fR.
.sp 2
         For Shifter containers, this keyword specifies the
         Shifter job run options for the shifter command, which
         are passed to the job container.
.sp 2
         \fBNote: \fR
.sp 2
         *  Run shifter --help in the command line to view the
            options that the shifter command supports.
.sp 2
         *  Before you specify the Shifter job run options, make
            sure that these options work with the shifter command
            in the command line.
.sp 2
         *  The \fB$LD_LIBRARY_PATH\fR directory is cleaned
            according to the setuid bit that Shifter uses to
            work. Therefore, for programs that depend on
            \fB$LD_LIBRARY_PATH\fR to work (such as openmpi),
            ensure that the setuid bit can be properly set inside
            the container by adding \fBLD_LIBRARY_PATH\fR to the
            \fBsiteEnvAppend\fR section of the udiRoot.conf file.
.sp 2
         For Singularity containers, this keyword specifies the
         Singularity job run options for the singularity exec
         command, which are passed to the job container.
.sp 2
         \fBNote: \fR
.sp 2
         *  Run singularity exec --help in the command line to
            view the options that the singularity command
            supports.
.sp 2
         *  Before you specify the Singularity job run options,
            make sure that these options work with the
            singularity exec command in the command line.
.sp 2
         *  The \fB$LD_LIBRARY_PATH\fR directory is cleaned
            according to the setuid bit that Singularity uses to
            work. Therefore, for programs that depend on
            \fB$LD_LIBRARY_PATH\fR to work (such as openmpi),
            ensure that the setuid bit can be properly set inside
            the container by adding \fBLD_LIBRARY_PATH\fR to the
            ld.so.conf file and run the ldconfig command.
.sp 2
         For Podman containers, this keyword specifies the Podman
         job run options for the podman run command, which are
         passed to the job container.
.sp 2
         \fBNote: \fR
.sp 2
         *  Before you specify the Podman job run options, make
            sure that these options work with the podman run
            command in the command line.
.sp 2
         *  The --user (or -u) option is reserved for LSF
            internal use. Do not use these options in the options
            keyword configuration, otherwise the job fails.
.sp 2
            If you specified a pre-execution script and the
            output of this script contains --user, or -u, the
            container job also fails.
.sp 2
         *  The -w and --ulimit options are automatically set for
            LSF. Do not use these options in the options keyword
            configuration because these specifications override
            the LSF settings.
.sp 2
         *  The -v option is automatically used by LSF to mount
            the working directories that LSF requires, such as
            the current working directory, job spool directory,
            destination file for the bsub -f command, tmp
            directory, the LSF_TOP, and the checkpoint directory
            on demand.
.sp 2
         *  You can configure the --rm option in the options
            keyword configuration to automatically remove
            containers after the job is done.
.sp 2
         *  Limitation: If you use the -d option, LSF incorrectly
            gets the status of the Docker jobs as \fRDONE\fR.
.sp 2
         For Enroot containers, this keyword specifies the Enroot
         job run options for the enroot start command, which are
         passed to the job container.
.sp 2
         \fBNote: \fRBefore you specify the Enroot job run
         options, make sure that these options work with the
         enroot start command in the command line.
.SH Examples

.sp 2
To specify an Ubuntu image for use with container jobs without
specifying any optional keywords,
.sp 2
Begin Queue
.br
NAME = dockerq
.br
CONTAINER = docker[image(repository.example.com:5000/file/path/ubuntu:latest)]
.br
DESCRIPTION = Docker User Service
.br
End Queue
.sp 2
Begin Queue
.br
NAME = ndockerq
.br
CONTAINER = nvidia-docker[image(repository.example.com:5000/file/path/ubuntu:latest)]
.br
DESCRIPTION = NVIDIA Docker User Service
.br
End Queue
.sp 2
Begin Queue
.br
NAME = shifterq
.br
CONTAINER = shifter[image(ubuntu:latest)]
.br
DESCRIPTION = Shifter User Service
.br
End Queue
.sp 2
Begin Queue
.br
NAME = singq
.br
CONTAINER = singularity[image(/file/path/ubuntu.img)]
.br
DESCRIPTION = Singularity User Service
.br
End Queue
.sp 2
Begin Queue
.br
NAME = podmanq
.br
CONTAINER = docker[image(repository.example.com:5000/file/path/ubuntu:latest)]
.br
DESCRIPTION = Podman User Service
.br
End Queue
.sp 2
Begin Queue
.br
NAME = enrootq
.br
CONTAINER = enroot[image(repository.example.com:5000/file/path/ubuntu:latest)]
.br
DESCRIPTION = Enroot User Service
.br
End Queue
.sp 2
To specify a pre-execution script in the /share/usr/ directory,
which generates the container startup options,
.sp 2
Begin Queue
.br
NAME = dockerqoptions
.br
CONTAINER = docker[image(repository.example.com:5000/file/path/ubuntu:latest) options(@/share/usr/docker-options.sh)]
.br
DESCRIPTION = Docker User Service with pre-execution script for options
.br
End Queue
.sp 2
Begin Queue
.br
NAME = ndockerqoptions
.br
CONTAINER = nvidia-docker[image(repository.example.com:5000/file/path/ubuntu:latest) options(@/share/usr/ndocker-options.sh)]
.br
DESCRIPTION = NVIDIA Docker User Service with pre-execution script for options
.br
End Queue
.sp 2
Begin Queue
.br
NAME = shifterqoptions
.br
CONTAINER = shifter[image(ubuntu:latest) options(@/share/usr/shifter-options.sh)]
.br
DESCRIPTION = Shifter User Service
.br
End Queue
.sp 2
Begin Queue
.br
NAME = singqoptions
.br
CONTAINER = singularity[image(/file/path/ubuntu.img) options(@/share/usr/sing-options.sh)]
.br
DESCRIPTION = Singularity User Service
.br
End Queue
.sp 2
Begin Queue
.br
NAME = podmanqoptions
.br
CONTAINER = docker[image(repository.example.com:5000/file/path/ubuntu:latest) options(@/share/usr/podman-options.sh)]
.br
DESCRIPTION = Podman User Service with pre-execution script for options
.br
End Queue
.sp 2
Begin Queue
.br
NAME = enrootqoptions
.br
CONTAINER = enroot[image(repository.example.com:5000/file/path/ubuntu:latest) options(@/share/usr/enroot-options.sh)]
.br
DESCRIPTION = Enroot User Service with pre-execution script for options
.br
End Queue
.sp 2
*  For sequential jobs, specify the following \fBCONTAINER\fR
   parameter value for LSF to automatically remove containers
   after the job is done:
.sp 2
   \fRCONTAINER=docker[image(image-name) options(--rm)]\fR
.sp 2
*  For parallel jobs, the network and IPC must work across
   containers to make blaunch work. The execution user ID and
   user name mapping file must be mounted to the container for
   blaunch authentication.
.sp 2
   Therefore, specify the following \fBCONTAINER\fR parameter
   value for LSF to configure the container IPC and network
   parameters so that blaunch can work across multiple
   containers, to configure the container password file for
   blaunch authentication, and automatically remove containers
   after the job is done:
.sp 2
   \fRCONTAINER=docker[image(image-name) options(--rm
   --network=host --ipc=host -v
   /path/to/my/passwd:/etc/passwd)]\fR
.sp 2
   The passwd file must be in the standard format for UNIX and
   Linux password files, such as the following format:
.sp 2
   user1:x:10001:10001:::
.br
   user2:x:10002:10002:::
.sp 2
*  To allow users to specify image names for Docker, NVIDIA
   Docker, Podman, and Enroot container jobs at job submission
   time, use the \fB$LSB_CONTAINER_IMAGE\fR environment variable
   as the image name when specifying the \fBimage\fR keyword.
.sp 2
   For example, when defining the \fBCONTAINER\fR parameter for
   the udockerGPU queue, add the \fB$LSB_CONTAINER_IMAGE\fR
   environment variable to the image specification:
.sp 2
   Begin Queue
.br
   NAME = udockerGPU
.br
   CONTAINER = docker[image(repository.example.com:5000/$LSB_CONTAINER_IMAGE) \
.br
               options(--rm --net=host --ipc=host  -v --runtime=nvidia /gpfs/u/fred:/data )]
.br
   DESCRIPTION = Docker User Service
.br
   End Queue
.sp 2
   Specify a container image name (such as \fRubuntu\fR) at the
   job submission time by setting the \fB$LSB_CONTAINER_IMAGE\fR
   environment using one of the following methods:
.sp 2
   *  Specify the \fB$LSB_CONTAINER_IMAGE\fR environment variable
      according to your shell environment:
.sp 2
      *  In csh or tcsh:
.sp 2
         \fRsetenv LSB_CONTAINER_IMAGE ubuntu\fR
.sp 2
      *  In sh, ksh, or bash:
.sp 2
         \fRexport LSB_CONTAINER_IMAGE=ubuntu\fR
.sp 2
   *  Use the bsub -env option:
.sp 2
      \fRbsub -env LSB_CONTAINER_IMAGE=ubuntu -q udocker a.out
      -in in.dat -out out.dat \fR
.sp 2
   *  Use an esub script to set the \fILSB_CONTAINER_IMAGE\fR
      environment variable, then call the esub with the bsub
      command.
.SH Default

.sp 2
Undefined
.sp 2

.ce 1000
\fBCORELIMIT\fR
.ce 0

.sp 2
Specifies the per-process core file size limit for all job
processes from this queue.
.sp 2

.SH Syntax

.sp 2
\fRCORELIMIT=\fR\fIinteger\fR
.SH Description

.sp 2
Specify this parameter to place a per-process hard core file size
limit, in KB, for all of the processes that belong to a job from
this queue (see getrlimit(2)).
.SH Default

.sp 2
Unlimited
.sp 2

.ce 1000
\fBCPU_FREQUENCY\fR
.ce 0

.sp 2
Specifies the CPU frequency for a queue.
.sp 2

.SH Syntax

.sp 2
\fBCPU_FREQUENCY=\fR[float_number][unit]
.SH Description

.sp 2
All jobs submit to the queue require the specified CPU frequency.
Value is a positive float number with units (GHz, MHz, or KHz).
If no units are set, the default is GHz.
.sp 2
You can also use bsub -freq to set this value.
.sp 2
The submission value overwrites the application profile value,
and the application profile value overwrites the queue value.
.SH Default

.sp 2
Not defined (Nominal CPU frequency is used)
.sp 2

.ce 1000
\fBCPULIMIT\fR
.ce 0

.sp 2
Specifies the maximum normalized CPU time and, optionally, the
default normalized CPU time that is allowed for all processes of
jobs that run in this queue. The name of a host or host model
specifies the CPU time normalization host to use.
.sp 2

.SH Syntax

.sp 2
\fRCPULIMIT=[\fR\fIdefault_limit\fR] \fImaximum_limit\fR
.sp 2
where \fIdefault_limit\fR and \fImaximum_limit\fR are defined by
the following formula:
.sp 2
[\fIhour\fR:]\fIminute\fR[/\fIhost_name\fR | /\fIhost_model\fR]
.SH Description

.sp 2
Limits the total CPU time the job can use. This parameter is
useful for preventing runaway jobs or jobs that use up too many
resources.
.sp 2
When the total CPU time for the whole job reaches the limit, a
SIGXCPU signal is sent to all processes that belong to the job.
If the job has no signal handler for SIGXCPU, the job is killed
immediately. If the SIGXCPU signal is handled, blocked, or
ignored by the application, then after the grace period expires,
LSF sends SIGINT, SIGTERM, and SIGKILL to the job to kill it.
.sp 2
If a job dynamically creates processes, the CPU time that is used
by these processes is accumulated over the life of the job.
.sp 2
Processes that exist for fewer than 30 seconds might be ignored.
.sp 2
By default, if a default CPU limit is specified, jobs submitted
to the queue without a job-level CPU limit are killed when the
default CPU limit is reached.
.sp 2
If you specify only one limit, it is the maximum, or hard, CPU
limit. If you specify two limits, the first one is the default,
or soft, CPU limit, and the second one is the maximum CPU limit.
.sp 2
If no host or host model is given with the CPU time, LSF uses the
default CPU time normalization host that is defined at the queue
level (DEFAULT_HOST_SPEC in lsb.queues) if it is configured.
Otherwise, the default CPU time normalization host that is
defined at the cluster level (DEFAULT_HOST_SPEC in lsb.params) is
used if it is configured. Otherwise, the host with the largest
CPU factor (the fastest host in the cluster) is used.
.sp 2
Because sbatchd periodically checks whether a CPU time limit was
exceeded, a Windows job that runs under a CPU time limit can
exceed that limit by up to SBD_SLEEP_TIME.
.sp 2
On UNIX systems, the CPU limit can be enforced by the operating
system at the process level.
.sp 2
You can define whether the CPU limit is a per-process limit that
is enforced by the OS or a per-job limit enforced by LSF with
LSB_JOB_CPULIMIT in lsf.conf.
.sp 2
Jobs that are submitted to a chunk job queue are not chunked if
CPULIMIT is greater than 30 minutes.
.SH Default

.sp 2
Unlimited
.sp 2

.ce 1000
\fBCPU_TIME_FACTOR\fR
.ce 0

.sp 2
Specifies the CPU time weighting factor. Used only with fairshare
scheduling.
.sp 2

.SH Syntax

.sp 2
\fRCPU_TIME_FACTOR=\fR\fInumber\fR
.SH Description

.sp 2
In the calculation of a user dynamic share priority, this factor
determines the relative importance of the cumulative CPU time
that is used by a user’s jobs.
.sp 2
If undefined, the cluster-wide value from the lsb.params
parameter of the same name is used.
.SH Default

.sp 2
0.7
.sp 2

.ce 1000
\fBCSM_REQ\fR
.ce 0

.sp 2
Specifies the required values for the CSM bsub job submission
options. These settings override job level CSM options and append
system level allocation flags to the job level allocation flags.
.sp 2

.SH Syntax

.sp 2
\fBCSM_REQ=\fR [\fBjsm=y\fR | \fBn\fR | \fBd\fR]
[\fB:step_cgroup=y\fR | \fBn\fR] [\fB:\fR [\fBcore_isolation 0\fR
| \fB1\fR | \fB2\fR | \fB3\fR | \fB4\fR | \fB5\fR | \fB6\fR]
[\fB:cn_mem=\fRmem_value] ] [\fB:alloc_flags "\fRflag1
[\fB\fRflag2 ...]\fB"\fR]
.SH Description

.sp 2
Use a colon (\fR:\fR) to separate multiple CSM job options. The
options can appear in any order and none are required. For the
alloc_flags keyword, specify an alphanumeric string of flags and
separate multiple flags with a space. The string cannot contain a
colon (\fR:\fR).
.SH Example

.sp 2
CSM_REQ=jsm=n:step_cgroup=y:core_isolation 3:cn_mem=1024
.SH Default

.sp 2
Not defined.
.sp 2

.ce 1000
\fBDATALIMIT\fR
.ce 0

.sp 2
Specifies the per-process data segment size limit for all job
processes from this queue.
.sp 2

.SH Syntax

.sp 2
\fRDATALIMIT=\fR[\fIdefault_limit\fR] \fImaximum_limit\fR
.SH Description

.sp 2
Set this parameter to place a per-process data segment size
limit, in KB, for all of the processes that belong to a job from
this queue (see getrlimit(2)).
.sp 2
By default, if a default data limit is specified, jobs submitted
to the queue without a job-level data limit are killed when the
default data limit is reached.
.sp 2
If you specify only one limit, it is the maximum, or hard, data
limit. If you specify two limits, the first one is the default,
or soft, data limit, and the second one is the maximum data
limit.
.SH Default

.sp 2
Unlimited
.sp 2

.ce 1000
\fBDATA_TRANSFER\fR
.ce 0

.sp 2
Configures the queue as a data transfer queue for LSF data
manager.
.sp 2

.SH Syntax

.sp 2
\fBDATA_TRANSFER=Y\fR | \fBN\fR
.SH Description

.sp 2
The \fBDATA_TRANSFER=Y\fR parameter enables a queue for data
transfer through LSF data manager.
.sp 2
Only one queue in a cluster can be a data transfer queue. Any
transfer jobs that are submitted by the data manager go to this
queue. If the lsf.datamanager file exists, then at least one
queue must define the \fBDATA_TRANSFER\fR parameter. If this
parameter is set, a corresponding lsf.datamanager file must
exist.
.sp 2
Regular jobs that are submitted to this queue through bsub are
rejected.
.sp 2
Use the bstop, bresume, and bkill commands to stop, resume, and
kill your own transfer jobs in a data transfer queue. LSF
administrators and queue administrators can additionally use the
btop and bbot commands to move transfer jobs in the queue. All
other commands on jobs in a data transfer queue are rejected. You
cannot use the bswitch command to switch jobs from other queues
to a data transfer queue.
.sp 2
If you change this parameter, LSF data manager transfer jobs that
were in the previous queue remain in that queue and are scheduled
and run as normal. The LSF data manager is notified of their
success or failure.
.sp 2
The following queue parameters cannot be used together with a
queue that defines the \fBDATA_TRANSFER\fR parameter:
.sp 2
*  INTERACTIVE=ONLY
.sp 2
*  RCVJOBS_FROM
.sp 2
*  MAX_RSCHED_TIME
.sp 2
*  SUCCESS_EXIT_VALUES
.sp 2
*  RERUNNABLE
.sp 2
A data transfer queue cannot appear in the list of default queues
that are defined by the \fBDEFAULT_QUEUE\fR parameter in the
lsb.params file. Jobs that are submitted to the data transfer
queue are not attached to the application specified by the
\fBDEFAULT_APPLICATION\fR parameter in the lsb.params file.
.SH Default

.sp 2
N
.sp 2

.ce 1000
\fBDEFAULT_EXTSCHED\fR
.ce 0

.sp 2
Specifies default external scheduling options for the queue.
.sp 2

.SH Syntax

.sp 2
\fRDEFAULT_EXTSCHED=\fR\fIexternal_scheduler_options\fR
.SH Description

.sp 2
-extsched options on the bsub command are merged with
DEFAULT_EXTSCHED options, and -extsched options override any
conflicting queue-level options set by DEFAULT_EXTSCHED.
.SH Default

.sp 2
Not defined
.sp 2

.ce 1000
\fBDEFAULT_HOST_SPEC\fR
.ce 0

.sp 2
Specifies the default CPU time normalization host for the queue.
.sp 2

.SH Syntax

.sp 2
\fRDEFAULT_HOST_SPEC=\fR\fIhost_name | host_model\fR
.SH Description

.sp 2
The CPU factor of the specified host or host model is used to
normalize the CPU time limit of all jobs in the queue, unless the
CPU time normalization host is specified at the job level.
.SH Default

.sp 2
Not defined. The queue uses the DEFAULT_HOST_SPEC defined in
lsb.params. If DEFAULT_HOST_SPEC is not defined in either file,
LSF uses the fastest host in the cluster.
.sp 2

.ce 1000
\fBDESCRIPTION \fR
.ce 0

.sp 2
Specifies a description of the job queue that is displayed by
bqueues -l.
.sp 2

.SH Syntax

.sp 2
\fRDESCRIPTION=\fR\fItext\fR
.SH Description

.sp 2
Use a description that clearly describes the service features of
this queue to help users select the proper queue for each job.
.sp 2
The text can include any characters, including white space. The
text can be extended to multiple lines by ending the preceding
line with a backslash (\fR\\\fR). The maximum length for the text
is 512 characters.
.sp 2

.ce 1000
\fBDISPATCH_BY_QUEUE (Obsolete)\fR
.ce 0

.sp 2
This parameter is obsolete in LSF Version 10.1 Fix Pack 10 and is
replaced by the \fBJOB_DISPATCH_PACK_SIZE\fR parameter in the
lsb.params file.
.sp 2

.SH Syntax

.sp 2
\fRDISPATCH_BY_QUEUE=Y|y|N|n\fR
.SH Description

.sp 2
Enables the scheduling decision for the specified queue to be
published without waiting for the whole scheduling session to
finish.
.sp 2
Set this parameter to increase queue responsiveness. The
scheduling decision for the jobs in the specified queue is final
and these jobs cannot be preempted within the same scheduling
cycle.
.sp 2
\fBTip: \fRSet this parameter only for your highest priority
queue (such as for an interactive queue) to ensure that this
queue has the highest responsiveness.
.SH Default

.sp 2
N
.sp 2

.ce 1000
\fBDISPATCH_ORDER\fR
.ce 0

.sp 2
Defines an ordered cross-queue fairshare set, which indicates
that jobs are dispatched according to the order of queue
priorities first, then user fairshare priority.
.sp 2

.SH Syntax

.sp 2
\fRDISPATCH_ORDER=QUEUE\fR
.SH Description

.sp 2
By default, a user has the same priority across the parent and
child queues. If the same user submits several jobs to these
queues, user priority is calculated by taking into account all
the jobs the user submits across the parent-child set.
.sp 2
If DISPATCH_ORDER=QUEUE is set in the parent queue, jobs are
dispatched according to queue priorities first, then user
priority. Jobs from users with lower fairshare priorities who
have pending jobs in higher priority queues are dispatched before
jobs in lower priority queues. This behavior avoids having users
with higher fairshare priority from getting jobs that are
dispatched from low-priority queues.
.sp 2
Jobs in queues with the same priority are dispatched according to
user priority.
.sp 2
Queues that are not part of the cross-queue fairshare can have
any priority; they are not limited to fall outside of the
priority range of cross-queue fairshare queues.
.SH Default

.sp 2
Not defined
.sp 2

.ce 1000
\fBDISPATCH_WINDOW\fR
.ce 0

.sp 2
Specifies the time windows in which jobs from this queue are
dispatched. After jobs are dispatched, they are no longer
affected by the dispatch window.
.sp 2

.SH Syntax

.sp 2
\fRDISPATCH_WINDOW=\fR\fItime_window\fR ...
.SH Description

.sp 2
Jobs from this queue are not dispatched outside of the dispatch
window.
.SH Default

.sp 2
Not defined. Dispatch window is always open.
.sp 2

.ce 1000
\fBDOCKER_IMAGE_AFFINITY\fR
.ce 0

.sp 2

.SH Syntax

.sp 2
\fBDOCKER_IMAGE_AFFINITY=Y\fR | \fBy\fR | \fBN\fR | \fBn\fR
.SH Description

.sp 2
When scheduling Docker-based containerized jobs, setting this
parameter to \fRy\fR or \fRY\fR enables LSF to give preference
for execution hosts that already have the requested Docker image.
This reduces network bandwidth and the job start time because the
execution host does not have to pull the Docker image from the
repository and the job can immediately start on the execution
host.
.sp 2
When this feature is enabled, LSF considers Docker image location
information when scheduling Docker jobs. Docker image affinity
interacts with host preference and \fBorder[]\fR string requests
in the following manner:
.sp 2
*  If host preference is specified, the host preference is
   honored first. Among hosts with the same preference level,
   hosts with the requested Docker image are given higher
   priority.
.sp 2
*  If the \fBorder[]\fR string is specified, the hosts with the
   requested Docker image have a higher priority first. Among
   hosts that all have the requested Docker image, the
   \fBorder[]\fR string is then honored.
.sp 2
The \fBCONTAINER\fR parameter must be defined for this parameter
to work with this queue.
.SH Default

.sp 2
Not defined.
.sp 2

.ce 1000
\fBELIGIBLE_PEND_TIME_LIMIT\fR
.ce 0

.sp 2
Specifies the eligible pending time limit for a job.
.sp 2

.SH Syntax

.sp 2
\fRELIGIBLE_PEND_TIME_LIMIT=\fR[\fIhour\fR\fR:\fR]\fIminute\fR
.SH Description

.sp 2
LSF sends the queue-level eligible pending time limit
configuration to IBM Spectrum LSF RTM (LSF RTM), which handles
the alarm and triggered actions such as user notification (for
example, notifying the user that submitted the job and the LSF
administrator) and job control actions (for example, killing the
job). LSF RTM compares the job\(aqs eligible pending time to the
eligible pending time limit, and if the job is in an eligible
pending state for longer than this specified time limit, LSF RTM
triggers the alarm and actions. This parameter works without LSF
RTM, but LSF does not take any other alarm actions.
.sp 2
In MultiCluster job forwarding mode, the job\(aqs eligible pending
time limit is ignored in the execution cluster, while the
submission cluster merges the job\(aqs queue-, application-, and
job-level eligible pending time limit according to local
settings.
.sp 2
The eligible pending time limit is in the form of
[\fIhour\fR\fR:\fR]\fIminute\fR. The minutes can be specified as
a number greater than 59. For example, three and a half hours can
either be specified as 3:30, or 210.
.sp 2
The job-level eligible pending time limit (bsub -eptl) overrides
the application-level limit (\fBELIGIBLE_PEND_TIME_LIMIT\fR in
lsb.applications), and the application-level limit overrides the
queue-level limit specified here.
.SH Default

.sp 2
Not defined.
.sp 2

.ce 1000
\fBENABLE_GPU_HIST_RUN_TIME\fR
.ce 0

.sp 2
Enables the use of historical GPU run time in the calculation of
fairshare scheduling priority. Used only with fairshare
scheduling.
.sp 2

.SH Syntax

.sp 2
\fRENABLE_GPU_HIST_RUN_TIME=y | Y | n | N\fR
.SH Description

.sp 2
If set to Y, enables the use of historical GPU run time in the
calculation of fairshare scheduling priority.
.sp 2
If undefined, the cluster-wide value from the lsb.params
parameter of the same name is used.
.SH Default

.sp 2
Not defined.
.sp 2

.ce 1000
\fBENABLE_HIST_RUN_TIME\fR
.ce 0

.sp 2
Enables the use of historical run time in the calculation of
fairshare scheduling priority. Used only with fairshare
scheduling.
.sp 2

.SH Syntax

.sp 2
\fRENABLE_HIST_RUN_TIME=y | Y | n | N\fR
.SH Description

.sp 2
If set to Y, enables the use of historical run time in the
calculation of fairshare scheduling priority.
.sp 2
If undefined, the cluster-wide value from the lsb.params
parameter of the same name is used.
.SH Default

.sp 2
Not defined.
.sp 2

.ce 1000
\fBESTIMATED_RUNTIME\fR
.ce 0

.sp 2

.SH Syntax

.sp 2
\fRESTIMATED_RUNTIME=\fR[\fIhour\fR\fR:\fR]\fIminute\fR[\fR/\fR\fIhost_name\fR
| \fR/\fR\fIhost_model\fR]
.SH Description

.sp 2
This parameter specifies an estimated run time for jobs
associated with a queue. LSF uses the ESTIMATED_RUNTIME value for
scheduling purposes only, and does not kill jobs that exceed this
value unless the jobs also exceed a defined RUNLIMIT. The format
of runtime estimate is same as the RUNLIMIT parameter.
.sp 2
The job-level runtime estimate specified by bsub -We or the
ESTIMATED_RUNTIME setting in an application override the
ESTIMATED_RUNTIME setting in the queue. The ESTIMATED_RUNTIME
setting in the queue overrides the cluster-wide ESTIMATED_RUNTIME
setting.
.sp 2
The following LSF features use the ESTIMATED_RUNTIME value to
schedule jobs:
.sp 2
*  Job chunking
.sp 2
*  Advance reservation
.sp 2
*  SLA
.sp 2
*  Slot reservation
.sp 2
*  Backfill
.sp 2
*  Allocation planner
.SH Default

.sp 2
Not defined
.sp 2

.ce 1000
\fBEXCLUSIVE \fR
.ce 0

.sp 2
Specifies an exclusive queue and compute unit type.
.sp 2

.SH Syntax

.sp 2
\fREXCLUSIVE=Y\fR | \fRN\fR | \fRCU\fR[\fIcu_type\fR]
.SH Description

.sp 2
If set to Y, specifies an exclusive queue.
.sp 2
If set to CU, CU[], or CU[\fIcu_type\fR], specifies an exclusive
queue as well as a queue exclusive to compute units of type
\fIcu_type\fR (as defined in lsb.params). If no type is
specified, the default compute unit type is used.
.sp 2
Jobs that are submitted to an exclusive queue with bsub -x are
only dispatched to a host that has no other running jobs. Jobs
that are submitted to a compute unit exclusive queue with bsub -R
"cu[excl]" only run on a compute unit that has no other running
jobs.
.sp 2
For hosts shared under the MultiCluster resource leasing model,
jobs are not dispatched to a host that has running jobs, even if
the jobs are from another cluster.
.sp 2
\fBNote: \fREXCLUSIVE=Y or EXCLUSIVE=CU[\fIcu_type\fR] must be
configured to enable affinity jobs to use CPUs exclusively, when
the \fRalljobs\fR scope is specified in the \fRexclusive\fR
option of an affinity[] resource requirement string.
.SH Default

.sp 2
N
.sp 2

.ce 1000
\fBEXEC_DRIVER\fR
.ce 0

.sp 2

.SH Syntax

.sp 2
Docker and Podman jobs:
\fREXEC_DRIVER=context[user(\fR\fIuser_name\fR\fR)]
starter[/\fR\fIfile_path_serverdir\fR\fR/docker-starter.py]
controller[/\fR\fIfile_path/to/serverdir\fR\fR/docker-control.py]
monitor[/\fR\fIfile_path/to/serverdir\fR\fR/docker-monitor.py]\fR
.sp 2
Enroot jobs:
\fREXEC_DRIVER=starter[/\fR\fIfile_path_serverdir\fR\fR/enroot-starter.py]\fR
.sp 2
Replace \fIfile_path/to/serverdir\fR with the actual file path of
the \fILSF_SERVERDIR\fR directory.
.SH Description

.sp 2
Optional for Enroot jobs. Specifies the execution driver
framework for Docker or Podman container jobs in this queue. This
parameter uses the following keyword:
.sp 2
\fBuser\fR
.br
         Optional for Docker jobs and ignored for Enroot jobs.
         This keyword specifies the user account for starting
         scripts. The configured value is a user name instead of
         a user ID. For Docker jobs, this user must be a member
         of the Docker user group. For Podman jobs, the user name
         must be set to "\fRdefault\fR".
.sp 2
         By default, this is the LSF primary administrator.
.sp 2
         \fBNote: \fRThis cannot be the root user.
.sp 2
LSF includes three execution driver scripts that are used to
start a job (docker-starter.py), monitor the resource of a job
(docker-monitor.py), and send a signal to a job
(docker-control.py). These scripts are located in the
\fILSF_SERVERDIR\fR directory. Change the owner of the script
files to the context user and change the file permissions to 700
or 500 before using them in the \fBEXEC_DRIVER\fR parameter.
.sp 2
The starter script is required. For Docker container jobs, the
monitor and control scripts are required if the cgroupfs driver
is systemd, but are optional if the cgroupfs driver is cgroupfs.
For Podman container jobs, the monitor script is optional while
the control script is required. For Enroot container jobs, the
starter script is required while all other scripts are ignored.
.SH Interaction with the CONTAINER parameter for Docker, Podman, or
Enroot jobs, Podman, or Enroot jobsjobs

.sp 2
For Docker , Podman, or Enrootjobs, the \fBEXEC_DRIVER\fR
parameter interacts with the following keywords in the
\fBCONTAINER\fR parameter:
.sp 2
*  \fBimage\fR, which specifies the image name
   (\fB$LSB_CONTAINER_IMAGE\fR environment variable) is supported
   when specifying the script names.
.sp 2
*  \fBoptions\fR with runtime options and the option script is
   supported.
.SH Example

.sp 2
Begin Queue
.br
NAME = dockerq
.br
CONTAINER = docker[image(repository.example.com:5000/file/path/ubuntu:latest)
.br
            options(--rm --network=host --ipc=host -v /path/to/my/passwd:/etc/passwd)]
.br
EXEC_DRIVER = context[user(user-name)] starter[/path/to/driver/docker-starter.py]
.br
              controller[/path/to/driver/docker-control.py]
.br
              monitor[/path/to/driver/docker-monitor.py]
.br
DESCRIPTION = Docker User Service
.br
End Queue
.sp 2
Begin Queue
.br
NAME = podmanq
.br
CONTAINER = docker[image(repository.example.com:5000/file/path/ubuntu:latest)
.br
            options(--rm --network=host --ipc=host -v /path/to/my/passwd:/etc/passwd)]
.br
EXEC_DRIVER = context[user(default)] starter[/path/to/driver/docker-starter.py]
.br
              controller[/path/to/driver/docker-control.py]
.br
              monitor[/path/to/driver/docker-monitor.py]
.br
DESCRIPTION = Podman User Service
.br
End Queue
.sp 2
Begin Queue
.br
NAME = enrootq
.br
CONTAINER = enroot[image(repository.example.com:5000/file/path/ubuntu:latest)
.br
            options(--mount /mydir:/mydir2]
.br
EXEC_DRIVER = starter[/path/to/driver/enroot-starter.py]
.br
DESCRIPTION = Enroot User Service
.br
End Queue
.SH Default

.sp 2
Undefined for Docker and Podman jobs.
.sp 2
\fRstarter[\fI$LSF_SERVERDIR\fR/enroot-starter.py]\fR for Enroot
jobs
.sp 2

.ce 1000
\fBEXTENDABLE_RUNLIMIT\fR
.ce 0

.sp 2
Enables the LSF allocation planner to extend the run limits of a
job by changing its soft run limit if the resources that are
occupied by this job are not needed by other jobs in queues with
the same or higher priority. A soft run limit can be extended,
while a hard run limit cannot be extended. The allocation planner
looks at job plans of other jobs to determine if there are other
jobs that require this job\(aqs resources.
.sp 2

.SH Syntax

.sp 2
\fREXTENDABLE_RUNLIMIT=BASE[\fIminutes\fR]
INCREMENT[\fIminutes\fR] GRACE[\fIminutes\fR] REQUEUE[Y|N]\fR
.SH Description

.sp 2
If configured, this parameter applies an extendable run limit
policy to jobs with plans in the queue. When a job with a plan is
dispatched, LSF sets an initial soft run limit for the job.
Whenever a job reaches the soft run limit, LSF considers whether
another job has a planned allocation on the resources. If not,
LSF extends the job\(aqs soft run limit. Otherwise, LSF sets a hard
run limit for the job. Whenever a job reaches the hard run limit,
LSF terminates or requeues the job.
.sp 2
This parameter uses the following keywords:
.sp 2
\fB\fRBASE[\fB\fIminutes\fB\fR]\fB\fR
.br
         The initial soft run limit that is imposed on jobs in
         the queue. Whenever the job reaches the soft run limit,
         the allocation planner considers whether the resources
         that are held by the job are needed by another job in
         the queue by looking at plans for other jobs. If the
         resources are not required, LSF extends the soft run
         limit for the job. Otherwise, LSF sets a hard run limit.
.sp 2
         Specify an integer value for the initial soft run limit.
.sp 2
\fB\fRINCREMENT[\fB\fIminutes\fB\fR]\fB\fR
.br
         Optional. If LSF decides to extend the soft run limit
         for the job, this keyword specifies the amount of time
         that LSF extends the soft run limit.
.sp 2
         Specify an integer value for the soft run limit
         extension time. The default value is the value of the
         \fBBASE[]\fR keyword.
.sp 2
\fB\fRGRACE[\fB\fIminutes\fB\fR]\fB\fR
.br
         Optional. If LSF decides not to extend the soft run
         limit for the job, a hard run limit is set for this
         amount of minutes from the time the decision is made.
.sp 2
         The default value is \fR0\fR (the job is terminated or
         requeued immediately).
.sp 2
\fB\fRREQUEUE[Y\fB | \fRN]\fB\fR
.br
         Optional. Specifies the action that LSF takes when a job
         reaches its hard run limit. If set to \fRN\fR, LSF
         terminates the job. If set to \fRY\fR LSF requeues the
         job.
.sp 2
         The default value is \fRN\fR (LSF terminates the job
         once the job reaches its hard run limit).
.SH Default

.sp 2
Not defined.
.sp 2
Jobs that reach the specified run limit time (as specified by the
\fBRUNLIMIT\fR parameter or the \fR-W\fR option) are checkpointed
(if checkpointable), then terminated, regardless of whether
resources are available.
.sp 2

.ce 1000
\fBFAIRSHARE\fR
.ce 0

.sp 2
Enables queue-level user-based fairshare and specifies share
assignments.
.sp 2

.SH Syntax

.sp 2
\fRFAIRSHARE=USER_SHARES\fR[\fR[\fR\fIuser\fR\fR,
\fR\fInumber_shares\fR\fR]\fR ...]
.sp 2
*  Specify at least one user share assignment.
.sp 2
*  Enclose the list in square brackets, as shown.
.sp 2
*  Enclose each user share assignment in square brackets, as
   shown.
.sp 2
*  \fIuser\fR: specify users who are also configured to use
   queue. You can assign the shares to the following types of
   users:
.sp 2
   *  A single user (specify \fIuser_name\fR). To specify a
      Windows user account, include the domain name in uppercase
      letters (\fIDOMAIN_NAME\fR\\\fIuser_name\fR).
.sp 2
   *  Users in a group, individually (specify \fIgroup_name\fR@)
      or collectively (specify \fIgroup_name\fR). To specify a
      Windows user group, include the domain name in uppercase
      letters (\fIDOMAIN_NAME\fR\\\fIgroup_name\fR.
.sp 2
   *  Users not included in any other share assignment,
      individually (specify the keyword default) or collectively
      (specify the keyword others)
.sp 2
      *  By default, when resources are assigned collectively to
         a group, the group members compete for the resources on
         a first-come, first-served (FCFS) basis. You can use
         hierarchical fairshare to further divide the shares
         among the group members.
.sp 2
      *  When resources are assigned to members of a group
         individually, the share assignment is recursive. Members
         of the group and of all subgroups always compete for the
         resources according to FCFS scheduling, regardless of
         hierarchical fairshare policies.
.sp 2
*  \fInumber_shares\fR
.sp 2
   *  Specify a positive integer that represents the number of
      shares of the cluster resources that are assigned to the
      user.
.sp 2
   *  The number of shares that are assigned to each user is only
      meaningful when you compare it to the shares assigned to
      other users or to the total number of shares. The total
      number of shares is just the sum of all the shares that are
      assigned in each share assignment.
.SH Description

.sp 2
Enables queue-level user-based fairshare and specifies optional
share assignments. If share assignments are specified, only users
with share assignments can submit jobs to the queue.
.SH Compatibility

.sp 2
Do not configure hosts in a cluster to use fairshare at both
queue and host levels. However, you can configure user-based
fairshare and queue-based fairshare together.
.SH Default

.sp 2
Not defined. No fairshare.
.sp 2

.ce 1000
\fBFAIRSHARE_ADJUSTMENT_FACTOR\fR
.ce 0

.sp 2
Specifies the fairshare adjustment plug-in weighting factor. Used
only with fairshare scheduling.
.sp 2

.SH Syntax

.sp 2
\fRFAIRSHARE_ADJUSTMENT_FACTOR=\fR\fInumber\fR
.SH Description

.sp 2
In the calculation of a user dynamic share priority, this factor
determines the relative importance of the user-defined adjustment
that is made in the fairshare plug-in (libfairshareadjust.*).
.sp 2
A positive float number both enables the fairshare plug-in and
acts as a weighting factor.
.sp 2
If undefined, the cluster-wide value from the lsb.params
parameter of the same name is used.
.SH Default

.sp 2
Not defined.
.sp 2

.ce 1000
\fBFAIRSHARE_QUEUES\fR
.ce 0

.sp 2
Defines cross-queue fairshare.
.sp 2

.SH Syntax

.sp 2
\fRFAIRSHARE_QUEUES=\fR\fIqueue_name\fR[\fIqueue_name\fR ...]
.SH Description

.sp 2
When this parameter is defined:
.sp 2
*  The queue in which this parameter is defined becomes the
   “\fImanagement\fR”.
.sp 2
*  Queues that are listed with this parameter are \fIchild
   queues\fR and inherit the fairshare policy of the parent
   queue.
.sp 2
*  A user has the same priority across the parent and child
   queues. If the same user submits several jobs to these queues,
   user priority is calculated by taking into account all the
   jobs that the user submitted across the parent-child set.
.SH Notes

.sp 2
*  By default, the PRIORITY range that is defined for queues in
   cross-queue fairshare cannot be used with any other queues.
   For example, you have 4 queues: \fRqueue1\fR, \fRqueue2\fR,
   \fRqueue3\fR, \fRqueue4\fR. You assign the following
   cross-queue fairshare: priorities
.sp 2
   *  \fRqueue1\fR priority: 30
.sp 2
   *  \fRqueue2\fR priority: 40
.sp 2
   *  \fRqueue3\fR priority: 50
.sp 2
*  By default, the priority of \fRqueue4\fR (which is not part of
   the cross-queue fairshare) cannot fall between the priority
   range of the cross-queue fairshare queues (30-50). It can be
   any number up to 29 or higher than 50. It does not matter if
   \fRqueue4\fR is a fairshare queue or FCFS queue. If
   DISPATCH_ORDER=QUEUE is set in the parent queue, the priority
   of \fRqueue4\fR (which is not part of the cross-queue
   fairshare) can be any number, including a priority that falls
   between the priority range of the cross-queue fairshare queues
   (30-50).
.sp 2
*  FAIRSHARE must be defined in the parent queue. If it is also
   defined in the queues that are listed in FAIRSHARE_QUEUES, it
   is ignored.
.sp 2
*  Cross-queue fairshare can be defined more than once within
   lsb.queues. You can define several sets of parent-child
   queues. However, a queue cannot belong to more than one
   parent-child set. For example, you can define:
.sp 2
   *  In queue \fRnormal\fR: \fRFAIRSHARE_QUEUES=short\fR
.sp 2
   *  In queue \fRpriority\fR: \fRFAIRSHARE_QUEUES=night
      owners\fR
.sp 2
      \fBRestriction: \fRYou cannot, however, define \fRnight\fR,
      \fRowners\fR, or \fRpriority \fRas children in the queue
      \fRnormal\fR; or \fRnormal\fRand \fRshort\fR as children in
      the \fRpriority\fR queue; or \fRshort\fR, \fRnight\fR,
      \fRowners\fR as parent queues of their own.
.sp 2
*  Cross-queue fairshare cannot be used with host partition
   fairshare. It is part of queue-level fairshare.
.sp 2
*  Cross-queue fairshare cannot be used with absolute priority
   scheduling.
.SH Default

.sp 2
Not defined
.sp 2

.ce 1000
\fBFILELIMIT\fR
.ce 0

.sp 2
Specifies the per-process file size limit for all job processes
from this queue.
.sp 2

.SH Syntax

.sp 2
\fRFILELIMIT=\fR\fIinteger\fR
.SH Description

.sp 2
Set this parameter to place a per-process hard file size limit,
in KB, for all of the processes that belong to a job from this
queue (see getrlimit(2)).
.SH Default

.sp 2
Unlimited
.sp 2

.ce 1000
\fBFWD_JOB_FACTOR\fR
.ce 0

.sp 2
Forwarded job slots weighting factor. Used only with fairshare
scheduling.
.sp 2

.SH Syntax

.sp 2
\fRFWD_JOB_FACTOR=\fR\fInumber\fR
.SH Description

.sp 2
In the calculation of a user\(aqs dynamic share priority, this
factor determines the relative importance of the number of
forwarded job slots reserved and in use by a user when using the
LSF multicluster capability.
.sp 2
If undefined, the cluster-wide value from the lsb.params
parameter of the same name is used.
.SH Default

.sp 2
Not defined.
.SH See also

.sp 2
\fBRUN_JOB_FACTOR\fR
.sp 2

.ce 1000
\fBFWD_USERS\fR
.ce 0

.sp 2
Specifies a space-separated list of user names or user groups
that can forward jobs to remote clusters in this queue when using
the LSF multicluster capability.
.sp 2

.SH Syntax

.sp 2
\fRFWD_USERS=all\fR [\fR~\fR\fIuser_name\fR ...]
[\fR~\fR\fIuser_group\fR ...] | [\fIuser_name\fR ...]
[\fIuser_group\fR [\fR~\fR\fIuser_group\fR ...] ...]
.SH Description

.sp 2
If user groups are specified, each user in the group can forward
jobs to remote clusters in this queue. Use local user groups when
specifying user groups.
.sp 2
User names must be valid login names. To specify a Windows user
account, include the domain name in uppercase letters
(\fIDOMAIN_NAME\fR\\\fIuser_name\fR).
.sp 2
User group names can be LSF user groups or UNIX and Windows user
groups. To specify a Windows user group, include the domain name
in uppercase letters (\fIDOMAIN_NAME\fR\\\fIuser_group\fR).
.sp 2
Use the keyword all to specify all users or user groups in a
cluster.
.sp 2
Use the not operator (~) to exclude users from the all
specification or from user groups. This is useful if you have a
large number of users but only want to exclude a few users or
groups from the queue definition.
.sp 2
The not operator (~) can only be used with the all keyword or to
exclude users from user groups.
.SH Default

.sp 2
all (all users can forward jobs to remote clusters in the queue)
.SH Examples

.sp 2
*  \fRFWD_USERS=user1 user2\fR
.sp 2
*  \fRFWD_USERS=all ~user1 ~user2\fR
.sp 2
*  \fRFWD_USERS=all ~ugroup1\fR
.sp 2
*  \fRFWD_USERS=groupA ~user3 ~user4\fR
.sp 2

.ce 1000
\fBGPU_REQ\fR
.ce 0

.sp 2
Specify GPU requirements together in one statement.
.sp 2

.SH Syntax

.sp 2
\fBGPU_REQ = "\fR[\fBnum=\fRnum_gpus[\fB/\fR\fBtask\fR |
\fBhost\fR]] [\fB:mode=shared\fR | \fBexclusive_process\fR]
[\fB:mps=\fR\fByes\fR[\fB,shared\fR][\fB,nocvd\fR] | \fBno\fR]
[\fB:j_exclusive=\fR\fByes\fR | \fBno\fR] [\fB:gvendor=amd\fR |
\fBnvidia\fR] [\fB:gmodel=\fRmodel_name[\fB#\fRmem_size]]
[\fB:gtile=\fRtile_num|\fB\(aq!\(aq\fR] [\fB:gmem=\fRmem_value]
[\fB:glink=yes\fR][\fB:block=\fR\fByes\fR | \fBno\fR]
[\fB:gpack=\fR\fByes\fR | \fBno\fR]\fB"\fR
.SH Description

.sp 2
The \fBGPU_REQ\fR parameter takes the following arguments:
.sp 2
\fBnum=\fInum_gpus\fB[/task | host]\fR
.br
         The number of physical GPUs required by the job. By
         default, the number is per host. You can also specify
         that the number is per task by specifying \fR/task\fR
         after the number.
.sp 2
         If you specified that the number is per task, the
         configuration of the \fRngpus_physical\fR resource in
         the lsb.resources file is set to \fRPER_TASK\fR, or the
         \fBRESOURCE_RESERVE_PER_TASK=Y\fR parameter is set in
         the lsb.params file, this number is the requested count
         per task.
.sp 2
\fBmode=shared | exclusive_process\fR
.br
         The GPU mode when the job is running, either shared or
         exclusive_process. The default mode is shared.
.sp 2
         The shared mode corresponds to the Nvidia or AMD DEFAULT
         compute mode. The exclusive_process mode corresponds to
         the Nvidia EXCLUSIVE_PROCESS compute mode.
.sp 2
         \fBNote: \fR Do not specify exclusive_process when you
         are using AMD GPUs (that is, when gvendor=amd is
         specified).
.sp 2
\fBmps=yes[,nocvd][,shared] | no\fR
.br
         Enables or disables the Nvidia Multi-Process Service
         (MPS) for the GPUs that are allocated to the job. Using
         MPS effectively causes the EXCLUSIVE_PROCESS mode to
         behave like the DEFAULT mode for all MPS clients. MPS
         always allows multiple clients to use the GPU through
         the MPS server.
.sp 2
         \fBNote: \fRTo avoid inconsistent behavior, do not
         enable mps when you are using AMD GPUs (that is, when
         gvendor=amd is specified). If the result of merging the
         GPU requirements at the cluster, queue, application, and
         job levels is gvendor=amd and mps is enabled (for
         example, if gvendor=amd is specified at the job level
         without specifying mps=no, but mps=yes is specified at
         the application, queue, or cluster level), LSF ignores
         the mps requirement.
.sp 2
         MPS is useful for both shared and exclusive process
         GPUs, and allows more efficient sharing of GPU resources
         and better GPU utilization. See the Nvidia documentation
         for more information and limitations.
.sp 2
         When using MPS, use the EXCLUSIVE_PROCESS mode to ensure
         that only a single MPS server is using the GPU, which
         provides additional insurance that the MPS server is the
         single point of arbitration between all CUDA process for
         that GPU.
.sp 2
         You can also enable MPS daemon sharing by adding the
         \fBshare\fR keyword with a comma and no space (for
         example, \fRmps=yes,shared\fR enables MPS daemon sharing
         on the host). If sharing is enabled, all jobs that are
         submitted by the same user with the same resource
         requirements share the same MPS daemon on the host,
         socket, or GPU.
.sp 2
         \fBImportant: \fRUsing EXCLUSIVE_THREAD mode with MPS is
         not supported and might cause unexpected behavior.
.sp 2
\fBj_exclusive=yes | no\fR
.br
         Specifies whether the allocated GPUs can be used by
         other jobs. When the mode is set to exclusive_process,
         the j_exclusive=yes option is set automatically.
.sp 2
\fBblock=yes | no\fR
.br
         Specifies whether to enable block distribution, that is,
         to distribute the allocated GPUs of a job as blocks when
         the number of tasks is greater than the requested number
         of GPUs. If set to \fRyes\fR, LSF distributes all the
         allocated GPUs of a job as blocks when the number of
         tasks is bigger than the requested number of GPUs. By
         default, \fRblock=no\fR is set so that allocated GPUs
         are not distributed as blocks.
.sp 2
         For example, if a GPU job requests to run on a host with
         4 GPUs and 40 tasks, block distribution assigns GPU0 for
         ranks 0-9, GPU1 for ranks 10-19, GPU2 for tanks 20-29,
         and GPU3 for ranks 30-39.
.sp 2
         \fBNote: \fRThe \fRblock=yes\fR setting conflicts with
         \fRaff=yes\fR (strict CPU-GPU affinity binding). This is
         because strict CPU-GPU binding allocates GPUs to tasks
         based on the CPU NUMA ID, which conflicts with the
         distribution of allocated GPUs as blocks. If
         \fRblock=yes\fR and \fRaff=yes\fR are both specified in
         the GPU requirements string, the \fRblock=yes\fR setting
         takes precedence and strict CPU-GPU affinity binding is
         disabled (that is, \fRaff=no\fR is automatically set).
.sp 2
\fBgpack=yes | no\fR
.br
         For shared mode jobs only. Specifies whether to enable
         pack scheduling. If set to \fRyes\fR, LSF packs multiple
         shared mode GPU jobs to allocated GPUs. LSF schedules
         shared mode GPUs as follows:
.sp 2
         1. LSF sorts the candidate hosts (from largest to
            smallest) based on the number of shared GPUs that
            already have running jobs, then by the number of GPUs
            that are not exclusive.
.sp 2
            If the order[] keyword is defined in the resource
            requirements string, after sorting order[], LSF
            re-sorts the candidate hosts by the gpack policy (by
            shared GPUs that already have running jobs first,
            then by the number of GPUs that are not exclusive).
            The gpack policy sort priority is higher than the
            order[] sort.
.sp 2
         2. LSF sorts the candidate GPUs on each host (from
            largest to smallest) based on the number of running
            jobs.
.sp 2
         After scheduling, the shared mode GPU job packs to the
         allocated shared GPU that is sorted first, not to a new
         shared GPU.
.sp 2
         If Docker attribute affinity is enabled, the order of
         candidate hosts are sorted by Docker attribute affinity
         before sorting by GPUs.
.sp 2
         By default, \fRgpack=no\fR is set so that pack
         scheduling is disabled.
.sp 2
\fBgvendor=amd | nvidia\fR
.br
         Specifies the GPU vendor type. LSF allocates GPUs with
         the specified vendor type.
.sp 2
         Specify \fRamd\fR to request AMD GPUs, or specify
         \fRnvidia\fR to request Nvidia GPUs.
.sp 2
         By default, LSF requests Nvidia GPUs.
.sp 2
\fBgmodel=\fImodel_name\fB[-\fImem_size\fB]\fR
.br
         Specifies GPUs with the specific model name and,
         optionally, its total GPU memory. By default, LSF
         allocates the GPUs with the same model, if available.
.sp 2
         The \fBgmodel\fR keyword supports the following formats:
.sp 2
         \fBgmodel=\fImodel_name\fB\fR
.br
                  Requests GPUs with the specified brand and
                  model name (for example, TeslaK80).
.sp 2
         \fBgmodel=\fIshort_model_name\fB\fR
.br
                  Requests GPUs with a specific brand name (for
                  example, Tesla, Quadro, NVS, ) or model type
                  name (for example, K80, P100).
.sp 2
         \fBgmodel=\fImodel_name\fB-\fImem_size\fB\fR
.br
                  Requests GPUs with the specified brand name and
                  total GPU memory size. The GPU memory size
                  consists of the number and its unit, which
                  includes \fRM\fR, \fRG\fR, \fRT\fR, \fRMB\fR,
                  \fRGB\fR, and \fRTB\fR (for example,
                  \fR12G\fR).
.sp 2
         To find the available GPU model names on each host, run
         the lsload –gpuload, lshosts –gpu, or bhosts -gpu
         commands. The model name string does not contain space
         characters. In addition, the slash (\fR/\fR) and hyphen
         (\fR-\fR) characters are replaced with the underscore
         character (\fR_\fR). For example, the GPU model name
         “\fRTesla C2050 / C2070\fR” is converted to
         “\fRTeslaC2050_C2070\fR” in LSF.
.sp 2
\fBgmem=\fImem_value\fB\fR
.br
         Specify the GPU memory on each GPU required by the job.
         The format of \fImem_value\fR is the same to other
         resource value (for example, \fRmem\fR or \fRswap\fR) in
         the rusage section of the job resource requirements
         (-R).
.sp 2
\fBgtile=! | \fItile_num\fB\fR
.br
         Specifies the number of GPUs per socket. Specify an
         number to explicitly define the number of GPUs per
         socket on the host, or specify an exclamation mark
         (\fR!\fR) to enable LSF to automatically calculate the
         number, which evenly divides the GPUs along all sockets
         on the host. LSF guarantees the \fBgtile\fR requirements
         even for affinity jobs. This means that LSF might not
         allocate the GPU\(aqs affinity to the allocated CPUs when
         the \fBgtile\fR requirements cannot be satisfied.
.sp 2
         If the \fBgtile\fR keyword is not specified for an
         affinity job, LSF attempts to allocate enough GPUs on
         the sockets that allocated GPUs. If there are not enough
         GPUs on the optimal sockets, jobs cannot go to this
         host.
.sp 2
         If the \fBgtile\fR keyword is not specified for a
         non-affinity job, LSF attempts to allocate enough GPUs
         on the same socket. If this is not available, LSF might
         allocate GPUs on separate GPUs.
.sp 2
\fBnvlink=yes\fR
.br
         Obsolete in LSF, Version 10.1 Fix Pack 11. Use the
         \fBglink\fR keyword instead. Enables the job enforcement
         for NVLink connections among GPUs. LSF allocates GPUs
         with NVLink connections in force.
.sp 2
\fBglink=yes\fR
.br
         Enables job enforcement for special connections among
         GPUs. LSF must allocate GPUs with the special
         connections that are specific to the GPU vendor.
.sp 2
         If the job requests AMD GPUs, LSF must allocate GPUs
         with the xGMI connection. If the job requests Nvidia
         GPUs, LSF must allocate GPUs with the NVLink connection.
.sp 2
         Do not use \fBglink\fR together with the obsolete
         \fBnvlink\fR keyword.
.sp 2
         By default, LSF can allocate GPUs without special
         connections when there are not enough GPUs with these
         connections.
.sp 2
\fBmig=\fIGI_size\fB[/\fICI_size\fB]\fR
.br
         Specifies Nvidia Multi-Instance GPU (MIG) device
         requirements.
.sp 2
         Specify the requested number of GPU instances for the
         MIG job. Valid GPU instance sizes are 1, 2, 3, 4, 7.
.sp 2
         Optionally, specify the requested number of compute
         instances after the specified GPU instance size and a
         slash character (\fR/\fR). The requested compute
         instance size must be less than or equal to the
         requested GPU instance size. In addition, Nvidia MIG
         does not support the following GPU/compute instance size
         combinations: 4/3, 7/5, 7/6. If this is not specified,
         the default compute instance size is 1.
.sp 2
The syntax of the GPU requirement in the -gpu option is the same
as the syntax in the \fBLSB_GPU_REQ\fR parameter in the lsf.conf
file and the \fBGPU_REQ\fR parameter in the lsb.queues and
lsb.applications files.
.sp 2
\fBNote: \fRThe bjobs output does not show \fRaff=yes\fR even if
you specify \fRaff=yes\fR in the bsub -gpu option.
.sp 2
If the \fBGPU_REQ_MERGE\fR parameter is defined as \fRY\fR or
\fRy\fR in the lsb.params file and a GPU requirement is specified
at multiple levels (at least two of the default cluster, queue,
application profile, or job level requirements), each option of
the GPU requirement is merged separately. Job level overrides
application level, which overrides queue level, which overrides
the default cluster GPU requirement. For example, if the mode
option of the GPU requirement is defined on the -gpu option, and
the mps option is defined in the queue, the mode of job level and
the mps value of queue is used.
.sp 2
If the \fBGPU_REQ_MERGE\fR parameter is not defined as \fRY\fR or
\fRy\fR in the lsb.params file and a GPU requirement is specified
at multiple levels (at least two of the default cluster, queue,
application profile, or job level requirements), the entire GPU
requirement string is replaced. The entire job level GPU
requirement string overrides application level, which overrides
queue level, which overrides the default GPU requirement.
.sp 2
The esub parameter \fBLSB_SUB4_GPU_REQ\fR modifies the value of
the -gpu option.
.sp 2
LSF selects the GPU that meets the topology requirement first. If
the GPU mode of the selected GPU is not the requested mode, LSF
changes the GPU to the requested mode. For example, if LSF
allocates an \fRexclusive_process\fR GPU to a job that needs a
shared GPU, LSF changes the GPU mode to shared before the job
starts and then changes the mode back to \fRexclusive_process\fR
when the job finishes.
.sp 2
The GPU requirements are converted to rusage resource
requirements for the job. For example, \fRnum=2\fR is converted
to \fRrusage[ngpus_physical=2]\fR. Use the bjobs, bhist, and
bacct commands to see the merged resource requirement.
.sp 2
There might be complex GPU requirements that the bsub -gpu option
and \fBGPU_REQ\fR parameter syntax cannot cover, including
compound GPU requirements (for different GPU requirements for
jobs on different hosts, or for different parts of a parallel
job) and alternate GPU requirements (if more than one set of GPU
requirements might be acceptable for a job to run). For complex
GPU requirements, use the bsub -R command option, or the
\fBRES_REQ\fR parameter in the lsb.applications or lsb.queues
file to define the resource requirement string.
.sp 2
\fBImportant: \fRYou can define the mode, j_exclusive, and mps
options only with the -gpu option, the \fBLSB_GPU_REQ\fR
parameter in the lsf.conf file, or the \fBGPU_REQ\fR parameter in
the lsb.queues or lsb.applications files. You cannot use these
options with the rusage resource requirement string in the bsub
-R command option or the \fBRES_REQ\fR parameter in the
lsb.queues or lsb.applications files.
.SH Default

.sp 2
Not defined
.SH See also

.sp 2
*  \fBLSB_GPU_REQ\fR
.sp 2
*  bsub -gpu
.sp 2

.ce 1000
\fBGPU_RUN_TIME_FACTOR\fR
.ce 0

.sp 2
Specifies the GPU run time weighting factor. Used only with
fairshare scheduling.
.sp 2

.SH Syntax

.sp 2
\fRGPU_RUN_TIME_FACTOR=\fR\fInumber\fR
.SH Description

.sp 2
In the calculation of a user’s dynamic share priority, this
factor determines the relative importance of the total GPU run
time of a user\(aqs running GPU jobs.
.sp 2
If undefined, the cluster-wide value from the lsb.params
parameter of the same name is used.
.SH Default

.sp 2
Not defined.
.sp 2

.ce 1000
\fBHIST_HOURS\fR
.ce 0

.sp 2
Determines a rate of decay for cumulative CPU time, run time, and
historical run time. Used only with fairshare scheduling.
.sp 2

.SH Syntax

.sp 2
\fRHIST_HOURS=\fR\fIhours\fR
.SH Description

.sp 2
To calculate dynamic user priority, LSF uses a decay factor to
scale the actual CPU time and run time. One hour of recently used
time is equivalent to 0.1 hours after the specified number of
hours elapses.
.sp 2
To calculate dynamic user priority with decayed run time and
historical run time, LSF uses the same decay factor to scale the
accumulated run time of finished jobs and run time of running
jobs, so that one hour of recently used time is equivalent to 0.1
hours after the specified number of hours elapses.
.sp 2
When \fRHIST_HOURS=0\fR, CPU time and run time that is
accumulated by running jobs is not decayed.
.sp 2
If undefined, the cluster-wide value from the lsb.params
parameter of the same name is used.
.SH Default

.sp 2
Not defined.
.sp 2

.ce 1000
\fBHJOB_LIMIT\fR
.ce 0

.sp 2
Specifies the per-host job slot limit.
.sp 2

.SH Syntax

.sp 2
\fRHJOB_LIMIT=\fR\fIinteger\fR
.SH Description

.sp 2
This parameter defines the maximum number of job slots that this
queue can use on any host. This limit is configured per host,
regardless of the number of processors it might have.
.sp 2
\fBNote: \fRWhen \fRLSB_ENABLE_HPC_ALLOCATION=Y\fR is defined,
all slots on the execution hosts must be allocated to exclusive
jobs. If the \fBHJOB_LIMIT\fR value for the queue is less than
the maximum number of slots on the execution host, jobs submitted
to this queue remaining pending indefinitely. This is because LSF
cannot allocate all slots in this host for your exclusive job.
.SH Example

.sp 2
The following queue runs a maximum of one job on each of
\fRhostA\fR, \fRhostB\fR, and \fRhostC\fR:
.sp 2
Begin Queue 
.br
 ... 
.br

.br
HJOB_LIMIT = 1 
.br

.br
HOSTS=hostA hostB hostC 
.br
 ... 
.br

.br
End Queue
.br

.SH Default

.sp 2
Unlimited
.sp 2

.ce 1000
\fBHOST_POST_EXEC\fR
.ce 0

.sp 2
Enables host-based post-execution processing at the queue level.
.sp 2

.SH Syntax

.sp 2
\fBHOST_POST_EXEC=\fRcommand
.SH Description

.sp 2
The \fBHOST_POST_EXEC\fR command runs on all execution hosts
after the job finishes. If job-based post-execution
\fBPOST_EXEC\fR was defined at the queue-level,
application-level, or job-level, the \fBHOST_POST_EXEC\fR command
runs after \fBPOST_EXEC\fR of any level.
.sp 2
Host-based post-execution commands can be configured at the queue
and application level, and run in the following order:
.sp 2
1. The application-level command
.sp 2
2. The queue-level command.
.sp 2
The supported command rule is the same as the existing
\fBPOST_EXEC\fR for the queue section. See the \fBPOST_EXEC\fR
topic for details.
.sp 2
\fBNote: \fRThe host-based post-execution command cannot run on
Windows systems. This parameter cannot be used to configure
job-based post-execution processing.
.SH Default

.sp 2
Not defined.
.sp 2

.ce 1000
\fBHOST_PRE_EXEC\fR
.ce 0

.sp 2
Enables host-based pre-execution processing at the queue level.
.sp 2

.SH Syntax

.sp 2
\fBHOST_PRE_EXEC=\fRcommand
.SH Description

.sp 2
The \fBHOST_PRE_EXEC\fR command runs on all execution hosts
before the job starts. If job based pre-execution \fBPRE_EXEC\fR
was defined at the queue-level/application-level/job-level, the
\fBHOST_PRE_EXEC\fR command runs before \fBPRE_EXEC\fR of any
level.
.sp 2
Host-based pre-execution commands can be configured at the queue
and application level, and run in the following order:
.sp 2
1. The queue-level command
.sp 2
2. The application-level command.
.sp 2
The supported command rule is the same as the existing
\fBPRE_EXEC\fR for the queue section. See the \fBPRE_EXEC\fR
topic for details.
.sp 2
\fBNote: \fRThe host-based pre-execution command cannot be
executed on Windows platforms. This parameter cannot be used to
configure job-based pre-execution processing.
.SH Default

.sp 2
Not defined.
.sp 2

.ce 1000
\fBHOSTLIMIT_PER_JOB\fR
.ce 0

.sp 2
Specifies the per-job host limit.
.sp 2

.SH Syntax

.sp 2
\fRHOSTLIMIT_PER_JOB=\fR\fIinteger\fR
.SH Description

.sp 2
This parameter defines the maximum number of hosts that a job in
this queue can use. LSF verifies the host limit during the
allocation phase of scheduling. If the number of hosts requested
for a parallel job exceeds this limit and LSF cannot satisfy the
minimum number of request slots, the parallel job will pend.
However, for resumed parallel jobs, this parameter does not stop
the job from resuming even if the job\(aqs host allocation exceeds
the per-job host limit specified in this parameter.
.SH Default

.sp 2
Unlimited
.sp 2

.ce 1000
\fBHOSTS\fR
.ce 0

.sp 2
Specifies a space-separated list of hosts on which jobs from this
queue can be run.
.sp 2

.SH Syntax

.sp 2
\fRHOSTS=\fR\fIhost_list\fR | \fRnone\fR
.sp 2
*  \fIhost_list\fR is a space-separated list of the following
   items:
.sp 2
   *  \fIhost_name\fR[@\fIcluster_name\fR][[!] |
      +\fIpref_level\fR]
.sp 2
   *  \fIhost_partition\fR[+\fIpref_level\fR]
.sp 2
   *  \fIhost_group\fR[[!] | +\fIpref_level\fR]
.sp 2
   *  \fIcompute_unit\fR[[!] | +\fIpref_level\fR]
.sp 2
   *  [~]\fIhost_name\fR
.sp 2
   *  [~]\fIhost_group\fR
.sp 2
   *  [~]\fIcompute_unit\fR
.sp 2
*  The list can include the following items only once:
.sp 2
   *  all@\fIcluster_name \fR
.sp 2
   *  others[+\fIpref_level\fR]
.sp 2
   *  all
.sp 2
   *  allremote
.sp 2
   \fBNote: \fRThe \fRallremote\fR and
   \fRall@\fIcluster_name\fR\fR keywords are deprecated and might
   be removed in a future version of LSF.
.sp 2
*  The none keyword is only used with the MultiCluster job
   forwarding model, to specify a remote-only queue.
.SH Description

.sp 2
If compute units, host groups, or host partitions are included in
the list, the job can run on any host in the unit, group, or
partition. All the members of the host list should either belong
to a single host partition or not belong to any host partition.
Otherwise, job scheduling may be affected.
.sp 2
Some items can be followed by a plus sign (+) and a positive
number to indicate the preference for dispatching a job to that
host. A higher number indicates a higher preference. If a host
preference is not given, it is assumed to be 0. If there are
multiple candidate hosts, LSF dispatches the job to the host with
the highest preference; hosts at the same level of preference are
ordered by load.
.sp 2
If compute units, host groups, or host partitions are assigned a
preference, each host in the unit, group, or partition has the
same preference.
.sp 2
Use the keyword others to include all hosts not explicitly
listed.
.sp 2
Use the keyword all to include all hosts not explicitly excluded.
.sp 2
Use the keyword all@\fIcluster_name\fR \fIhostgroup_name\fR or
allremote \fIhostgroup_name\fR to include lease in hosts.
.sp 2
Use the not operator (~) to exclude hosts from the all
specification in the queue. This is useful if you have a large
cluster but only want to exclude a few hosts from the queue
definition.
.sp 2
The not operator can only be used with the all keyword. It is
\fInot\fR valid with the keywords others and none.
.sp 2
The not operator (~) can be used to exclude host groups.
.sp 2
For parallel jobs, specify first execution host candidates when
you want to ensure that a host has the required resources or
runtime environment to handle processes that run on the first
execution host.
.sp 2
To specify one or more hosts, host groups, or compute units as
first execution host candidates, add the exclamation point (!)
symbol after the name.
.sp 2
Follow these guidelines when you specify first execution host
candidates:
.sp 2
*  If you specify a compute unit or host group, you must first
   define the unit or group in the file lsb.hosts.
.sp 2
*  Do not specify a dynamic host group as a first execution host.
.sp 2
*  Do not specify all, allremote, or others, or a host partition
   as a first execution host.
.sp 2
*  Do not specify a preference (+) for a host identified by (!)
   as a first execution host candidate.
.sp 2
*  For each parallel job, specify enough regular hosts to satisfy
   the CPU requirement for the job. Once LSF selects a first
   execution host for the current job, the other first execution
   host candidates
.sp 2
   *  Become unavailable to the current job
.sp 2
   *  Remain available to other jobs as either regular or first
      execution hosts
.sp 2
*  You cannot specify first execution host candidates when you
   use the brun command.
.sp 2
\fBRestriction: \fRIf you have enabled EGO, host groups and
compute units are not honored.
.sp 2
With MultiCluster resource leasing model, use the format
\fIhost_name\fR\fR@\fR\fIcluster_name\fR to specify a borrowed
host. LSF does not validate the names of remote hosts. The
keyword others indicates all local hosts not explicitly listed.
The keyword all indicates all local hosts not explicitly
excluded. Use the keyword allremote to specify all hosts borrowed
from all remote clusters. Use \fRall@\fR\fIcluster_name\fR to
specify the group of all hosts borrowed from one remote cluster.
You cannot specify a host group or partition that includes remote
resources, unless it uses the keyword allremote to include all
remote hosts. You cannot specify a compute unit that includes
remote resources.
.sp 2
With MultiCluster resource leasing model, the not operator (~)
can be used to exclude local hosts or host groups. You cannot use
the not operator (~) with remote hosts.
.sp 2
\fBRestriction: \fRHosts that participate in queue-based
fairshare cannot be in a host partition.
.SH Behavior with host intersection

.sp 2
Host preferences specified by bsub -m combine intelligently with
the queue specification and advance reservation hosts. The jobs
run on the hosts that are both specified at job submission and
belong to the queue or have advance reservation.
.SH Example 1

.sp 2
HOSTS=hostA+1 hostB hostC+1 hostD+3
.br

.sp 2
This example defines three levels of preferences: run jobs on
\fRhostD\fR as much as possible, otherwise run on either
\fRhostA\fR or \fRhostC\fR if possible, otherwise run on
\fRhostB\fR. Jobs should not run on \fRhostB\fR unless all other
hosts are too busy to accept more jobs.
.SH Example 2

.sp 2
HOSTS=hostD+1 others
.br

.sp 2
Run jobs on \fRhostD\fR as much as possible, otherwise run jobs
on the least-loaded host available.
.SH Example 3

.sp 2
HOSTS=all ~hostA
.br

.sp 2
Run jobs on all hosts in the cluster, except for \fRhostA\fR.
.SH Example 4

.sp 2
HOSTS=Group1 ~hostA hostB hostC
.br

.sp 2
Run jobs on \fRhostB\fR, \fRhostC\fR, and all hosts in
\fRGroup1\fR except for \fRhostA\fR.
.SH Example 5

.sp 2
HOSTS=hostA! hostB+ hostC hostgroup1!
.br

.sp 2
Runs parallel jobs using either \fRhostA\fR or a host defined in
\fRhostgroup1\fR as the first execution host. If the first
execution host cannot run the entire job due to resource
requirements, runs the rest of the job on \fRhostB\fR. If
\fRhostB\fR is too busy to accept the job, or if \fRhostB\fR does
not have enough resources to run the entire job, runs the rest of
the job on \fRhostC\fR.
.SH Example 6

.sp 2
HOSTS=computeunit1! hostB hostC
.br

.sp 2
Runs parallel jobs using a host in \fRcomputeunit1\fR as the
first execution host. If the first execution host cannot run the
entire job due to resource requirements, runs the rest of the job
on other hosts in \fRcomputeunit1\fR followed by \fRhostB\fR and
finally \fRhostC\fR.
.SH Example 7

.sp 2
HOSTS=hostgroup1! computeunitA computeunitB computeunitC
.br

.sp 2
Runs parallel jobs using a host in \fRhostgroup1\fR as the first
execution host. If additional hosts are required, runs the rest
of the job on other hosts in the same compute unit as the first
execution host, followed by hosts in the remaining compute units
in the order they are defined in the lsb.hosts ComputeUnit
section.
.SH Default

.sp 2
all (the queue can use all hosts in the cluster, and every host
has equal preference)
.sp 2

.ce 1000
\fBIGNORE_DEADLINE\fR
.ce 0

.sp 2
Disables deadline constraint scheduling.
.sp 2

.SH Syntax

.sp 2
\fRIGNORE_DEADLINE=Y\fR
.SH Description

.sp 2
If set to Y, LSF ignores deadline constraint scheduling and
starts all jobs regardless of deadline constraints.
.sp 2

.ce 1000
\fBIMPT_JOBBKLG\fR
.ce 0

.sp 2
Specifies the MultiCluster pending job limit for a receive-jobs
queue. Used only with the MultiCluster job forwarding model.
.sp 2

.SH Syntax

.sp 2
\fRIMPT_JOBBKLG=\fR\fIinteger\fR |\fRinfinit\fR
.SH Description

.sp 2
This parameter represents the maximum number of MultiCluster jobs
that can be pending in the queue; once the limit has been
reached, the queue stops accepting jobs from remote clusters.
.sp 2
Use the keyword infinit to make the queue accept an unlimited
number of pending MultiCluster jobs.
.SH Default

.sp 2
50
.sp 2

.ce 1000
\fBIMPT_TASKBKLG\fR
.ce 0

.sp 2
Specifies the MultiCluster pending job task limit for a
receive-jobs queue. Used only with the MultiCluster job
forwarding model.
.sp 2

.SH Syntax

.sp 2
\fRIMPT_TASKBKLG=\fR\fIinteger\fR |\fRinfinit\fR
.SH Description

.sp 2
In the submission cluster, if the total of requested job tasks
and the number of imported pending tasks in the receiving queue
is greater than \fBIMPT_TASKBKLG\fR, the queue stops accepting
jobs from remote clusters, and the job is not forwarded to the
receiving queue.
.sp 2
Specify an integer between 0 and 2147483646 for the number of
tasks.
.sp 2
Use the keyword infinit to make the queue accept an unlimited
number of pending MultiCluster job tasks.
.sp 2
Set \fBIMPT_TASKBKLG\fR to 0 to forbid any job being forwarded to
the receiving queue.
.sp 2
\fBNote: \fR\fBIMPT_SLOTBKLG\fR has been changed to
\fBIMPT_TASKBKLG\fR and the concept has changed from slot to task
as of LSF 9.1.3,
.SH Default

.sp 2
infinit (The queue accepts an unlimited number of pending
MultiCluster job tasks.)
.sp 2

.ce 1000
\fBINTERACTIVE\fR
.ce 0

.sp 2
Specifies whether the queue accepts interactive or
non-interactive jobs.
.sp 2

.SH Syntax

.sp 2
\fRINTERACTIVE=YES\fR | \fRNO\fR | \fRONLY\fR
.SH Description

.sp 2
If set to YES, causes the queue to accept both interactive and
non-interactive batch jobs.
.sp 2
If set to NO, causes the queue to reject interactive batch jobs.
.sp 2
If set to ONLY, causes the queue to accept interactive batch jobs
and reject non-interactive batch jobs.
.sp 2
Interactive batch jobs are submitted via bsub -I.
.SH Default

.sp 2
YES. The queue accepts both interactive and non-interactive jobs.
.sp 2

.ce 1000
\fBINTERRUPTIBLE_BACKFILL\fR
.ce 0

.sp 2
Configures interruptible backfill scheduling policy, which allows
reserved job slots to be used by low priority small jobs that are
terminated when the higher priority large jobs are about to
start.
.sp 2

.SH Syntax

.sp 2
\fRINTERRUPTIBLE_BACKFILL=\fR\fIseconds\fR
.SH Description

.sp 2
There can only be one interruptible backfill queue.It should be
the lowest priority queue in the cluster.
.sp 2
Specify the minimum number of seconds for the job to be
considered for backfilling.This minimal time slice depends on the
specific job properties; it must be longer than at least one
useful iteration of the job. Multiple queues may be created if a
site has jobs of distinctively different classes.
.sp 2
An interruptible backfill job:
.sp 2
*  Starts as a regular job and is killed when it exceeds the
   queue runtime limit, or
.sp 2
*  Is started for backfill whenever there is a backfill time
   slice longer than the specified minimal time, and killed
   before the slot-reservation job is about to start
.sp 2
The queue RUNLIMIT corresponds to a maximum time slice for
backfill, and should be configured so that the wait period for
the new jobs submitted to the queue is acceptable to users. 10
minutes of runtime is a common value.
.sp 2
You should configure REQUEUE_EXIT_VALUES for interruptible
backfill queues.
.sp 2
BACKFILL and RUNLIMIT must be configured in the queue. The queue
is disabled if BACKFILL and RUNLIMIT are not configured.
.SH Assumptions and limitations:

.sp 2
*  The interruptible backfill job holds the slot-reserving job
   start until its calculated start time, in the same way as a
   regular backfill job. The interruptible backfill job are not
   preempted in any way other than being killed when its time
   come.
.sp 2
*  While the queue is checked for the consistency of
   interruptible backfill, backfill and runtime specifications,
   the requeue exit value clause is not verified, nor executed
   automatically. Configure requeue exit values according to your
   site policies.
.sp 2
*  The interruptible backfill job must be able to do at least one
   unit of useful calculations and save its data within the
   minimal time slice, and be able to continue its calculations
   after it has been restarted
.sp 2
*  Interruptible backfill paradigm does not explicitly prohibit
   running parallel jobs, distributed across multiple nodes;
   however, the chance of success of such job is close to zero.
.SH Default

.sp 2
Not defined. No interruptible backfilling.
.sp 2

.ce 1000
\fBJOB_ACCEPT_INTERVAL\fR
.ce 0

.sp 2
Specifies the amount of time to wait after dispatching a job to a
host before dispatching a second job to the same host.
.sp 2

.SH Syntax

.sp 2
\fRJOB_ACCEPT_INTERVAL=\fR\fIinteger\fR
.SH Description

.sp 2
This parameter value is multiplied by the value of lsb.params
MBD_SLEEP_TIME (60 seconds by default). The result of the
calculation is the number of seconds to wait after dispatching a
job to a host before dispatching a second job to the same host.
.sp 2
If set to 0 (zero), a host may accept more than one job in each
dispatch turn. By default, there is no limit to the total number
of jobs that can run on a host, so if this parameter is set to 0,
a very large number of jobs might be dispatched to a host all at
once. This can overload your system to the point that it is
unable to create any more processes. It is not recommended to set
this parameter to 0.
.sp 2
JOB_ACCEPT_INTERVAL set at the queue level (lsb.queues) overrides
JOB_ACCEPT_INTERVAL set at the cluster level (lsb.params).
.sp 2
\fBNote: \fR
.sp 2
The parameter JOB_ACCEPT_INTERVAL only applies when there are
running jobs on a host. A host running a short job which finishes
before JOB_ACCEPT_INTERVAL has elapsed is free to accept a new
job without waiting.
.SH Default

.sp 2
Not defined. The queue uses JOB_ACCEPT_INTERVAL defined in
lsb.params, which has a default value of 1.
.sp 2

.ce 1000
\fBJOB_ACTION_WARNING_TIME\fR
.ce 0

.sp 2
Specifies the amount of time before a job control action occurs
that a job warning action is to be taken.
.sp 2

.SH Syntax

.sp 2
\fRJOB_ACTION_WARNING_TIME=\fR[\fIhour\fR\fR:\fR]\fIminute\fR
.SH Description

.sp 2
Job action warning time is not normalized.
.sp 2
A job action warning time must be specified with a job warning
action (the \fBJOB_WARNING_ACTION\fR parameter) in order for job
warning to take effect.
.sp 2
The warning time specified by the bsub -wt option overrides
\fBJOB_ACTION_WARNING_TIME\fR in the queue.
\fBJOB_ACTION_WARNING_TIME\fR is used as the default when no
command line option is specified.
.SH Example

.sp 2
JOB_ACTION_WARNING_TIME=2
.br
JOB_WARNING_ACTION=URG
.sp 2
2 minutes before the job reaches runtime limit or termination
deadline, or the queue\(aqs run window is closed, an URG signal is
sent to the job.
.SH Default

.sp 2
Not defined
.sp 2

.ce 1000
\fBJOB_CONTROLS\fR
.ce 0

.sp 2
Changes the behavior of the SUSPEND, RESUME, and TERMINATE
actions in LSF.
.sp 2

.SH Syntax

.sp 2
\fRJOB_CONTROLS=SUSPEND[\fR\fIsignal\fR | \fIcommand\fR |
\fRCHKPNT]\fR \fRRESUME[\fR\fIsignal\fR | \fIcommand\fR\fR]
TERMINATE[\fR\fIsignal\fR | \fIcommand\fR | \fRCHKPNT]\fR
.sp 2
*  \fIsignal\fR is a UNIX signal name (for example, SIGTSTP or
   SIGTERM). The specified signal is sent to the job. The same
   set of signals is not supported on all UNIX systems. To
   display a list of the symbolic names of the signals (without
   the SIG prefix) supported on your system, use the kill -l
   command.
.sp 2
*  \fIcommand\fR specifies a /bin/sh command line to be invoked.
.sp 2
   \fBRestriction: \fR
.sp 2
   Do not quote the command line inside an action definition. Do
   not specify a signal followed by an action that triggers the
   same signal. For example, do not specify
   \fRJOB_CONTROLS=TERMINATE[bkill]\fR or
   \fRJOB_CONTROLS=TERMINATE[brequeue]\fR. This causes a deadlock
   between the signal and the action.
.sp 2
*  CHKPNT is a special action, which causes the system to
   checkpoint the job. Only valid for SUSPEND and TERMINATE
   actions:
.sp 2
   *  If the SUSPEND action is CHKPNT, the job is checkpointed
      and then stopped by sending the SIGSTOP signal to the job
      automatically.
.sp 2
   *  If the TERMINATE action is CHKPNT, then the job is
      checkpointed and killed automatically.
.SH Description

.sp 2
*  The contents of the configuration line for the action are run
   with \fR/bin/sh -c\fR so you can use shell features in the
   command.
.sp 2
*  The standard input, output, and error of the command are
   redirected to the NULL device, so you cannot tell directly
   whether the command runs correctly. The default null device on
   UNIX is /dev/null.
.sp 2
*  The command is run as the user of the job.
.sp 2
*  All environment variables set for the job are also set for the
   command action. The following additional environment variables
   are set:
.sp 2
   *  LSB_JOBPGIDS: a list of current process group IDs of the
      job
.sp 2
   *  LSB_JOBPIDS: a list of current process IDs of the job
.sp 2
*  For the SUSPEND action command, the following environment
   variables are also set:
.sp 2
   *  LSB_SUSP_REASONS - an integer representing a bitmap of
      suspending reasons as defined in lsbatch.h. The suspending
      reason can allow the command to take different actions
      based on the reason for suspending the job.
.sp 2
   *  LSB_SUSP_SUBREASONS - an integer representing the load
      index that caused the job to be suspended. When the
      suspending reason SUSP_LOAD_REASON (suspended by load) is
      set in LSB_SUSP_REASONS, LSB_SUSP_SUBREASONS set to one of
      the load index values defined in lsf.h. Use
      LSB_SUSP_REASONS and LSB_SUSP_SUBREASONS together in your
      custom job control to determine the exact load threshold
      that caused a job to be suspended.
.sp 2
*  If an additional action is necessary for the SUSPEND command,
   that action should also send the appropriate signal to the
   application. Otherwise, a job can continue to run even after
   being suspended by LSF. For example,
   \fRJOB_CONTROLS=SUSPEND[kill $LSB_JOBPIDS;
   \fR\fIcommand\fR\fR]\fR
.sp 2
*  If the job control command fails, LSF retains the original job
   status.
.sp 2
*  If you set preemption with the signal SIGTSTP you use IBM
   Spectrum LSF License Scheduler, define
   \fBLIC_SCHED_PREEMPT_STOP=Y\fR in lsf.conf for License
   Scheduler preemption to work.
.sp 2
\fBNote: \fRWhen you use blaunch to run parallel jobs on multiple
hosts, job control actions defined in \fBJOB_CONTROLS\fR in
lsb.queues only take effect on the first execution host. Job
control actions defined in the queue do no affect tasks running
on other hosts. If \fBJOB_CONTROLS\fR is defined, the default job
control signals of LSF (SUSPEND, RESUME, TERMINATE) do not reach
each task on each execution host.
.SH Default

.sp 2
On UNIX, by default, SUSPEND sends SIGTSTP for parallel or
interactive jobs and SIGSTOP for other jobs. RESUME sends
SIGCONT. TERMINATE sends SIGINT, SIGTERM and SIGKILL in that
order.
.sp 2
On Windows, actions equivalent to the UNIX signals have been
implemented to do the default job control actions. Job control
messages replace the SIGINT and SIGTERM signals, but only
customized applications are able to process them. Termination is
implemented by the \fRTerminateProcess( )\fR system call.
.sp 2

.ce 1000
\fBJOB_IDLE\fR
.ce 0

.sp 2
Specifies a threshold for idle job exception handling.
.sp 2

.SH Syntax

.sp 2
\fRJOB_IDLE=\fR\fInumber\fR
.SH Description

.sp 2
The value should be a number between 0.0 and 1.0 representing CPU
time/runtime. If the job idle factor is less than the specified
threshold, LSF invokes LSF_SERVERDIR/eadmin to trigger the action
for a job idle exception.
.sp 2
The minimum job run time before mbatchd reports that the job is
idle is defined as DETECT_IDLE_JOB_AFTER in lsb.params.
.SH Valid values

.sp 2
Any positive number between 0.0 and 1.0
.SH Example

.sp 2
JOB_IDLE=0.10
.br

.sp 2
A job idle exception is triggered for jobs with an idle value
(CPU time/runtime) less than 0.10.
.SH Default

.sp 2
Not defined. No job idle exceptions are detected.
.sp 2

.ce 1000
\fBJOB_OVERRUN\fR
.ce 0

.sp 2
Specifies a threshold for job overrun exception handling.
.sp 2

.SH Syntax

.sp 2
\fRJOB_OVERRUN=\fR\fIrun_time\fR
.SH Description

.sp 2
If a job runs longer than the specified run time, LSF invokes
LSF_SERVERDIR/eadmin to trigger the action for a job overrun
exception.
.SH Example

.sp 2
JOB_OVERRUN=5
.br

.sp 2
A job overrun exception is triggered for jobs running longer than
5 minutes.
.SH Default

.sp 2
Not defined. No job overrun exceptions are detected.
.sp 2

.ce 1000
\fBJOB_SIZE_LIST\fR
.ce 0

.sp 2
Specifies a list of job sizes (number of tasks) that are allowed
on this queue.
.sp 2

.SH Syntax

.sp 2
\fRJOB_SIZE_LIST=\fR\fIdefault_size\fR\fR\fR[\fIsize\fR ...]
.SH Description

.sp 2
When submitting a job or modifying a pending job that requests a
job size by using the -n or -R options for bsub and bmod, the
requested job size must be a single fixed value that matches one
of the values that \fBJOB_SIZE_LIST\fR specifies, which are the
job sizes that are allowed on this queue. LSF rejects the job if
the requested job size is not in this list. In addition, when
using bswitch to switch a pending job with a requested job size
to another queue, the requested job size in the pending job must
also match one of the values in \fBJOB_SIZE_LIST\fR for the new
queue.
.sp 2
The first value in this list is the default job size, which is
the assigned job size request if the job was submitted without
requesting one. The remaining values are the other job sizes
allowed in the queue, and may be defined in any order.
.sp 2
When defined in both a queue and an application profile
(lsb.applications), the job size request must satisfy both
requirements. In addition, \fBJOB_SIZE_LIST\fR overrides any
\fBTASKLIMIT\fR parameters defined at the same level. Job size
requirements do not apply to queues and application profiles with
no job size lists, nor do they apply to other levels of job
submissions (that is, host level or cluster level job
submissions).
.sp 2
\fBNote: \fRAn exclusive job may allocate more slots on the host
then is required by the tasks. For example, if
\fRJOB_SIZE_LIST=8\fR and an exclusive job requesting -n8 runs on
a 16 slot host, all 16 slots are assigned to the job. The job
runs as expected, since the 8 tasks specified for the job matches
the job size list.
.SH Valid values

.sp 2
A space-separated list of positive integers between 1 and
2147483646.
.SH Default

.sp 2
Undefined
.sp 2

.ce 1000
\fBJOB_STARTER\fR
.ce 0

.sp 2
Creates a specific environment for submitted jobs prior to
execution.
.sp 2

.SH Syntax

.sp 2
\fRJOB_STARTER=\fR\fIstarter\fR [\fIstarter\fR] [\fR"%USRCMD"\fR]
[\fIstarter\fR]
.SH Description

.sp 2
\fIstarter\fR is any executable that can be used to start the job
(i.e., can accept the job as an input argument). Optionally,
additional strings can be specified.
.sp 2
By default, the user commands run after the job starter. A
special string, %USRCMD, can be used to represent the position of
the user’s job in the job starter command line. The %USRCMD
string and any additional commands must be enclosed in quotation
marks (\fR" "\fR).
.sp 2
If your job starter script runs on a Windows execution host and
includes symbols (like & or |), you can use the
\fBJOB_STARTER_EXTEND=preservestarter\fR parameter in lsf.conf
and set \fBJOB_STARTER=preservestarter\fR in lsb.queues. A
customized \fBuserstarter\fR can also be used.
.SH Example

.sp 2
JOB_STARTER=csh -c "%USRCMD;sleep 10"
.br

.sp 2
In this case, if a user submits a job
.sp 2
% bsub myjob arguments
.br

.sp 2
the command that actually runs is:
.sp 2
% csh -c "myjob arguments;sleep 10"
.br

.SH Default

.sp 2
Not defined. No job starter is used.
.sp 2

.ce 1000
\fBJOB_UNDERRUN\fR
.ce 0

.sp 2
Specifies a threshold for job underrun exception handling.
.sp 2

.SH Syntax

.sp 2
JOB_UNDERRUN=\fR\fR\fIrun_time\fR
.SH Description

.sp 2
If a job exits before the specified number of minutes, LSF
invokes LSF_SERVERDIR/eadmin to trigger the action for a job
underrun exception.
.SH Example

.sp 2
JOB_UNDERRUN=2
.br

.sp 2
A job underrun exception is triggered for jobs running less than
2 minutes.
.SH Default

.sp 2
Not defined. No job underrun exceptions are detected.
.sp 2

.ce 1000
\fBJOB_WARNING_ACTION\fR
.ce 0

.sp 2
Specifies the job action to be taken before a job control action
occurs.
.sp 2

.SH Syntax

.sp 2
\fRJOB_WARNING_ACTION=\fR\fIsignal\fR
.SH Description

.sp 2
A job warning action must be specified with a job action warning
time (the \fBJOB_ACTION_WARNING_TIME\fR parameter) in order for
job warning to take effect.
.sp 2
If \fBJOB_WARNING_ACTION\fR is specified, LSF sends the warning
action to the job before the actual control action is taken. This
allows the job time to save its result before being terminated by
the job control action.
.sp 2
The warning action specified by the bsub -wa option overrides
\fBJOB_WARNING_ACTION\fR in the queue. \fBJOB_WARNING_ACTION\fR
is used as the default when no command line option is specified.
.SH Example

.sp 2
JOB_ACTION_WARNING_TIME=2
.br
JOB_WARNING_ACTION=URG
.sp 2
2 minutes before the job reaches runtime limit or termination
deadline, or the queue\(aqs run window is closed, an URG signal is
sent to the job.
.SH Default

.sp 2
Not defined
.sp 2

.ce 1000
\fBLOAD_INDEX\fR
.ce 0

.sp 2
Specifies scheduling and suspending thresholds for the specified
dynamic load index.
.sp 2

.SH Syntax

.sp 2
\fIload_index\fR\fR=\fR\fIloadSched\fR[\fR/\fR\fIloadStop\fR]
.sp 2
Specify \fRio\fR, \fRit\fR, \fRls\fR, \fRmem\fR, \fRpg\fR,
\fRr15s\fR, \fRr1m\fR, \fRr15m\fR, \fRswp\fR, \fRtmp\fR,
\fRut\fR, or a non-shared custom external load index. Specify
multiple lines to configure thresholds for multiple load indices.
.sp 2
Specify \fRio\fR, \fRit\fR, \fRls\fR, \fRmem\fR, \fRpg\fR,
\fRr15s\fR, \fRr1m\fR, \fRr15m\fR, \fRswp\fR, \fRtmp\fR,
\fRut\fR, or a non-shared custom external load index as a column.
Specify multiple columns to configure thresholds for multiple
load indices.
.SH Description

.sp 2
The \fRloadSched\fR condition must be satisfied before a job is
dispatched to the host. If a RESUME_COND is not specified, the
\fRloadSched\fR condition must also be satisfied before a
suspended job can be resumed.
.sp 2
If the \fRloadStop\fR condition is satisfied, a job on the host
is suspended.
.sp 2
The \fRloadSched\fR and \fRloadStop\fR thresholds permit the
specification of conditions using simple AND/OR logic. Any load
index that does not have a configured threshold has no effect on
job scheduling.
.sp 2
LSF does not suspend a job if the job is the only batch job
running on the host and the machine is interactively idle
(\fRit\fR>0).
.sp 2
The \fRr15s\fR, \fRr1m\fR, and \fRr15m\fR CPU run queue length
conditions are compared to the effective queue length as reported
by lsload -E, which is normalized for multiprocessor hosts.
Thresholds for these parameters should be set at appropriate
levels for single processor hosts.
.SH Example

.sp 2
MEM=100/10 
.br

.br
SWAP=200/30
.br

.sp 2
These two lines translate into a \fRloadSched\fR condition of
.sp 2
mem>=100 && swap>=200 
.br

.sp 2
and a \fRloadStop\fR condition of
.sp 2
mem < 10 || swap < 30
.br

.SH Default

.sp 2
Not defined
.sp 2

.ce 1000
\fBLOCAL_MAX_PREEXEC_RETRY\fR
.ce 0

.sp 2
Specifies the maximum number of times to attempt the
pre-execution command of a job on the local cluster.
.sp 2

.SH Syntax

.sp 2
\fRLOCAL_MAX_PREEXEC_RETRY=\fR\fIinteger\fR
.SH Description

.sp 2
When this limit is reached, the default behavior of the job is
defined by the \fBLOCAL_MAX_PREEXEC_RETRY_ACTION\fR parameter in
lsb.params, lsb.queues, or lsb.applications.
.SH Valid values

.sp 2
0 < MAX_PREEXEC_RETRY < INFINIT_INT
.sp 2
INFINIT_INT is defined in lsf.h.
.SH Default

.sp 2
Not defined. The number of preexec retry times is unlimited
.SH See also

.sp 2
\fBLOCAL_MAX_PREEXEC_RETRY_ACTION\fR in lsb.params, lsb.queues,
and lsb.applications.
.sp 2

.ce 1000
\fBLOCAL_MAX_PREEXEC_RETRY_ACTION\fR
.ce 0

.sp 2
The default behavior of a job when it reaches the maximum number
of times to attempt its pre-execution command on the local
cluster.
.sp 2

.SH Syntax

.sp 2
\fRLOCAL_MAX_PREEXEC_RETRY_ACTION=SUSPEND\fR | \fREXIT\fR
.SH Description

.sp 2
This parameter specifies the default behavior of a job when it
reaches the maximum number of times to attempt its pre-execution
command on the local cluster (as specified by
\fBLOCAL_MAX_PREEXEC_RETRY\fR in lsb.params, lsb.queues, or
lsb.applications).
.sp 2
*  If set to \fRSUSPEND\fR, the local or leased job is suspended
   and its status is set to PSUSP
.sp 2
*  If set to \fREXIT\fR, the local or leased job exits and its
   status is set to EXIT. The job exits with the same exit code
   as the last pre-execution fail exit code.
.sp 2
This parameter is configured cluster-wide (lsb.params), at the
queue level (lsb.queues), and at the application level
(lsb.applications). The action specified in lsb.applications
overrides lsb.queues, and lsb.queues overrides the lsb.params
configuration.
.SH Default

.sp 2
Not defined. If not defined in lsb.params, the default action is
SUSPEND.
.SH See also

.sp 2
\fBLOCAL_MAX_PREEXEC_RETRY\fR in lsb.params, lsb.queues, and
lsb.applications.
.sp 2

.ce 1000
\fBMANDATORY_EXTSCHED\fR
.ce 0

.sp 2
Specifies mandatory external scheduling options for the queue.
.sp 2

.SH Syntax

.sp 2
\fRMANDATORY_EXTSCHED=\fR\fIexternal_scheduler_options\fR
.SH Description

.sp 2
-extsched options on the bsub command are merged with
MANDATORY_EXTSCHED options, and MANDATORY_EXTSCHED options
override any conflicting job-level options set by -extsched.
.SH Default

.sp 2
Not defined
.sp 2

.ce 1000
\fBMAX_JOB_PREEMPT\fR
.ce 0

.sp 2
Specifies the maximum number of times a job can be preempted.
Applies to queue-based preemption only.
.sp 2

.SH Syntax

.sp 2
\fRMAX_JOB_PREEMPT=\fR\fIinteger\fR
.SH Valid values

.sp 2
0 < MAX_JOB_PREEMPT < INFINIT_INT
.sp 2
INFINIT_INT is defined in lsf.h.
.SH Default

.sp 2
Not defined. The number of preemption times is unlimited.
.sp 2

.ce 1000
\fBMAX_JOB_REQUEUE\fR
.ce 0

.sp 2
Specifies the maximum number of times to requeue a job
automatically.
.sp 2

.SH Syntax

.sp 2
\fRMAX_JOB_REQUEUE=\fR\fIinteger\fR
.SH Valid values

.sp 2
0 < MAX_JOB_REQUEUE < INFINIT_INT
.sp 2
INFINIT_INT is defined in lsf.h.
.SH Default

.sp 2
Not defined. The number of requeue times is unlimited
.sp 2

.ce 1000
\fBMAX_PREEXEC_RETRY\fR
.ce 0

.sp 2
Use REMOTE_MAX_PREEXEC_RETRY instead. This parameter is
maintained for backwards compatibility. The maximum number of
times to attempt the pre-execution command of a job from a remote
cluster. Used only with the MultiCluster job forwarding model.
.sp 2

.SH Syntax

.sp 2
\fRMAX_PREEXEC_RETRY=\fR\fIinteger\fR
.SH Description

.sp 2

.sp 2

.sp 2
If the job\(aqs pre-execution command fails all attempts, the job is
returned to the submission cluster.
.SH Valid values

.sp 2
0 < MAX_PREEXEC_RETRY < INFINIT_INT
.sp 2
INFINIT_INT is defined in lsf.h.
.SH Default

.sp 2
5
.sp 2

.ce 1000
\fBMAX_PROTOCOL_INSTANCES\fR
.ce 0

.sp 2
For LSF IBM Parallel Environment (PE) integration. Specifies the
number of parallel communication paths (windows) available to the
protocol on each network.
.sp 2

.SH Syntax

.sp 2
\fRMAX_PROTOCOL_INSTANCES=\fIinteger\fR\fR
.SH Description

.sp 2
\fBNote: \fRThis parameter is deprecated and might be removed in
a future version of LSF.
.sp 2
If number of windows specified for the job (with the instances
option of bsub -network or the NETWORK_REQ parameter in
lsb.queues or lsb.applications), or it is greater than the
specified maximum value, LSF rejects the job.
.sp 2
Specify MAX_PROTOCOL_INSTANCES in a queue (lsb.queues) or
cluster-wide in lsb.params. The value specified in a queue
overrides the value specified in lsb.params.
.sp 2
LSF_PE_NETWORK_NUM must be defined to a non-zero value in
lsf.conf for MAX_PROTOCOL_INSTANCES to take effect and for LSF to
run PE jobs. If LSF_PE_NETWORK_NUM is not defined or is set to 0,
the value of MAX_PROTOCOL_INSTANCES is ignored with a warning
message.
.sp 2
For best performance, set MAX_PROTOCOL_INSTANCES so that the
communication subsystem uses every available adapter before it
reuses any of the adapters.
.SH Default

.sp 2
No default value
.sp 2

.ce 1000
\fBMAX_RSCHED_TIME \fR
.ce 0

.sp 2
Determines how long a MultiCluster job stays pending in the
execution cluster before returning to the submission cluster.
Used only with the MultiCluster job forwarding model.
.sp 2

.SH Syntax

.sp 2
\fRMAX_RSCHED_TIME=\fR\fIinteger\fR | \fRinfinit\fR
.SH Description

.sp 2
The remote timeout limit in seconds is:
.sp 2
MAX_RSCHED_TIME * MBD_SLEEP_TIME=timeout
.br

.sp 2
Specify infinit to disable remote timeout (jobs always get
dispatched in the correct FCFS order because MultiCluster jobs
never get rescheduled, but MultiCluster jobs can be pending in
the receive-jobs queue forever instead of being rescheduled to a
better queue).
.sp 2
\fBNote: \fR
.sp 2
apply to the queue in the submission cluster (only). This
parameter is ignored by the receiving queue.
.sp 2
Remote timeout limit never affects advance reservation jobs
.sp 2
Jobs that use an advance reservation always behave as if remote
timeout is disabled.
.SH Default

.sp 2
20 (20 minutes by default)
.sp 2

.ce 1000
\fBMAX_SLOTS_IN_POOL\fR
.ce 0

.sp 2
Queue-based fairshare only. Specifies the maximum number of job
slots available in the slot pool the queue belongs to for queue
based fairshare.
.sp 2

.SH Syntax

.sp 2
\fRMAX_SLOTS_IN_POOL=\fR\fIinteger\fR
.SH Description

.sp 2
\fBNote: \fRThis parameter is deprecated and might be removed in
a future version of LSF.
.sp 2
Defined in the first queue of the slot pool. Definitions in
subsequent queues have no effect.
.sp 2
When defined together with other slot limits (\fBQJOB_LIMIT\fR,
\fBHJOB_LIMIT\fR or \fBUJOB_LIMIT\fR in lsb.queues or queue
limits in lsb.resources) the lowest limit defined applies.
.sp 2
When \fBMAX_SLOTS_IN_POOL\fR, \fBSLOT_RESERVE\fR, and
\fBBACKFILL\fR are defined for the same queue, jobs in the queue
cannot backfill using slots reserved by other jobs in the same
queue.
.SH Valid values

.sp 2
MAX_SLOTS_IN_POOL can be any number from 0 to INFINIT_INT, where
INFINIT_INT is defined in lsf.h.
.SH Default

.sp 2
Not defined
.sp 2

.ce 1000
\fBMAX_TOTAL_TIME_PREEMPT\fR
.ce 0

.sp 2
Specifies the accumulated preemption time after which a job
cannot be preempted again.
.sp 2

.SH Syntax

.sp 2
\fRMAX_TOTAL_TIME_PREEMPT=\fR\fIminutes\fR
.sp 2
where \fIminutes\fR is wall-clock time, not normalized time.
.SH Description

.sp 2
Setting the parameter of the same name in lsb.applications
overrides this parameter; setting this parameter overrides the
parameter of the same name in lsb.params.
.SH Valid values

.sp 2
Any positive integer greater than or equal to one (1)
.SH Default

.sp 2
Unlimited
.sp 2

.ce 1000
\fBMC_FORWARD_DELAY\fR
.ce 0

.sp 2
When using the LSF multicluster capability, specifies the job
forwarding behavior and the amount of time after job submission
and scheduling for LSF revert to the default job forwarding
behavior.
.sp 2

.SH Syntax

.sp 2
\fRMC_FORWARD_DELAY=\fR[\fR-\fR]\fIseconds\fR
.SH Description

.sp 2
If this value is positive, LSF does not forward the job to a
remote cluster and only attempts the local hosts for the
specified amount of time in seconds.
.sp 2
If this value is negative, LSF forwards the job to a remote
cluster and does not attempt the local hosts for the specified
amount of time in seconds.
.sp 2
This specified delay time starts after LSF submitted and
scheduled the job. After this amount of time, this parameter no
longer takes effect and LSF reverts to the default job forwarding
behavior, which is to attempt the local hosts first, then forward
the job to a remote cluster if this failed.
.sp 2
LSF repeats these steps if LSF recalled the job from remote
clusters due to the \fBMAX_RSCHED_TIME\fR parameter setting. LSF
also repeats these steps if LSF requeued, suspended, or resumed
the job, or if the scheduler daemon restarts.
.SH Valid values

.sp 2
Any positive or negative integer. If set to 0, this parameter is
disabled.
.SH Default

.sp 2
0. This parameter is disabled.
.sp 2

.ce 1000
\fBMEMLIMIT\fR
.ce 0

.sp 2
Specifies the per-process resident size limit for all job
processes from this queue.
.sp 2

.SH Syntax

.sp 2
\fRMEMLIMIT=\fR[\fIdefault_limit\fR] \fImaximum_limit\fR
.SH Description

.sp 2
Set this parameter to place a per-process hard process resident
set size limit, in KB, for all of the processes belonging to a
job from this queue (see getrlimit(2)).
.sp 2
Sets the maximum amount of physical memory (resident set size,
RSS) that may be allocated to a process.
.sp 2
By default, if a default memory limit is specified, jobs
submitted to the queue without a job-level memory limit are
killed when the default memory limit is reached.
.sp 2
If you specify only one limit, it is the maximum, or hard, memory
limit. If you specify two limits, the first one is the default,
or soft, memory limit, and the second one is the maximum memory
limit.
.sp 2
LSF has two methods of enforcing memory usage:
.sp 2
*  OS Memory Limit Enforcement
.sp 2
*  LSF Memory Limit Enforcement
.SH OS memory limit enforcement

.sp 2
OS memory limit enforcement is the default MEMLIMIT behavior and
does not require further configuration. OS enforcement usually
allows the process to eventually run to completion. LSF passes
MEMLIMIT to the OS that uses it as a guide for the system
scheduler and memory allocator. The system may allocate more
memory to a process if there is a surplus. When memory is low,
the system takes memory from and lowers the scheduling priority
(re-nice) of a process that has exceeded its declared MEMLIMIT.
Only available on systems that support RLIMIT_RSS for
setrlimit().
.sp 2
Not supported on:
.sp 2
*  Sun Solaris 2.x
.sp 2
*  Windows
.SH LSF memory limit enforcement

.sp 2
To enable LSF memory limit enforcement, set LSB_MEMLIMIT_ENFORCE
in lsf.conf to y. LSF memory limit enforcement explicitly sends a
signal to kill a running process once it has allocated memory
past MEMLIMIT.
.sp 2
You can also enable LSF memory limit enforcement by setting
LSB_JOB_MEMLIMIT in lsf.conf to y. The difference between
LSB_JOB_MEMLIMIT set to y and LSB_MEMLIMIT_ENFORCE set to y is
that with LSB_JOB_MEMLIMIT, only the per-job memory limit
enforced by LSF is enabled. The per-process memory limit enforced
by the OS is disabled. With LSB_MEMLIMIT_ENFORCE set to y, both
the per-job memory limit enforced by LSF and the per-process
memory limit enforced by the OS are enabled.
.sp 2
Available for all systems on which LSF collects total memory
usage.
.SH Example

.sp 2
The following configuration defines a queue with a memory limit
of 5000 KB:
.sp 2
Begin Queue 
.br

.sp 2
QUEUE_NAME  = default 
.br

.sp 2
DESCRIPTION = Queue with memory limit of 5000 kbytes 
.br

.sp 2
MEMLIMIT    = 5000 
.br

.sp 2
End Queue
.br

.SH Default

.sp 2
Unlimited
.sp 2

.ce 1000
\fBMIG\fR
.ce 0

.sp 2
Enables automatic job migration and specifies the migration
threshold for checkpointable or rerunnable jobs.
.sp 2

.SH Syntax

.sp 2
\fRMIG=\fR\fIminutes\fR
.SH Description

.sp 2
LSF automatically migrates jobs that have been in the SSUSP state
for more than the specified number of minutes. Specify a value of
0 to migrate jobs immediately upon suspension. The migration
threshold applies to all jobs running on the host.
.sp 2
Job-level command line migration threshold overrides threshold
configuration in application profile and queue. Application
profile configuration overrides queue level configuration.
.sp 2
When a host migration threshold is specified, and is lower than
the value for the job, the queue, or the application, the host
value is used..
.sp 2
Members of a chunk job can be migrated. Chunk jobs in WAIT state
are removed from the job chunk and put into PEND state.
.sp 2
Does not affect MultiCluster jobs that are forwarded to a remote
cluster.
.SH Default

.sp 2
Not defined. LSF does not migrate checkpointable or rerunnable
jobs automatically.
.sp 2

.ce 1000
\fBNETWORK_REQ\fR
.ce 0

.sp 2
For LSF IBM Parallel Environment (PE) integration. Specifies the
network resource requirements for a PE job.
.sp 2

.SH Syntax

.sp 2
\fBNETWORK_REQ="\fRnetwork_res_req\fB"\fR
.sp 2
\fInetwork_res_req\fR has the following syntax:
.sp 2
[\fBtype=sn_all\fR | \fBsn_single\fR]
[\fB:protocol=\fRprotocol_name[\fB(\fRprotocol_number\fB)\fR][\fB,\fRprotocol_name[\fB(\fRprotocol_number\fB)\fR]]
[\fB:mode=US\fR | \fBIP\fR] [\fB:usage=dedicated\fR |
\fBshared\fR] [\fB:instance=\fRpositive_integer]
.SH Description

.sp 2
\fBNote: \fRThis parameter is deprecated and might be removed in
a future version of LSF.
.sp 2
If any network resource requirement is specified in the job,
queue, or application profile, the job is treated as a PE job. PE
jobs can only run on hosts where IBM PE pnsd daemon is running.
.sp 2
The network resource requirement string \fInetwork_res_req\fR has
the same syntax as the bsub -network option.
.sp 2
The -network bsub option overrides the value of NETWORK_REQ
defined in lsb.queues or lsb.applications. The value of
NETWORK_REQ defined in lsb.applications overrides queue-level
NETWORK_REQ defined in lsb.queues.
.sp 2
The following IBM LoadLeveller job command file options are not
supported in LSF:
.sp 2
*  collective_groups
.sp 2
*  imm_send_buffers
.sp 2
*  rcxtblocks
.sp 2
The following network resource requirement options are supported:
.sp 2
\fBtype=sn_all | sn_single\fR
.br
         Specifies the adapter device type to use for message
         passing: either sn_all or sn_single.
.sp 2
         \fBsn_single\fR
.br
                  When used for switch adapters, specifies that
                  all windows are on a single network
.sp 2
         \fBsn_all\fR
.br
                  Specifies that one or more windows are on each
                  network, and that striped communication should
                  be used over all available switch networks. The
                  networks specified must be accessible by all
                  hosts selected to run the PE job. See the
                  Parallel Environment Runtime Edition for AIX:
                  Operation and Use guide (SC23-6781-05) for more
                  information about submitting jobs that use
                  striping.
.sp 2
         If mode is IP and type is specified as sn_all or
         sn_single, the job will only run on InfiniBand (IB)
         adapters (IPoIB). If mode is IP and type is not
         specified, the job will only run on Ethernet adapters
         (IPoEth). For IPoEth jobs, LSF ensures the job is
         running on hosts where pnsd is installed and running.
         For IPoIB jobs, LSF ensures the job the job is running
         on hosts where pnsd is installed and running, and that
         IB networks are up. Because IP jobs do not consume
         network windows, LSF does not check if all network
         windows are used up or the network is already occupied
         by a dedicated PE job.
.sp 2
         Equivalent to the PE MP_EUIDEVICE environment variable
         and -euidevice PE flag See the Parallel Environment
         Runtime Edition for AIX: Operation and Use guide
         (SC23-6781-05) for more information. Only sn_all or
         sn_single are supported by LSF. The other types
         supported by PE are not supported for LSF jobs.
.sp 2
\fBprotocol=\fIprotocol_name\fB[(\fIprotocol_number\fB)]\fR
.br
         Network communication protocol for the PE job,
         indicating which message passing API is being used by
         the application. The following protocols are supported
         by LSF:
.sp 2
         \fBmpi\fR
.br
                  The application makes only MPI calls. This
                  value applies to any MPI job regardless of the
                  library that it was compiled with (PE MPI,
                  MPICH2).
.sp 2
         \fBpami\fR
.br
                  The application makes only PAMI calls.
.sp 2
         \fBlapi\fR
.br
                  The application makes only LAPI calls.
.sp 2
         \fBshmem\fR
.br
                  The application makes only OpenSHMEM calls.
.sp 2
         \fB\fIuser_defined_parallel_api\fB\fR
.br
                  The application makes only calls from a
                  parallel API that you define. For example:
                  protocol=myAPI or protocol=charm.
.sp 2
         The default value is mpi.
.sp 2
         LSF also supports an optional \fIprotocol_number\fR (for
         example, \fRmpi(2)\fR, which specifies the number of
         contexts (endpoints) per parallel API instance. The
         number must be a power of 2, but no greater than 128 (1,
         2, 4, 8, 16, 32, 64, 128). LSF will pass the
         communication protocols to PE without any change. LSF
         will reserve network windows for each protocol.
.sp 2
         When you specify multiple parallel API protocols, you
         cannot make calls to both LAPI and PAMI (lapi, pami) or
         LAPI and OpenSHMEM (lapi, shmem) in the same
         application. Protocols can be specified in any order.
.sp 2
         See the MP_MSG_API and MP_ENDPOINTS environment
         variables and the -msg_api and -endpoints PE flags in
         the Parallel Environment Runtime Edition for AIX:
         Operation and Use guide (SC23-6781-05) for more
         information about the communication protocols that are
         supported by IBM Parallel Edition.
.sp 2
\fBmode=US | IP\fR
.br
         The network communication system mode used by the
         communication specified communication protocol: US (User
         Space) or IP (Internet Protocol). A US job can only run
         with adapters that support user space communications,
         such as the IB adapter. IP jobs can run with either
         Ethernet adapters or IB adapters. When IP mode is
         specified, the instance number cannot be specified, and
         network usage must be unspecified or shared.
.sp 2
         Each instance on the US mode requested by a task running
         on switch adapters requires and adapter window. For
         example, if a task requests both the MPI and LAPI
         protocols such that both protocol instances require US
         mode, two adapter windows will be used.
.sp 2
         The default value is US.
.sp 2
\fBusage=dedicated | shared\fR
.br
         Specifies whether the adapter can be shared with tasks
         of other job steps: dedicated or shared. Multiple tasks
         of the same job can share one network even if usage is
         dedicated.
.sp 2
         The default usage is shared.
.sp 2
\fBinstances=\fIpositive_integer\fB\fR
.br
         The number of parallel communication paths (windows) per
         task made available to the protocol on each network. The
         number actually used depends on the implementation of
         the protocol subsystem.
.sp 2
         The default value is 1.
.sp 2
         If the specified value is greater than
         MAX_PROTOCOL_INSTANCES in lsb.params or lsb.queues, LSF
         rejects the job.
.sp 2
LSF_PE_NETWORK_NUM must be defined to a non-zero value in
lsf.conf for NETWORK_REQ to take effect. If LSF_PE_NETWORK_NUM is
not defined or is set to 0, NETWORK_REQ is ignored with a warning
message.
.SH Example

.sp 2
The following network resource requirement string specifies that
the requirements for an sn_all job (one or more windows are on
each network, and striped communication should be used over all
available switch networks). The PE job uses MPI API calls
(protocol), runs in user-space network communication system mode,
and requires 1 parallel communication path (window) per task.
.sp 2
\fRNETWORK_REQ = "protocol=mpi:mode=us:instance=1:type=sn_all"\fR
.SH Default

.sp 2
No default value, but if you specify no value
(\fRNETWORK_REQ=""\fR), the job uses the following:
protocol=mpi:mode=US:usage=shared:instance=1 in the queue.
.sp 2

.ce 1000
\fBNEW_JOB_SCHED_DELAY\fR
.ce 0

.sp 2
Specifies the number of seconds that a new job waits, before
being scheduled.
.sp 2

.SH Syntax

.sp 2
\fRNEW_JOB_SCHED_DELAY=\fR\fIseconds\fR
.SH Description

.sp 2
A value of zero (0) means the job is scheduled without any delay.
The scheduler still periodically fetches jobs from mbatchd. Once
it gets jobs, scheduler schedules them without any delay. This
may speed up job scheduling a bit, but it also generates some
communication overhead. Therefore, you should only set it to 0
for high priority, urgent or interactive queues for a small
workloads.
.sp 2
If \fBNEW_JOB_SCHED_DELAY\fR is set to a non-zero value,
scheduler will periodically fetch new jobs from mbatchd, after
which it sets job scheduling time to job submission time +
\fBNEW_JOB_SCHED_DELAY\fR.
.SH Default

.sp 2
0 seconds
.sp 2

.ce 1000
\fBNICE\fR
.ce 0

.sp 2
Adjusts the UNIX scheduling priority at which jobs from this
queue execute.
.sp 2

.SH Syntax

.sp 2
\fRNICE=\fR\fIinteger\fR
.SH Description

.sp 2
The default value of 0 (zero) maintains the default scheduling
priority for UNIX interactive jobs. This value adjusts the
run-time priorities for batch jobs on a queue-by-queue basis, to
control their effect on other batch or interactive jobs. See the
\fRnice\fR(1) manual page for more details.
.sp 2
On Windows, this value is mapped to Windows process priority
classes as follows:
.sp 2
*  \fRnice>=0\fR corresponds to an priority class of \fRIDLE\fR
.sp 2
*  \fRnice<0\fR corresponds to an priority class of \fRNORMAL\fR
.sp 2
LSF on Windows does not support \fRHIGH\fR or \fRREAL-TIME\fR
priority classes.
.sp 2
This value is overwritten by the \fBNICE\fR setting in
lsb.applications, if defined.
.SH Default

.sp 2
0 (zero)
.sp 2

.ce 1000
\fBNO_PREEMPT_INTERVAL\fR
.ce 0

.sp 2
Specifies the number of minutes a preemptable job can run before
it is preempted. If the uninterrupted run time of a preemptable
job is longer than the specified time, it can be preempted.
.sp 2

.SH Syntax

.sp 2
\fBNO_PREEMPT_INTERVAL\fR=minutes
.sp 2
The value of \fIminutes\fR is wall-clock time, not normalized
time.
.SH Description

.sp 2
The \fRNO_PREEMPT_INTERVAL=0\fR parameter allows immediate
preemption of jobs as soon as they start or resume running.
.sp 2
For example, if a job \fRA\fR needs to preempt other candidate
preemptable jobs\fRB\fR, \fRC\fR, and \fRD\fR, the
\fBNO_PREEMPT_INTERVAL\fR parameter determines which job is
preempted:
.sp 2
*  Run time of job \fRB\fR and job \fRC\fR is less than the
   \fBNO_PREEMPT_INTERVAL\fR parameter: job \fRB\fR and \fRC\fR
   are not preempted.
.sp 2
*  Run time of job \fRD\fR is greater than or equal to the
   \fBNO_PREEMPT_INTERVAL\fR parameter: job \fRD\fR is preempted.
.sp 2
The parameter of the same name in the lsb.applications file
overrides this parameter. This parameter overrides the parameter
of the same name in the lsb.params file.
.SH Default

.sp 2
0
.sp 2

.ce 1000
\fBPEND_TIME_LIMIT\fR
.ce 0

.sp 2
Specifies the pending time limit for a job.
.sp 2

.SH Syntax

.sp 2
\fRPEND_TIME_LIMIT=\fR[\fIhour\fR\fR:\fR]\fIminute\fR
.SH Description

.sp 2
LSF sends the queue-level pending time limit configuration to IBM
Spectrum LSF RTM (LSF RTM), which handles the alarm and triggered
actions such as user notification (for example, notifying the
user that submitted the job and the LSF administrator) and job
control actions (for example, killing the job). LSF RTM compares
the job\(aqs pending time to the pending time limit, and if the job
is pending for longer than this specified time limit, LSF RTM
triggers the alarm and actions. This parameter works without LSF
RTM, but LSF does not take any other alarm actions.
.sp 2
In MultiCluster job forwarding mode, the job\(aqs pending time limit
is ignored in the execution cluster, while the submission cluster
merges the job\(aqs queue-, application-, and job-level pending time
limit according to local settings.
.sp 2
The pending time limit is in the form of
[\fIhour\fR\fR:\fR]\fIminute\fR. The minutes can be specified as
a number greater than 59. For example, three and a half hours can
either be specified as 3:30, or 210.
.sp 2
The job-level pending time limit (bsub -ptl) overrides the
application-level limit (\fBPEND_TIME_LIMIT\fR in
lsb.applications), and the application-level limit overrides the
queue-level limit specified here.
.SH Default

.sp 2
Not defined.
.sp 2

.ce 1000
\fBPJOB_LIMIT\fR
.ce 0

.sp 2
Specifies the per-processor job slot limit for the queue.
.sp 2

.SH Syntax

.sp 2
\fRPJOB_LIMIT=\fR\fIfloat\fR
.SH Description

.sp 2
Maximum number of job slots that this queue can use on any
processor. This limit is configured per processor, so that
multiprocessor hosts automatically run more jobs.
.SH Default

.sp 2
Unlimited
.sp 2

.ce 1000
\fBPLAN\fR
.ce 0

.sp 2
For use when the \fBALLOCATION_PLANNER\fR parameter is enabled.
Used to identify the jobs that are candidates for planning.
.sp 2

.SH Syntax

.sp 2
\fRPLAN = Y | N | "<\fIkey\fR>[\fIvalue\fR] ..."\fR
.SH Description

.sp 2
LSF requires that the \fBALLOCATION_PLANNER\fR parameter is
enabled in order to use \fBPLAN=Y\fR.
.sp 2
Also defined at the cluster and application levels. The
precedence is: application, queue, global. For example,
application level setting overrides the queue level setting.
.sp 2
The following key-value pairs are supported:
.sp 2
\fBTable 1. Key-Value pairs for PLAN\fR
.sp 2
+---------------+---------------+---------------+---------------+
| key           | value         | Default       | Description   |
+---------------+---------------+---------------+---------------+
| DELAY         | positive      | -             | Number of     |
|               | integer       |               | minutes to    |
|               |               |               | delay before  |
|               |               |               | considering   |
|               |               |               | making a plan |
|               |               |               | for a job     |
|               |               |               | following the |
|               |               |               | job\(aqs         |
|               |               |               | submission    |
|               |               |               | time.         |
+---------------+---------------+---------------+---------------+
| MAX_JOBS      | positive      | -             | Maximum       |
|               | integer       |               | number of     |
|               |               |               | jobs that can |
|               |               |               | have a plan   |
|               |               |               | concurrently. |
+---------------+---------------+---------------+---------------+
.sp 2
\fBNote: \fR
.sp 2
The \fBPLAN\fR parameter replaces the existing \fBSLOT_RESERVE\fR
parameter and \fBRESOURCE_RESERVE\fR parameter when the
\fBALLOCATION_PLANNER\fR parameter is enabled.
.SH Default

.sp 2
N
.sp 2

.ce 1000
\fBPOST_EXEC\fR
.ce 0

.sp 2
Enables post-execution processing at the queue level.
.sp 2

.SH Syntax

.sp 2
\fBPOST_EXEC=\fRcommand
.SH Description

.sp 2
The \fBPOST_EXEC\fR command runs on the execution host after the
job finishes. Post-execution commands can be configured at the
application and queue levels. Application-level post-execution
commands run \fIbefore\fR queue-level post-execution commands.
.sp 2
The \fBPOST_EXEC\fR command uses the same environment variable
values as the job, and, by default, runs under the user account
of the user who submits the job. To run post-execution commands
under a different user account (such as root for privileged
operations), configure the parameter \fBLSB_PRE_POST_EXEC_USER\fR
in lsf.sudoers.
.sp 2
When a job exits with one of the queue’s
\fBREQUEUE_EXIT_VALUES\fR, LSF requeues the job and sets the
environment variable \fBLSB_JOBPEND\fR. The post-execution
command runs after the requeued job finishes.
.sp 2
When the post-execution command is run, the environment variable
\fBLSB_JOBEXIT_STAT\fR is set to the exit status of the job. If
the execution environment for the job cannot be set up,
\fBLSB_JOBEXIT_STAT\fR is set to 0 (zero).
.sp 2
The command path can contain up to 4094 characters for UNIX and
Linux, or up to 255 characters for Windows, including the
directory, file name, and expanded values for %J \fI(job_ID\fR)
and %I (\fIindex_ID\fR).
.sp 2
For UNIX:
.sp 2
*  The pre- and post-execution commands run in the /tmp directory
   under /bin/sh -c, which allows the use of shell features in
   the commands. The following example shows valid configuration
   lines:
.sp 2
   PRE_EXEC= /usr/share/lsf/misc/testq_pre >> /tmp/pre.out
.br

.sp 2
   POST_EXEC= /usr/share/lsf/misc/testq_post | grep -v "Hey!"
.br

.sp 2
*  LSF sets the \fBPATH\fR environment variable to
.sp 2
   PATH=\(aq/bin /usr/bin /sbin /usr/sbin\(aq
.br

.sp 2
*  The stdin, stdout, and stderr are set to /dev/null
.sp 2
*  To allow UNIX users to define their own post-execution
   commands, an LSF administrator specifies the environment
   variable \fB$USER_POSTEXEC\fR as the \fBPOST_EXEC\fR command.
   A user then defines the post-execution command:
.sp 2
   setenv USER_POSTEXEC /path_name
.br

.sp 2
   \fBNote: \fRThe path name for the post-execution command must
   be an absolute path. Do not define
   \fBPOST_EXEC\fR=$USER_POSTEXEC when
   \fBLSB_PRE_POST_EXEC_USER\fR=root. This parameter cannot be
   used to configure host-based post-execution processing.
.sp 2
For Windows:
.sp 2
*  The pre- and post-execution commands run under cmd.exe /c
.sp 2
*  The standard input, standard output, and standard error are
   set to NULL
.sp 2
*  The \fBPATH\fR is determined by the setup of the LSF service
.sp 2
\fBNote: \fR
.sp 2
For post-execution commands that execute on a Windows Server
2003, x64 Edition platform, users must have read and execute
privileges for cmd.exe.
.SH Default

.sp 2
Not defined. No post-execution commands are associated with the
queue.
.sp 2

.ce 1000
\fBPRE_EXEC\fR
.ce 0

.sp 2
Enables pre-execution processing at the queue level.
.sp 2

.SH Syntax

.sp 2
\fBPRE_EXEC=\fRcommand
.SH Description

.sp 2
The \fBPRE_EXEC\fR command runs on the execution host before the
job starts. If the \fBPRE_EXEC\fR command exits with a non-zero
exit code, LSF requeues the job to the front of the queue.
.sp 2
Pre-execution commands can be configured at the queue,
application, and job levels and run in the following order:
.sp 2
1. The queue-level command
.sp 2
2. The application-level or job-level command. If you specify a
   command at both the application and job levels, the job-level
   command overrides the application-level command; the
   application-level command is ignored.
.sp 2
The \fBPRE_EXEC\fR command uses the same environment variable
values as the job, and runs under the user account of the user
who submits the job. To run pre-execution commands under a
different user account (such as root for privileged operations),
configure the parameter \fBLSB_PRE_POST_EXEC_USER\fR in
lsf.sudoers.
.sp 2
The command path can contain up to 4094 characters for UNIX and
Linux, or up to 255 characters for Windows, including the
directory, file name, and expanded values for %J \fI(job_ID\fR)
and %I (\fIindex_ID\fR).
.sp 2
For UNIX:
.sp 2
*  The pre- and post-execution commands run in the /tmp directory
   under /bin/sh -c, which allows the use of shell features in
   the commands. The following example shows valid configuration
   lines:
.sp 2
   PRE_EXEC= /usr/share/lsf/misc/testq_pre >> /tmp/pre.out
.br

.sp 2
   POST_EXEC= /usr/share/lsf/misc/testq_post | grep -v "Hey!"
.br

.sp 2
*  LSF sets the \fBPATH\fR environment variable to
.sp 2
   PATH=\(aq/bin /usr/bin /sbin /usr/sbin\(aq
.br

.sp 2
*  The stdin, stdout, and stderr are set to /dev/null
.sp 2
For Windows:
.sp 2
*  The pre- and post-execution commands run under cmd.exe /c
.sp 2
*  The standard input, standard output, and standard error are
   set to NULL
.sp 2
*  The \fBPATH\fR is determined by the setup of the LSF Service
.sp 2
\fBNote: \fR
.sp 2
For pre-execution commands that execute on a Windows Server 2003,
x64 Edition platform, users must have read and execute privileges
for cmd.exe. This parameter cannot be used to configure
host-based pre-execution processing.
.SH Default

.sp 2
Not defined. No pre-execution commands are associated with the
queue.
.sp 2

.ce 1000
\fBPREEMPTION\fR
.ce 0

.sp 2
Enables preemptive scheduling and defines this queue as
preemptive or preemptable.
.sp 2

.SH Syntax

.sp 2
\fBPREEMPTION=PREEMPTIVE\fR[\fB[\fRlow_queue_name[\fB+\fRpref_level]...\fB]\fR]
\fBPREEMPTION=\fR\fBPREEMPTABLE\fR[\fB[\fRhi_queue_name...\fB]\fR]
\fBPREEMPTION=PREEMPTIVE\fR[\fB[\fRlow_queue_name[\fB+\fRpref_level]...\fB]\fR]
\fBPREEMPTABLE\fR[\fB[\fRhi_queue_name...\fB]\fR]
.SH Description

.sp 2
\fBPREEMPTIVE\fR
.br
         Enables preemptive scheduling and defines this queue as
         preemptive. Jobs in this queue preempt jobs from the
         specified lower-priority queues or from all
         lower-priority queues if the parameter is specified with
         no queue names. PREEMPTIVE can be combined with
         PREEMPTABLE to specify that jobs in this queue can
         preempt jobs in lower-priority queues, and can be
         preempted by jobs in higher-priority queues.
.sp 2
\fBPREEMPTABLE\fR
.br
         Enables preemptive scheduling and defines this queue as
         preemptable. Jobs in this queue can be preempted by jobs
         from specified higher-priority queues, or from all
         higher-priority queues, even if the higher-priority
         queues are not preemptive. PREEMPTIVE can be combined
         with PREEMPTIVE to specify that jobs in this queue can
         be preempted by jobs in higher-priority queues, and can
         preempt jobs in lower-priority queues.
.sp 2
\fB\fIlow_queue_name\fB \fR
.br
         Specifies the names of lower-priority queues that can be
         preempted.
.sp 2
         To specify multiple queues, separate the queue names
         with a space, and enclose the list in a single set of
         square brackets.
.sp 2
\fB\fR+\fB\fIpref_level\fB\fR
.br
         Specifies to preempt this queue before preempting other
         queues. When multiple queues are indicated with a
         preference level, an order of preference is indicated:
         queues with higher relative preference levels are
         preempted before queues with lower relative preference
         levels set.
.sp 2
\fB\fIhi_queue_name\fB \fR
.br
         Specifies the names of higher-priority queues that can
         preempt jobs in this queue.
.sp 2
         To specify multiple queues, separate the queue names
         with a space and enclose the list in a single set of
         square brackets.
.SH Example: configure selective, ordered preemption across queues

.sp 2
The following example defines four queues, as follows:
.sp 2
*  high
.sp 2
   *  Has the highest relative priority of 99
.sp 2
   *  Jobs from this queue can preempt jobs from all other queues
.sp 2
*  medium
.sp 2
   *  Has the second-highest relative priority at 10
.sp 2
   *  Jobs from this queue can preempt jobs from \fRnormal\fR and
      \fRlow\fR queues, beginning with jobs from \fRlow\fR, as
      indicated by the preference (+1)
.sp 2
*  normal
.sp 2
   *  Has the second-lowest relative priority, at 5
.sp 2
   *  Jobs from this queue can preempt jobs from \fRlow\fR, and
      can be preempted by jobs from both \fRhigh\fR and
      \fRmedium\fR queues
.sp 2
*  low
.sp 2
   *  Has the lowest relative priority, which is also the default
      priority, at 1
.sp 2
   *  Jobs from this queue can be preempted by jobs from all
      preemptive queues, even though it does not have the
      PREEMPTABLE keyword set
.sp 2
Begin Queue
.br
QUEUE_NAME=high
.br
PREEMPTION=PREEMPTIVE
.br
PRIORITY=99
.br
End Queue
.sp 2
Begin Queue
.br
QUEUE_NAME=medium
.br
PREEMPTION=PREEMPTIVE[normal low+1]
.br
PRIORITY=10
.br
End Queue
.sp 2
Begin Queue
.br
QUEUE_NAME=normal
.br
PREEMPTION=PREEMPTIVE[low]
.br
PREEMPTABLE[high medium]
.br
PRIORITY=5
.br
End Queue
.sp 2
Begin Queue
.br
QUEUE_NAME=low
.br
PRIORITY=1
.br
End Queue
.sp 2

.ce 1000
\fBPREEMPT_DELAY\fR
.ce 0

.sp 2
Preemptive jobs will wait the specified number of seconds from
the submission time before preempting any low priority
preemptable jobs.
.sp 2

.SH Syntax

.sp 2
\fRPREEMPT_DELAY=\fR\fIseconds\fR
.SH Description

.sp 2
During the grace period, preemption will not be trigged, but the
job can be scheduled and dispatched by other scheduling policies.
.sp 2
This feature can provide flexibility to tune the system to reduce
the number of preemptions. It is useful to get better performance
and job throughput. When the low priority jobs are short, if high
priority jobs can wait a while for the low priority jobs to
finish, preemption can be avoided and cluster performance is
improved. If the job is still pending after the grace period has
expired, the preemption will be triggered.
.sp 2
The waiting time is for preemptive jobs in the pending status
only. It will not impact the preemptive jobs that are suspended.
.sp 2
The time is counted from the submission time of the jobs. The
submission time means the time mbatchd accepts a job, which
includes newly submitted jobs, restarted jobs (by brestart) or
forwarded jobs from a remote cluster.
.sp 2
When the preemptive job is waiting, the pending reason is:
.sp 2
The preemptive job is allowing a grace period before preemption.
.sp 2
If you use an older version of bjobs, the pending reason is:
.sp 2
Unknown pending reason code <6701>;
.sp 2
The parameter is defined in lsb.params, lsb.queues (overrides
lsb.params), and lsb.applications (overrides both lsb.params and
lsb.queues).
.sp 2
Run badmin reconfig to make your changes take effect.
.SH Default

.sp 2
Not defined (if the parameter is not defined anywhere, preemption
is immediate).
.sp 2

.ce 1000
\fBPRIORITY\fR
.ce 0

.sp 2
Specifies the relative queue priority for dispatching jobs. A
higher value indicates a higher job-dispatching priority,
relative to other queues.
.sp 2

.SH Syntax

.sp 2
\fBPRIORITY\fR=integer
.SH Description

.sp 2

.sp 2
LSF schedules jobs from one queue at a time, starting with the
highest-priority queue. If multiple queues have the same
priority, LSF schedules all the jobs from these queues in
first-come, first-served order.
.sp 2
LSF queue priority is independent of the UNIX scheduler priority
system for time-sharing processes. In LSF, the NICE parameter is
used to set the UNIX time-sharing priority for batch jobs.
.sp 2
\fB\fBinteger\fB\fR
.br
         Specify a number greater than or equal to 1, where 1 is
         the lowest priority.
.SH Default

.sp 2
1
.sp 2

.ce 1000
\fBPROCESSLIMIT\fR
.ce 0

.sp 2
Limits the number of concurrent processes that can be part of a
job.
.sp 2

.SH Syntax

.sp 2
\fRPROCESSLIMIT=[\fR\fIdefault_limit\fR] \fImaximum_limit\fR
.SH Description

.sp 2
By default, if a default process limit is specified, jobs
submitted to the queue without a job-level process limit are
killed when the default process limit is reached.
.sp 2
If you specify only one limit, it is the maximum, or hard,
process limit. If you specify two limits, the first one is the
default, or soft, process limit, and the second one is the
maximum process limit.
.SH Default

.sp 2
Unlimited
.sp 2

.ce 1000
\fBQJOB_LIMIT\fR
.ce 0

.sp 2
Specifies the job slot limit for the queue.
.sp 2

.SH Syntax

.sp 2
\fRQJOB_LIMIT=\fR\fIinteger\fR
.SH Description

.sp 2
This parameter specifies the total number of job slots that this
queue can use.
.SH Default

.sp 2
Unlimited
.sp 2

.ce 1000
\fBQUEUE_GROUP\fR
.ce 0

.sp 2
Configures absolute priority scheduling (APS) across multiple
queues.
.sp 2

.SH Syntax

.sp 2
\fRQUEUE_GROUP=\fR\fIqueue1\fR, \fIqueue2\fR ...
.SH Description

.sp 2
When APS is enabled in the queue with APS_PRIORITY, the
FAIRSHARE_QUEUES parameter is ignored. The QUEUE_GROUP parameter
replaces FAIRSHARE_QUEUES, which is obsolete in LSF 7.0.
.SH Default

.sp 2
Not defined
.sp 2

.ce 1000
\fBQUEUE_NAME\fR
.ce 0

.sp 2
Required. Specifies the name of the queue.
.sp 2

.SH Syntax

.sp 2
\fRQUEUE_NAME=\fR\fIstring\fR
.SH Description

.sp 2
Specify any ASCII string up to 59 characters long. You can use
letters, digits, underscores (_) or dashes (-). You cannot use
blank spaces. You cannot specify the reserved name \fRdefault\fR.
.SH Default

.sp 2
You must specify this parameter to define a queue. The default
queue automatically created by LSF is named \fRdefault\fR.
.sp 2

.ce 1000
\fBRC_ACCOUNT\fR
.ce 0

.sp 2
Assigns an account name (tag) to hosts borrowed through LSF
resource connector, so that they cannot be used by other user
groups, users, or jobs.
.sp 2

.SH Syntax

.sp 2
\fRRC_ACCOUNT=\fR\fIaccount_name\fR
.SH Description

.sp 2
When a job is submitted to a queue with the \fBRC_ACCOUNT\fR
parameter specified, hosts borrowed to run the job are tagged
with the value of the \fBRC_ACCOUNT\fR parameter. The borrowed
host cannot be used by other queues that have a different value
for the \fBRC_ACCOUNT\fR parameter (or that don\(aqt have the
\fBRC_ACCOUNT\fR parameter defined).
.sp 2
After the borrowed host joins the cluster, use the lshosts -s
command to view the value of the \fBRC_ACCOUNT\fR parameter for
the host.
.SH Example

.sp 2
RC_ACCOUNT=project1
.SH Default

.sp 2
The string "default" - Meaning, no account is defined for the
queue.
.sp 2

.ce 1000
\fBRC_DEMAND_POLICY\fR
.ce 0

.sp 2
Defines threshold conditions for the determination of whether
demand is triggered to borrow resources through resource
connector for all the jobs in a queue. As long as pending jobs at
the queue meet at least one threshold condition, LSF expresses
the demand to resource connector to trigger borrowing.
.sp 2

.SH Syntax

.sp 2
\fRRC_DEMAND_POLICY = THRESHOLD[ [\fR
\fInum_pend_jobs\fR[,\fIduration\fR]\fR]\fR … \fR]\fR
.SH Description

.sp 2
The demand policy defined by the \fBRC_DEMAND_POLICY\fR parameter
can contain multiple conditions, in an OR relationship. A
condition is defined as \fR[\fR
\fInum_pend_jobs\fR[,\fIduration\fR]\fR]\fR. The queue has more
than the specified number of eligible pending jobs that are
expected to run at least the specified duration in minutes. The
\fInum_pend_jobs\fR option is required, and the duration is
optional. The default duration is 0 minutes.
.sp 2
LSF considers eligible pending jobs for the policy. An ineligible
pending job (for example, a job dependency is not satisfied yet)
keeps pending even thought hosts are available. The policy counts
a job for eligibility no matter how many tasks or slots the job
requires. Each job element is counted as a job. Pending demand
for a resizable job is not counted, though LSF can allocate
borrowed resources to the resizable job.
.sp 2
LSF evaluates the policies at each demand calculation cycle, and
accumulates duration if the \fInum_pend_jobs\fR option is
satisfied. The mbschd daemon resets the duration of the condition
when it restarts or if the condition has not been evaluated in
the past 2 minutes. For example, if no pending jobs are in the
cluster, for 2 minutes, mbschd stops evaluating them.
.SH Example

.sp 2
In the following example, LSF calculates demand if the queue has
5 or more pending jobs in past 10 minutes, or 1 or more pending
jobs in past 60 minutes, or 100 or more pending jobs.
.sp 2
RC_DEMAND_POLICY = THRESHOLD[ [ 5, 10] [1, 60] [100] ]
.SH Default

.sp 2
Not defined for the queue
.sp 2

.ce 1000
\fBRC_HOSTS\fR
.ce 0

.sp 2
Enables LSF resource connector to borrow specific host types from
a resource provider.
.sp 2

.SH Syntax

.sp 2
\fRRC_HOSTS=\fIstring\fR\fR
.sp 2
\fRRC_HOSTS = none\fR | \fRall\fR | \fIhost_type\fR
[\fIhost_type\fR ...]
.SH Description

.sp 2
The \fIhost_type\fR flag is a Boolean resource that is a member
of the list of host resources that are defined in the
\fBLSB_RC_EXTERNAL_HOST_FLAG\fR parameter in the lsf.conf file.
.sp 2
If the \fBRC_HOSTS\fR parameter is not defined in the queue, its
default value is none. Borrowing is disabled for any queue that
explicitly defines \fBRC_HOSTS=none\fR, even if the
\fBLSB_RC_EXTERNAL_HOST_FLAG\fR parameter is defined in the
lsf.conf file.
.sp 2
If the \fBRC_HOSTS\fR parameter is not defined in any queue,
borrowing cannot happen for any job.
.sp 2
\fBNote: \fRThe \fBHOSTS\fR parameter in the lsb.queues file and
the bsub -m option do not apply to hosts that are managed through
the resource connector. To specify the resource connector host
types that can be used by a queue, you must specify the
\fBRC_HOSTS\fR parameter in that queue.
.SH Example

.sp 2
RC_HOSTS=awshost
.SH Default

.sp 2
none - host borrowing from resource providers is disabled, and no
borrowed hosts can be used by the queue.
.sp 2

.ce 1000
\fBRCVJOBS_FROM\fR
.ce 0

.sp 2
Defines a receive-jobs queue for LSF multicluster capability.
.sp 2

.SH Syntax

.sp 2
\fRRCVJOBS_FROM=\fR\fIcluster_name\fR ... | \fRallclusters\fR
.SH Description

.sp 2
Specify cluster names, separated by a space. The administrator of
each remote cluster determines which queues in that cluster
forward jobs to the local cluster.
.sp 2
If you enabled an LSF data manager data transfer queue as a
remote send-jobs queue in the execution cluster (that is, if you
added a queue from the submission cluster to the \fBSNDJOBS_TO\fR
parameter in the lsb.queues file in the execution cluster), you
must include the execution cluster in the \fBRCVJOBS_FROM\fR
parameter in the corresponding submission cluster.
.sp 2
Use the keyword \fRallclusters\fR to specify any remote cluster.
.SH Example

.sp 2
The following queue accepts remote jobs from clusters 2, 4, and
6.
.sp 2
RCVJOBS_FROM=cluster2 cluster4 cluster6
.br

.SH Example for LSF data managerLSF data managerdata manager

.sp 2
If the execution cluster \fRclusterE\fR has the LSF data manager
transfer queue \fRdata_transfer\fR set as a remote send-jobs
queue to the \fRreceive_q\fR queue in the submission cluster
\fRclusterS\fR, according to the following configuration for the
\fRclusterE\fR cluster:
.sp 2
Begin Queue
.br
QUEUE_NAME=data_transfer
.br
DATA_TRANSFER=Y
.br
SNDJOBS_TO=receive_q@clusterS
.br
HOSTS=hostS1 hostS2 # Transfer nodes in the execution cluster
.br
End Queue
.sp 2
You must define the \fBRCVJOBS_FROM\fR parameter for the
\fRreceive_q\fR queue in the submission cluster \fRclusterS\fR to
accept jobs from (and push data to) the execution cluster
\fRclusterE\fR, as shown in the following configuration for the
\fRclusterS\fR cluster:
.sp 2
Begin Queue
.br
QUEUE_NAME=receive_q
.br
RCVJOBS_FROM=clusterE
.br
PRIORITY=40
.br
HOSTS=hostS1 hostS2 # Transfer nodes in the submission cluster
.br
RES_REQ=select[type==any]
.br
End Queue
.sp 2
Alternatively, you can define \fRRCVJOBS_FROM=allclusters\fR to
accept jobs from all clusters, which includes the execution
cluster.
.sp 2

.ce 1000
\fBRELAX_JOB_DISPATCH_ORDER\fR
.ce 0

.sp 2
Allows LSF to deviate from standard job prioritization policies
to improve cluster utilization.
.sp 2

.SH Syntax

.sp 2
\fRRELAX_JOB_DISPATCH_ORDER=Y\fR | \fRy\fR | \fRN\fR | \fRn\fR |
\fRALLOC_REUSE_DURATION[\fR[\fImin\fR] \fImax\fR\fR]\fR
[\fRSHARE[\fR[\fRuser\fR] [\fRgroup\fR] [\fRproject\fR]\fR]\fR]
.SH Description

.sp 2
When this parameter is enabled, LSF allows multiple jobs with
common resource requirements to run consecutively on the same
allocation. Whenever a job finishes, LSF attempts to quickly
replace it with a pending job that has the same resource
requirements. To ensure that limits are not violated, LSF selects
pending jobs that belong to the same user and have other
attributes in common.
.sp 2
Since LSF bypasses most of the standard scheduling logic between
jobs, reusing resource allocation can help improve cluster
utilization. This improvement is most evident in clusters with
several shorter jobs (that is, jobs that run from a few seconds
to several minutes).
.sp 2
To ensure that the standard job prioritization policies are
approximated, there is a limit on the length of time that each
allocation is reusable. LSF automatically sets this time limit to
achieve a high level of resource utilization. By default, this
reuse time cannot exceed 30 minutes. If you specify a maximum
reuse time and an optional minimum reuse time (by using
\fBALLOC_REUSE_DURATION\fR), LSF adjusts the time limit within
this specified range to achieve the highest level of resource
utilization.
.sp 2
Use the \fBSHARE[]\fR keyword to further relax the constraints of
what types of pending jobs can reuse the resource allocation of a
finished job. This allows more jobs to reuse the resource
allocation, but might result in resource limits and policies
being temporarily violated because these limits and policies are
relaxed. The \fBSHARE[]\fR keyword specifies constraints that the
mbatchd daemon no longer has to apply when determining which
pending jobs can reuse the resource allocation of a finished job.
If a job is finished and LSF does not find any pending jobs with
the same user or other attributes in common, LSF considers the
specifications in the \fBSHARE[]\fR keyword. Specify one or more
of the following keywords within \fBSHARE[]\fR for LSF to also
consider the following pending jobs:
.sp 2
\fBuser\fR
.br
         Pending jobs that do not have the same job owner as the
         finished job.
.sp 2
\fBgroup\fR
.br
         Pending jobs that are not associated with the same
         fairshare group (bsub -G command option) as the finished
         job.
.sp 2
\fBproject\fR
.br
         Pending jobs that are not assigned to the same project
         (bsub -P command option) as the finished job.
.sp 2
If using the LSF multicluster capability, \fBSHARE[]\fR applies
only to the job forward mode.
.sp 2
If undefined, the cluster-wide value from the lsb.params
parameter of the same name is used.
.SH Examples

.sp 2
*  
   RELAX_JOB_DISPATCH_ORDER=Y
.br

.sp 2
   The resource allocation of a finished job can be reused from 0
   to 30 minutes.
.sp 2
*  
   RELAX_JOB_DISPATCH_ORDER=ALLOC_REUSE_DURATION[45]
.sp 2
   The resource allocation of a finished job can be reused from 0
   to 45 minutes.
.sp 2
*  
   RELAX_JOB_DISPATCH_ORDER=ALLOC_REUSE_DURATION[5 45]
.sp 2
   The resource allocation of a finished job can be reused from 5
   to 45 minutes.
.sp 2
*  
   RELAX_JOB_DISPATCH_ORDER=SHARE[user]
.sp 2
   The resource allocation of a finished job can be reused from 0
   to 30 minutes. If there are no pending jobs with the same
   common attributes, pending jobs that belong to different users
   can also reuse the resource allocation.
.sp 2
*  
   RELAX_JOB_DISPATCH_ORDER=SHARE[user group]
.sp 2
   The resource allocation of a finished job can be reused from 0
   to 30 minutes. If there are no pending jobs with the same
   common attributes, pending jobs that belong to different users
   and are associated with different fairshare groups can also
   reuse the resource allocation.
.sp 2
*  
   RELAX_JOB_DISPATCH_ORDER=ALLOC_REUSE_DURATION[45] SHARE[user group]
.sp 2
   The resource allocation of a finished job can be reused from 0
   to 45 minutes. If there are no pending jobs with the same
   common attributes, pending jobs that belong to different users
   and are associated with different fairshare groups can also
   reuse the resource allocation.
.SH Default

.sp 2
Not defined.
.sp 2

.ce 1000
\fBREMOTE_MAX_PREEXEC_RETRY\fR
.ce 0

.sp 2
Define the maximum number of times to attempt the pre-execution
command of a job from the remote cluster. Used only with the
MultiCluster job forwarding model.
.sp 2

.SH Syntax

.sp 2
\fRREMOTE_MAX_PREEXEC_RETRY=\fR\fIinteger\fR
.SH Description

.sp 2
This parameter applies to the execution cluster.
.SH Valid values

.sp 2
0 - INFINIT_INT
.sp 2
INFINIT_INT is defined in lsf.h.
.SH Default

.sp 2
5
.sp 2

.ce 1000
\fBREQUEUE_EXIT_VALUES\fR
.ce 0

.sp 2
Enables automatic job requeue and sets the LSB_EXIT_REQUEUE
environment variable.
.sp 2

.SH Syntax

.sp 2
\fRREQUEUE_EXIT_VALUES=\fR[\fIexit_code \fR...]
[\fREXCLUDE(\fR\fIexit_code ...\fR\fR)\fR]
.SH Description

.sp 2
Use spaces to separate multiple exit codes. Application-level
exit values override queue-level values. Job-level exit values
(bsub -Q) override application-level and queue-level values.
.sp 2
\fIexit_code\fR has the following form:
.sp 2
"[all] [~number ...] | [number ...]"
.br

.sp 2
The reserved keyword all specifies all exit codes. Exit codes are
typically between 0 and 255. Use a tilde (~) to exclude specified
exit codes from the list.
.sp 2
Jobs are requeued to the head of the queue. The output from the
failed run is not saved, and the user is not notified by LSF.
.sp 2
Define an exit code as EXCLUDE(\fIexit_code\fR) to enable
exclusive job requeue, ensuring the job does not rerun on the
samehost. Exclusive job requeue does not work for parallel jobs.
.sp 2
For MultiCluster jobs forwarded to a remote execution cluster,
the exit values specified in the submission cluster with the
EXCLUDE keyword are treated as if they were non-exclusive.
.sp 2
You can also requeue a job if the job is terminated by a signal.
.sp 2
If a job is killed by a signal, the exit value is
128+\fIsignal_value\fR. The sum of 128 and the signal value can
be used as the exit code in the parameter REQUEUE_EXIT_VALUES.
.sp 2
For example, if you want a job to rerun if it is killed with a
signal 9 (SIGKILL), the exit value would be 128+9=137. You can
configure the following requeue exit value to allow a job to be
requeue if it was kill by signal 9:
.sp 2
REQUEUE_EXIT_VALUES=137
.br

.sp 2
In Windows, if a job is killed by a signal, the exit value is
signal_value. The signal value can be used as the exit code in
the parameter REQUEUE_EXIT_VALUES.
.sp 2
For example, if you want to rerun a job after it was killed with
a signal 7 (SIGKILL), the exit value would be 7. You can
configure the following requeue exit value to allow a job to
requeue after it was killed by signal 7:
.sp 2
\fRREQUEUE_EXIT_VALUES=7\fR
.sp 2
You can configure the following requeue exit value to allow a job
to requeue for both Linux and Windows after it was killed:
.sp 2
\fRREQUEUE_EXIT_VALUES=137 7\fR
.sp 2
If mbatchd is restarted, it does not remember the previous hosts
from which the job exited with an exclusive requeue exit code. In
this situation, it is possible for a job to be dispatched to
hosts on which the job has previously exited with an exclusive
exit code.
.sp 2
You should configure REQUEUE_EXIT_VALUES for interruptible
backfill queues (INTERRUPTIBLE_BACKFILL=\fIseconds\fR).
.SH Example

.sp 2
REQUEUE_EXIT_VALUES=30 EXCLUDE(20)
.sp 2
means that jobs with exit code 30 are requeued, jobs with exit
code 20 are requeued exclusively, and jobs with any other exit
code are not requeued.
.SH Default

.sp 2
Not defined. Jobs are not requeued.
.sp 2

.ce 1000
\fBRERUNNABLE\fR
.ce 0

.sp 2
Enables automatic rerun for jobs from this queue.
.sp 2

.SH Syntax

.sp 2
\fRRERUNNABLE=yes\fR | \fRno\fR
.SH Description

.sp 2
If set to yes, enables automatic job rerun (restart).
.sp 2
Rerun is disabled when RERUNNABLE is set to no. The yes and no
arguments are not case sensitive.
.sp 2
For MultiCluster jobs, the setting in the submission queue is
used, and the setting in the execution queue is ignored.
.sp 2
Members of a chunk job can be rerunnable. If the execution host
becomes unavailable, rerunnable chunk job members are removed
from the job chunk and dispatched to a different execution host.
.SH Default

.sp 2
no
.sp 2

.ce 1000
\fBRESOURCE_RESERVE\fR
.ce 0

.sp 2
Enables processor reservation and memory reservation for pending
jobs for this queue.
.sp 2

.SH Syntax

.sp 2
\fRRESOURCE_RESERVE=MAX_RESERVE_TIME[\fR\fIinteger\fR\fR]\fR
.SH Description

.sp 2
Specifies the number of dispatch turns (MAX_RESERVE_TIME) over
which a job can reserve job slots and memory.
.sp 2
Overrides the SLOT_RESERVE parameter. If both RESOURCE_RESERVE
and SLOT_RESERVE are defined in the same queue, an error is
displayed when the cluster is reconfigured, and SLOT_RESERVE is
ignored. Job slot reservation for parallel jobs is enabled by
RESOURCE_RESERVE if the LSF scheduler plug-in module names for
both resource reservation and parallel batch jobs
(schmod_parallel and schmod_reserve) are configured in the
lsb.modules file: The schmod_parallel name \fImust\fR come before
schmod_reserve in lsb.modules.
.sp 2
If a job has not accumulated enough memory or job slots to start
by the time MAX_RESERVE_TIME expires, it releases all its
reserved job slots or memory so that other pending jobs can run.
After the reservation time expires, the job cannot reserve memory
or slots for one scheduling session, so other jobs have a chance
to be dispatched. After one scheduling session, the job can
reserve available memory and job slots again for another period
specified by MAX_RESERVE_TIME.
.sp 2
If BACKFILL is configured in a queue, and a run limit is
specified with -W on bsub or with RUNLIMIT in the queue, backfill
jobs can use the accumulated memory reserved by the other jobs in
the queue, as long as the backfill job can finish before the
predicted start time of the jobs with the reservation.
.sp 2
Unlike slot reservation, which only applies to parallel jobs,
memory reservation and backfill on memory apply to sequential and
parallel jobs.
.SH Example

.sp 2
RESOURCE_RESERVE=MAX_RESERVE_TIME[5]
.br

.sp 2
This example specifies that jobs have up to 5 dispatch turns to
reserve sufficient job slots or memory (equal to 5 minutes, by
default).
.SH Default

.sp 2
Not defined. No job slots or memory is reserved.
.sp 2

.ce 1000
\fBRES_REQ\fR
.ce 0

.sp 2
Specifies resource requirements used to determine eligible hosts.
.sp 2

.SH Syntax

.sp 2
\fRRES_REQ=\fR\fIres_req\fR
.SH Description

.sp 2
Specify a resource requirement string as usual. The resource
requirement string lets you specify conditions in a more flexible
manner than using the load thresholds. Resource requirement
strings can be simple (applying to the entire job), compound
(applying to the specified number of slots) or can contain
alternative resources (alternatives between 2 or more simple
and/or compound). For alternative resources, if the first
resource cannot be found that satisfies the first resource
requirement, then the next resource requirement is tried, and so
on until the requirement is satisfied.
.sp 2
Compound and alternative resource requirements follow the same
set of rules for determining how resource requirements are going
to be merged between job, application, and queue level. For more
information on merge rules, see Administering IBM Spectrum LSF.
.sp 2
When a compound or alternative resource requirement is set for a
queue, it will be ignored unless it is the only resource
requirement specified (no resource requirements are set at the
job-level or application-level).
.sp 2
When a simple resource requirement is set for a queue and a
compound resource requirement is set at the job-level or
application-level, the queue-level requirements merge as they do
for simple resource requirements. However, any job-based
resources defined in the queue only apply to the first term of
the merged compound resource requirements.
.sp 2
Resource requirement strings in select sections must conform to a
more strict syntax. The strict resource requirement syntax only
applies to the select section. It does not apply to the other
resource requirement sections (order, rusage, same, span, cu or
affinity). LSF rejects resource requirement strings where an
rusage section contains a non-consumable resource.
.sp 2
For simple resource requirements, the \fRselect\fR sections from
all levels must be satisfied and the same sections from all
levels are combined. cu, order, and span sections at the
job-level overwrite those at the application-level which
overwrite those at the queue-level. Multiple rusage definitions
are merged, with the job-level rusage taking precedence over the
application-level, and application-level taking precedence over
the queue-level.
.sp 2
The simple resource requirement \fRrusage\fR section can specify
additional requests. To do this, use the \fROR\fR (\fR||\fR)
operator to separate additional \fRrusage\fR strings. Multiple -R
options cannot be used with multi-phase rusage resource
requirements.
.sp 2
For simple resource requirements the job-level affinity section
overrides the application-level, and the application-level
affinity section overrides the queue-level.
.sp 2
\fBNote: \fR
.sp 2
Compound and alternative resource requirements do not support use
of the || operator within rusage sections or the cu section.
.sp 2
The \fBRES_REQ\fR consumable resource requirements must satisfy
any limits set by the parameter \fBRESRSV_LIMIT\fR in lsb.queues,
or the \fBRES_REQ\fR will be ignored.
.sp 2
When both the \fBRES_REQ\fR and \fBRESRSV_LIMIT\fR are set in
lsb.queues for a consumable resource, the queue-level
\fBRES_REQ\fR no longer acts as a hard limit for the merged
\fBRES_REQ\fR \fRrusage\fR values from the job and application
levels. In this case only the limits set by \fBRESRSV_LIMIT\fR
must be satisfied, and the queue-level RES_REQ acts as a default
value.
.sp 2
For example:
.sp 2
\fBQueue-level RES_REQ:\fR
.br
         RES_REQ=rusage[mem=200:lic=1] ...
.sp 2
         For the job submission:
.sp 2
         bsub -R\(aqrusage[mem=100]\(aq ...
.sp 2
         the resulting requirement for the job is
.sp 2
         rusage[mem=100:lic=1]
.sp 2
         where \fRmem=100\fR specified by the job overrides
         \fRmem=200\fR specified by the queue. However,
         \fRlic=1\fR from queue is kept, since job does not
         specify it.
.sp 2
\fBQueue-level RES_REQ threshold:\fR
.br
         RES_REQ = rusage[bwidth =2:threshold=5] ...
.sp 2
         For the job submission:
.sp 2
         bsub -R "rusage[bwidth =1:threshold=6]" ...
.sp 2
         the resulting requirement for the job is
.sp 2
         rusage[bwidth =1:threshold=6]
.sp 2
\fBQueue-level RES_REQ with decay and duration defined:\fR
.br
         RES_REQ=rusage[mem=200:duration=20:decay=1] ...
.sp 2
         For a job submission with no decay or duration:
.sp 2
         bsub -R\(aqrusage[mem=100]\(aq ...
.sp 2
         the resulting requirement for the job is:
.sp 2
         rusage[mem=100:duration=20:decay=1]
.sp 2
         Queue-level duration and decay are merged with the
         job-level specification, and \fRmem=100\fR for the job
         overrides \fRmem=200\fR specified by the queue. However,
         \fRduration=20\fR and \fRdecay=1\fR from queue are kept,
         since job does not specify them.
.sp 2
\fBQueue-level RES_REQ with resource reservation method:\fR
.br
         RES_REQ=rusage[mem=200/host:duration=20:decay=1] ...
.sp 2
         For a job submission with no decay or duration:
.sp 2
         bsub -R\(aqrusage[mem=100/task]\(aq ...
.br

.sp 2
         the resulting requirement for the job is:
.sp 2
         rusage[mem=100/task:duration=20:decay=1]
.br

.sp 2
\fBQueue-level RES_REQ with multi-phase job-level rusage:\fR
.br
         RES_REQ=rusage[mem=200:duration=20:decay=1] ...
.sp 2
         For a job submission with no decay or duration:
.sp 2
         bsub -R\(aqrusage[mem=(300 200 100):duration=(10 10 10)]\(aq ...
.sp 2
         the resulting requirement for the job is:
.sp 2
         rusage[mem=(300 200 100):duration=(10 10 10)]
.sp 2
         Multi-phase rusage values in the job submission override
         the single phase specified by the queue.
.sp 2
         *  If \fBRESRSV_LIMIT\fR is defined in lsb.queues and
            has a maximum memory limit of 300 MB or greater, this
            job will be accepted.
.sp 2
         *  If \fBRESRSV_LIMIT\fR is defined in lsb.queues and
            has a maximum memory limit of less than 300 MB, this
            job will be rejected.
.sp 2
         *  If \fBRESRSV_LIMIT\fR is not defined in lsb.queues
            and the queue-level \fBRES_REQ\fR value of 200 MB
            acts as a ceiling, this job will be rejected.
.sp 2
\fBQueue-level multi-phase rusage RES_REQ:\fR
.br
         RES_REQ=rusage[mem=(350 200):duration=(20):decay=(1)] ...
.sp 2
         For a single phase job submission with no decay or
         duration:
.sp 2
         bsub -q q_name -R\(aqrusage[mem=100:swap=150]\(aq ...
.sp 2
         the resulting requirement for the job is:
.sp 2
         rusage[mem=100:swap=150]
.sp 2
         The job-level rusage string overrides the queue-level
         multi-phase rusage string.
.sp 2
The \fRorder\fR section defined at the job level overwrites any
resource requirements specified at the application level or queue
level. The \fRorder\fR section defined at the application level
overwrites any resource requirements specified at the queue
level. The default \fRorder\fR string is \fRr15s:pg\fR.
.sp 2
If RES_REQ is defined at the queue level and there are no load
thresholds defined, the pending reasons for each individual load
index are not displayed by bjobs.
.sp 2
The \fRspan\fR section defined at the queue level is ignored if
the \fRspan\fR section is also defined at the job level or in an
application profile.
.sp 2
\fBNote: \fRDefine span[hosts=-1] in the application profile or
bsub -R resource requirement string to override the span section
setting in the queue.
.SH Default

.sp 2
select[type==local] order[r15s:pg]. If this parameter is defined
and a host model or Boolean resource is specified, the default
type is any.
.sp 2

.ce 1000
\fBRESRSV_LIMIT\fR
.ce 0

.sp 2
Sets a range of allowed values for \fBRES_REQ\fR resources.
.sp 2

.SH Syntax

.sp 2
\fRRESRSV_LIMIT=\fR[\fIres1\fR={\fImin1\fR,} \fImax1\fR]
[\fIres2\fR={\fImin2\fR,} \fImax2\fR]...
.sp 2
Where \fIres\fR is a consumable resource name, \fImin\fR is an
optional minimum value and \fImax\fR is the maximum allowed
value. Both \fImax\fR and \fImin\fR must be float numbers between
0 and 2147483647, and \fImin\fR cannot be greater than \fImax\fR.
.SH Description

.sp 2
Queue-level \fBRES_REQ\fR rusage values (set in lsb.queues) must
be in the range set by \fBRESRSV_LIMIT\fR, or the queue-level
\fBRES_REQ\fR is ignored. Merged \fBRES_REQ\fR \fRrusage\fR
values from the job and application levels must be in the range
of \fBRESRSV_LIMIT\fR, or the job is rejected.
.sp 2
Changes made to the rusage values of running jobs using bmod -R
cannot exceed the maximum values of \fBRESRSV_LIMIT\fR, but can
be lower than the minimum values.
.sp 2
When both the \fBRES_REQ\fR and \fBRESRSV_LIMIT\fR are set in
lsb.queues for a consumable resource, the queue-level
\fBRES_REQ\fR no longer acts as a hard limit for the merged
\fBRES_REQ\fR \fRrusage\fR values from the job and application
levels. In this case only the limits set by \fBRESRSV_LIMIT\fR
must be satisfied, and the queue-level RES_REQ acts as a default
value.
.sp 2
For MultiCluster, jobs must satisfy the \fBRESRSV_LIMIT\fR range
set for the send-jobs queue in the submission cluster. After the
job is forwarded the resource requirements are also checked
against the \fBRESRSV_LIMIT\fR range set for the receive-jobs
queue in the execution cluster.
.sp 2
\fBNote: \fR
.sp 2
Only consumable resource limits can be set in \fBRESRSV_LIMIT\fR.
Other resources will be ignored.
.SH Default

.sp 2
Not defined.
.sp 2
If \fImax\fR is defined and optional \fImin\fR is not, the
default for \fImin\fR is 0.
.sp 2

.ce 1000
\fBRESUME_COND\fR
.ce 0

.sp 2
Specifies conditions for LSF to automatically resume a suspended
(SSUSP) job in this queue.
.sp 2

.SH Syntax

.sp 2
\fRRESUME_COND=\fR\fIres_req\fR
.sp 2
Use the \fRselect\fR section of the resource requirement string
to specify load thresholds. All other sections are ignored.
.SH Description

.sp 2
LSF automatically resumes a suspended (SSUSP) job in this queue
if the load on the host satisfies the specified conditions. The
conditions only support load indices and static boolean
resources.
.sp 2
If RESUME_COND is not defined, then the loadSched thresholds are
used to control resuming of jobs. The loadSched thresholds are
ignored, when resuming jobs, if RESUME_COND is defined.
.SH Default

.sp 2
Not defined. The loadSched thresholds are used to control
resuming of jobs.
.sp 2

.ce 1000
\fBRUN_JOB_FACTOR\fR
.ce 0

.sp 2
Job slots weighting factor. Used only with fairshare scheduling.
.sp 2

.SH Syntax

.sp 2
\fRRUN_JOB_FACTOR=\fR\fInumber\fR
.SH Description

.sp 2
In the calculation of a user’s dynamic share priority, this
factor determines the relative importance of the number of job
slots reserved and in use by a user.
.sp 2
If undefined, the cluster-wide value from the lsb.params
parameter of the same name is used.
.SH Default

.sp 2
Not defined.
.sp 2

.ce 1000
\fBRUN_TIME_DECAY\fR
.ce 0

.sp 2
Enables decay for run time at the same rate as the decay set by
HIST_HOURS for cumulative CPU time and historical run time. Used
only with fairshare scheduling.
.sp 2

.SH Syntax

.sp 2
\fRRUN_TIME_DECAY=Y | y | N | n\fR
.SH Description

.sp 2
In the calculation of a user’s dynamic share priority, this
factor determines whether run time is decayed.
.sp 2
If undefined, the cluster-wide value from the lsb.params
parameter of the same name is used.
.SH Restrictions

.sp 2
Running badmin reconfig or restarting mbatchd during a job\(aqs run
time results in the decayed run time being recalculated.
.sp 2
When a suspended job using run time decay is resumed, the decay
time is based on the elapsed time.
.SH Default

.sp 2
Not defined
.sp 2

.ce 1000
\fBRUN_TIME_FACTOR\fR
.ce 0

.sp 2
Specifies the run time weighting factor. Used only with fairshare
scheduling.
.sp 2

.SH Syntax

.sp 2
\fRRUN_TIME_FACTOR=\fR\fInumber\fR
.SH Description

.sp 2
In the calculation of a user’s dynamic share priority, this
factor determines the relative importance of the total run time
of a user’s running jobs.
.sp 2
If undefined, the cluster-wide value from the lsb.params
parameter of the same name is used.
.SH Default

.sp 2
Not defined.
.sp 2

.ce 1000
\fBRUN_WINDOW\fR
.ce 0

.sp 2
Specifies time periods during which jobs in the queue are allowed
to run.
.sp 2

.SH Syntax

.sp 2
\fRRUN_WINDOW=\fR\fItime_window \fR...
.SH Description

.sp 2
When the window closes, LSF suspends jobs running in the queue
and stops dispatching jobs from the queue. When the window
reopens, LSF resumes the suspended jobs and begins dispatching
additional jobs.
.SH Default

.sp 2
Not defined. Queue is always active.
.sp 2

.ce 1000
\fBRUNLIMIT\fR
.ce 0

.sp 2
Specifies the maximum run limit and optionally the default run
limit. The name of a host or host model specifies the runtime
normalization host to use.
.sp 2

.SH Syntax

.sp 2
\fRRUNLIMIT=\fR[\fIdefault_limit\fR] \fImaximum_limit\fR
.sp 2
where \fIdefault_limit\fR and \fImaximum_limit\fR are:
.sp 2
[\fIhour\fR:]\fIminute\fR[/\fIhost_name\fR | /\fIhost_model\fR]
.SH Description

.sp 2
By default, jobs that are in a running state (but not in
pre-execution or post-execution) for longer than the specified
maximum run limit are killed by LSF. You can optionally provide
your own termination job action to override this default.
.sp 2
Jobs submitted with a job-level run limit (bsub -W) that is less
than the maximum run limit are killed when their job-level run
limit is reached. Jobs submitted with a run limit greater than
the maximum run limit are rejected by the queue.
.sp 2
If a default run limit is specified, jobs submitted to the queue
without a job-level run limit are killed when the default run
limit is reached. The default run limit is used with backfill
scheduling of parallel jobs.
.sp 2
\fBNote: \fR
.sp 2
If you want to provide an estimated run time for scheduling
purposes without killing jobs that exceed the estimate, define
the RUNTIME parameter in an application profile instead of a run
limit (see lsb.applications for details).
.sp 2
If you specify only one limit, it is the maximum, or hard, run
limit. If you specify two limits, the first one is the default,
or soft, run limit, and the second one is the maximum run limit.
.sp 2
The run limit is in the form of [hour:]minute. The minutes can be
specified as a number greater than 59. For example, three and a
half hours can either be specified as 3:30, or 210.
.sp 2
The run limit you specify is the normalized run time. This is
done so that the job does approximately the same amount of
processing, even if it is sent to host with a faster or slower
CPU. Whenever a normalized run time is given, the actual time on
the execution host is the specified time multiplied by the CPU
factor of the normalization host then divided by the CPU factor
of the execution host.
.sp 2
If ABS_RUNLIMIT=Y is defined in lsb.params, the runtime limit is
not normalized by the host CPU factor. Absolute wall-clock run
time is used for all jobs submitted to a queue with a run limit
configured.
.sp 2
Optionally, you can supply a host name or a host model name
defined in LSF. You must insert ‘\fR/\fR’ between the run limit
and the host name or model name. (See lsinfo(1) to get host model
information.)
.sp 2
If no host or host model is given, LSF uses the default runtime
normalization host defined at the queue level (DEFAULT_HOST_SPEC
in lsb.queues) if it has been configured; otherwise, LSF uses the
default CPU time normalization host defined at the cluster level
(DEFAULT_HOST_SPEC in lsb.params) if it has been configured;
otherwise, the host with the largest CPU factor (the fastest host
in the cluster).
.sp 2
For MultiCluster jobs, if no other CPU time normalization host is
defined and information about the submission host is not
available, LSF uses the host with the largest CPU factor (the
fastest host in the cluster).
.sp 2
Jobs submitted to a chunk job queue are not chunked if RUNLIMIT
is greater than 30 minutes.
.sp 2
RUNLIMIT is required for queues configured with
INTERRUPTIBLE_BACKFILL.
.SH Default

.sp 2
Unlimited
.sp 2

.ce 1000
\fBSLA_GUARANTEES_IGNORE\fR
.ce 0

.sp 2
Applies to SLA guarantees only. Allows jobs in the queue access
to all guaranteed resources.
.sp 2

.SH Syntax

.sp 2
\fRSLA_GUARANTEES_IGNORE=Y| y | N | n\fR
.SH Description

.sp 2
\fRSLA_GUARANTEES_IGNORE=Y\fR allows jobs in the queue access to
all guaranteed resources. As a result, some guarantees might not
be honored. If a queue does not have this parameter set, jobs in
this queue cannot trigger preemption of an SLA job. If an SLA job
is suspended (e.g. by a bstop), jobs in queues without the
parameter set can still make use of the slots released by the
suspended job.
.sp 2
\fBNote: \fRUsing SLA_GUARANTEES_IGNORE=Y defeats the purpose of
guaranteeing resources. This should be used sparingly for low
traffic queues only.
.SH Default

.sp 2
Not defined (N). The queue must honor resource guarantees when
dispatching jobs.
.sp 2

.ce 1000
\fBSLOT_POOL\fR
.ce 0

.sp 2
Specifies the name of the pool of job slots the queue belongs to
for queue-based fairshare.
.sp 2

.SH Syntax

.sp 2
\fRSLOT_POOL=\fR\fIpool_name\fR
.SH Description

.sp 2
\fBNote: \fRThis parameter is deprecated and might be removed in
a future version of LSF.
.sp 2
A queue can only belong to one pool. All queues in the pool must
share the same set of hosts.
.SH Valid values

.sp 2
Specify any ASCII string up to 60 characters long. You can use
letters, digits, underscores (_) or dashes (-). You cannot use
blank spaces.
.SH Default

.sp 2
Not defined. No job slots are reserved.
.sp 2

.ce 1000
\fBSLOT_RESERVE\fR
.ce 0

.sp 2
Enables processor reservation for the queue and specifies the
reservation time.
.sp 2

.SH Syntax

.sp 2
\fRSLOT_RESERVE=MAX_RESERVE_TIME[\fR\fIinteger\fR\fR]\fR
.SH Description

.sp 2
Specify the keyword MAX_RESERVE_TIME and, in square brackets, the
number of MBD_SLEEP_TIME cycles over which a job can reserve job
slots. MBD_SLEEP_TIME is defined in lsb.params; the default value
is 60 seconds.
.sp 2
If a job has not accumulated enough job slots to start before the
reservation expires, it releases all its reserved job slots so
that other jobs can run. Then, the job cannot reserve slots for
one scheduling session, so other jobs have a chance to be
dispatched. After one scheduling session, the job can reserve job
slots again for another period specified by SLOT_RESERVE.
.sp 2
SLOT_RESERVE is overridden by the RESOURCE_RESERVE parameter.
.sp 2
If both RESOURCE_RESERVE and SLOT_RESERVE are defined in the same
queue, job slot reservation and memory reservation are enabled
and an error is displayed when the cluster is reconfigured.
SLOT_RESERVE is ignored.
.sp 2
Job slot reservation for parallel jobs is enabled by
RESOURCE_RESERVE if the LSF scheduler plug-in module names for
both resource reservation and parallel batch jobs
(schmod_parallel and schmod_reserve) are configured in the
lsb.modules file: The schmod_parallel name \fImust\fR come before
schmod_reserve in lsb.modules.
.sp 2
If BACKFILL is configured in a queue, and a run limit is
specified at the job level (bsub -W), application level (RUNLIMIT
in lsb.applications), or queue level (RUNLIMIT in lsb.queues), or
if an estimated run time is specified at the application level
(RUNTIME in lsb.applications), backfill parallel jobs can use job
slots reserved by the other jobs, as long as the backfill job can
finish before the predicted start time of the jobs with the
reservation.
.sp 2
Unlike memory reservation, which applies both to sequential and
parallel jobs, slot reservation applies only to parallel jobs.
.SH Example

.sp 2
SLOT_RESERVE=MAX_RESERVE_TIME[5]
.br

.sp 2
This example specifies that parallel jobs have up to 5 cycles of
MBD_SLEEP_TIME (5 minutes, by default) to reserve sufficient job
slots to start.
.SH Default

.sp 2
Not defined. No job slots are reserved.
.sp 2

.ce 1000
\fBSLOT_SHARE\fR
.ce 0

.sp 2
Specifies the share of job slots for queue-based fairshare.
.sp 2

.SH Syntax

.sp 2
\fRSLOT_SHARE=\fR\fIinteger\fR
.SH Description

.sp 2
\fBNote: \fRThis parameter is deprecated and might be removed in
a future version of LSF.
.sp 2
Represents the percentage of running jobs (job slots) in use from
the queue. SLOT_SHARE must be greater than zero (0) and less than
or equal to 100.
.sp 2
The sum of SLOT_SHARE for all queues in the pool does not need to
be 100%. It can be more or less, depending on your needs.
.SH Default

.sp 2
Not defined
.sp 2

.ce 1000
\fBSNDJOBS_TO\fR
.ce 0

.sp 2
Defines a send-jobs queue for IBM Spectrum LSF multicluster
capability.
.sp 2

.SH Syntax

.sp 2
\fBSNDJOBS_TO=\fR\fR[queue@]cluster_name[+preference] ...\fR
.SH Description

.sp 2
Specify remote queue names, in the form
\fRqueue_name@cluster_name[+preference]\fR, separated by a space.
.sp 2
This parameter is ignored if lsb.queues \fBHOSTS\fR specifies
remote (borrowed) resources.
.sp 2
Queue preference is defined at the queue level in
\fBSNDJOBS_TO\fR of the submission cluster for each corresponding
execution cluster queue receiving forwarded jobs.
.sp 2
You can enable an LSF data manager data transfer queue as a
remote send-jobs queue in the execution cluster.
.sp 2
If you specify a remote queue with the \fBSNDJOBS_TO\fR parameter
in the data transfer queue in the execution cluster, the path of
the \fBFILE_TRANSFER_CMD\fR parameter must exist in the
submission cluster. In addition, the corresponding remote queue
in the submission cluster must be configured to receive the data
transfer job (that is, the \fBRCVJOBS_FROM\fR parameter value for
the remote queue in the submission cluster either includes this
execution cluster, or is set to \fRallclusters\fR). This ensures
that the submission cluster can push data files back to the
execution cluster.
.SH Example

.sp 2
SNDJOBS_TO=queue2@cluster2+1 queue3@cluster2+2
.br

.SH Example for LSF data managerLSF data managerdata manager

.sp 2
The following configuration on the execution cluster
\fRclusterE\fR has the LSF data manager transfer queue
\fRdata_transfer\fR set as a remote send-jobs queue to the
\fRreceive_q\fR queue in the submission cluster \fRclusterS\fR:
.sp 2
Begin Queue
.br
QUEUE_NAME=data_transfer
.br
DATA_TRANSFER=Y
.br
SNDJOBS_TO=receive_q@clusterS
.br
HOSTS=hostS1 hostS2 # Transfer nodes in the execution cluster
.br
End Queue
.sp 2
You must also define the \fBRCVJOBS_FROM\fR parameter for the
\fRreceive_q\fR queue in the submission cluster \fRclusterS\fR to
accept jobs from (and push data to) the execution cluster
\fRclusterE\fR, as shown in the following configuration in the
\fRclusterS\fR cluster:
.sp 2
Begin Queue
.br
QUEUE_NAME=receive_q
.br
RCVJOBS_FROM=clusterE
.br
PRIORITY=40
.br
HOSTS=hostS1 hostS2 # Transfer nodes in the submission cluster
.br
RES_REQ=select[type==any]
.br
End Queue
.sp 2
Alternatively, you can define \fRRCVJOBS_FROM=allclusters\fR to
accept jobs from all clusters, which includes the execution
cluster.
.sp 2

.ce 1000
\fBSTACKLIMIT\fR
.ce 0

.sp 2
Specifies the per-process stack segment size limit for all job
processes from this queue.
.sp 2

.SH Syntax

.sp 2
\fRSTACKLIMIT=\fR\fIinteger\fR
.SH Description

.sp 2
Specify this parameter to place a per-process hard stack segment
size limit, in KB, for all of the processes belonging to a job
from this queue (see getrlimit(2)).
.SH Default

.sp 2
Unlimited
.sp 2

.ce 1000
\fBSTOP_COND\fR
.ce 0

.sp 2
Specifies conditions for LSF to automatically suspend a running
job in this queue.
.sp 2

.SH Syntax

.sp 2
\fRSTOP_COND=\fR\fIres_req\fR
.sp 2
Use the \fRselect\fR section of the resource requirement string
to specify load thresholds. All other sections are ignored.
.SH Description

.sp 2
LSF automatically suspends a running job in this queue if the
load on the host satisfies the specified conditions. The
conditions only support load indices and static boolean
resources.
.sp 2
*  LSF does not suspend the only job running on the host if the
   machine is interactively idle (\fRit\fR > 0).
.sp 2
*  LSF does not suspend a forced job (brun -f).
.sp 2
*  LSF does not suspend a job because of paging rate if the
   machine is interactively idle.
.sp 2
If STOP_COND is specified in the queue and there are no load
thresholds, the suspending reasons for each individual load index
is not displayed by bjobs.
.SH Example

.sp 2
STOP_COND= select[((!cs && it < 5) || (cs && mem < 15 && swp < 50))]
.br

.sp 2
In this example, assume “cs” is a Boolean resource indicating
that the host is a computer server. The stop condition for jobs
running on computer servers is based on the availability of swap
memory. The stop condition for jobs running on other kinds of
hosts is based on the idle time.
.sp 2

.ce 1000
\fBSUCCESS_EXIT_VALUES\fR
.ce 0

.sp 2
Specifies exit values used by LSF to determine if the job was
done successfully.
.sp 2

.SH Syntax

.sp 2
\fRSUCCESS_EXIT_VALUES=\fR[exit_code ...]
.SH Description

.sp 2
Application level success exit values defined with
\fBSUCCESS_EXIT_VALUES\fR in lsb.applications override the
configuration defined in lsb.queues. Job-level success exit
values specified with the \fBLSB_SUCCESS_EXIT_VALUES\fR
environment variable override the configration in lsb.queues and
lsb.applications.
.sp 2
Use \fBSUCCESS_EXIT_VALUES\fR for submitting jobs to specific
queues that successfully exit with non-zero values so that LSF
does not interpret non-zero exit codes as job failure.
.sp 2
If the same exit code is defined in \fBSUCCESS_EXIT_VALUES\fR and
\fBREQUEUE_EXIT_VALUES\fR, any job with this exit code is
requeued instead of being marked as DONE because sbatchd
processes requeue exit values before success exit values.
.sp 2
In MultiCluster job forwarding mode, LSF uses the
\fBSUCCESS_EXIT_VALUES\fR from the remote cluster.
.sp 2
In a MultiCluster resource leasing environment, LSF uses the
\fBSUCCESS_EXIT_VALUES\fR from the consumer cluster.
.sp 2
\fBexit_code\fR should be a value between 0 and 255. Use spaces
to separate multiple exit codes.
.sp 2
Any changes you make to \fBSUCCESS_EXIT_VALUES\fR will not affect
running jobs. Only pending jobs will use the new
\fBSUCCESS_EXIT_VALUES\fR definitions, even if you run badmin
reconfig and mbatchd restart to apply your changes.
.SH Default

.sp 2
Not defined.
.sp 2

.ce 1000
\fBSWAPLIMIT\fR
.ce 0

.sp 2
Specifies the amount of total virtual memory limit, in KB, for a
job from this queue.
.sp 2

.SH Syntax

.sp 2
\fRSWAPLIMIT=\fR\fIinteger\fR
.SH Description

.sp 2
This limit applies to the whole job, no matter how many processes
the job may contain.
.sp 2
The action taken when a job exceeds its SWAPLIMIT or PROCESSLIMIT
is to send SIGQUIT, SIGINT, SIGTERM, and SIGKILL in sequence. For
CPULIMIT, SIGXCPU is sent before SIGINT, SIGTERM, and SIGKILL.
.SH Default

.sp 2
Unlimited
.sp 2

.ce 1000
\fBTASKLIMIT\fR
.ce 0

.sp 2
Specifies the maximum number of tasks that can be allocated to a
job. For parallel jobs, the maximum number of tasks that can be
allocated to the job.
.sp 2

.SH Syntax

.sp 2
\fRTASKLIMIT=\fR[\fIminimum_limit\fR [\fIdefault_limit\fR]]
\fImaximum_limit\fR
.SH Description

.sp 2
\fBNote: \fR\fBTASKLIMIT\fR replaces \fBPROCLIMIT\fR as of LSF
9.1.3.
.sp 2
Queue level \fBTASKLIMIT\fR has the highest priority over
application level \fBTASKLIMIT\fR and job level \fBTASKLIMIT\fR.
Application level \fBTASKLIMIT\fR has higher priority than job
level \fBTASKLIMIT\fR. Job-level limits must fall within the
maximum and minimum limits of the application profile and the
queue.
.sp 2
\fBNote: \fRIf you also defined \fBJOB_SIZE_LIST\fR in the same
queue where you defined \fBTASKLIMIT\fR, the \fBTASKLIMIT\fR
parameter is ignored.
.sp 2
Optionally specifies the minimum and default number of job tasks.
.sp 2
All limits must be positive numbers greater than or equal to 1
that satisfy the following relationship:
.sp 2
1 <= \fIminimum\fR <= \fIdefault\fR <= \fImaximum\fR
.sp 2
If \fBRES_REQ\fR in a queue was defined as a compound resource
requirement with a block size \fR(span[block=value])\fR, the
default value for \fBTASKLIMIT\fR should be a multiple of a
block.
.sp 2
For example, this configuration would be accepted:
.sp 2
Queue-level\fR RES_REQ="1*{type==any } + {type==local
span[block=4]}"\fR
.sp 2
\fRTASKLIMIT = 5 9 13\fR
.sp 2
This configuration, for example, would not be accepted. An error
message will appear when doing badmin reconfig:
.sp 2
Queue-level \fRRES_REQ="1*{type==any } + {type==local
span[block=4]}"\fR
.sp 2
\fRTASKCLIMIT = 4 10 12\fR
.sp 2
In the MultiCluster job forwarding model, the local cluster
considers the receiving queue\(aqs \fBTASKLIMIT\fR on remote
clusters before forwarding jobs. If the receiving queue\(aqs
\fBTASKLIMIT\fR definition in the remote cluster cannot satisfy
the job\(aqs task requirements for a remote queue, the job is not
forwarded to that remote queue in the cluster.
.SH Default

.sp 2
Unlimited, the default number of tasks is 1
.sp 2

.ce 1000
\fBTERMINATE_WHEN\fR
.ce 0

.sp 2
Specifies the circumstances under which the queue invokes the
TERMINATE action instead of the SUSPEND action.
.sp 2

.SH Syntax

.sp 2
\fRTERMINATE_WHEN=\fR[\fRLOAD\fR] [\fRPREEMPT\fR] [\fRWINDOW\fR]
.SH Description

.sp 2
Configures the queue to invoke the TERMINATE action instead of
the SUSPEND action in the specified circumstance:
.sp 2
*  LOAD: kills jobs when the load exceeds the suspending
   thresholds.
.sp 2
*  PREEMPT: kills jobs that are being preempted.
.sp 2
*  WINDOW: kills jobs if the run window closes.
.sp 2
If the TERMINATE_WHEN job control action is applied to a chunk
job, sbatchd kills the chunk job element that is running and puts
the rest of the waiting elements into pending state to be
rescheduled later.
.SH Example

.sp 2
Set TERMINATE_WHEN to WINDOW to define a night queue that kills
jobs if the run window closes:
.sp 2
Begin Queue
.br
NAME           = night
.br
RUN_WINDOW     = 20:00-08:00 EDT
.br
TERMINATE_WHEN = WINDOW
.br
JOB_CONTROLS   = TERMINATE[kill -KILL $LS_JOBPGIDS; mail - s "job $LSB_JOBID 
.br
                 killed by queue run window" $USER < /dev/null]
.br
End Queue
.sp 2
Specifying the time zone is optional. If you do not specify a
time zone, LSF uses the local system time zone.
.sp 2

.ce 1000
\fBTHREADLIMIT\fR
.ce 0

.sp 2
Limits the number of concurrent threads that can be part of a
job. Exceeding the limit causes the job to terminate.
.sp 2

.SH Syntax

.sp 2
\fRTHREADLIMIT=\fR[\fIdefault_limit\fR] \fImaximum_limit\fR
.SH Description

.sp 2
The system sends the following signals in sequence to all
processes belongs to the job: SIGINT, SIGTERM, and SIGKILL.
.sp 2
By default, if a default thread limit is specified, jobs
submitted to the queue without a job-level thread limit are
killed when the default thread limit is reached.
.sp 2
If you specify only one limit, it is the maximum, or hard, thread
limit. If you specify two limits, the first one is the default,
or soft, thread limit, and the second one is the maximum thread
limit.
.sp 2
Both the default and the maximum limits must be positive
integers. The default limit must be less than the maximum limit.
The default limit is ignored if it is greater than the maximum
limit.
.SH Examples

.sp 2
THREADLIMIT=6
.br

.sp 2
No default thread limit is specified. The value 6 is the default
and maximum thread limit.
.sp 2
THREADLIMIT=6 8
.br

.sp 2
The first value (6) is the default thread limit. The second value
(8) is the maximum thread limit.
.SH Default

.sp 2
Unlimited
.sp 2

.ce 1000
\fBUJOB_LIMIT\fR
.ce 0

.sp 2
Specifies the per-user job slot limit for the queue.
.sp 2

.SH Syntax

.sp 2
\fRUJOB_LIMIT=\fR\fIinteger\fR
.SH Description

.sp 2
This parameter specifies the maximum number of job slots that
each user can use in this queue.
.sp 2
\fBUJOB_LIMIT\fR must be within or greater than the range set by
\fBTASKLIMIT\fR or bsub -n (if either is used), or jobs are
rejected.
.SH Default

.sp 2
Unlimited
.sp 2

.ce 1000
\fBUSE_PAM_CREDS\fR
.ce 0

.sp 2
Applies PAM limits to this queue.
.sp 2

.SH Syntax

.sp 2
\fRUSE_PAM_CREDS=y\fR | \fRn\fR | [\fRlimits\fR] [\fRsession\fR]
.SH Description

.sp 2
\fBUSE_PAM_CREDS\fR is only supported on Linux systems. If the
execution host does not have PAM configured and this parameter is
enabled, the job fails.
.sp 2
If \fBUSE_PAM_CREDS\fR is set to \fBy\fR or \fBlimits\fR, LSF can
apply PAM limits to a queue when its job is dispatched to a Linux
host using PAM. The LSF job does not run within the PAM session.
.sp 2
If \fBUSE_PAM_CREDS\fR is set to \fBsession\fR:
.sp 2
*  If a job is started on the first execution host, the job RES
   opens a PAM session for the user and forks a RES process
   within that session. This RES process becomes the user\(aqs job.
.sp 2
*  If a task is launched by the blaunch command or an API, the
   task RES opens a PAM session for the user and executes a RES
   process within that session. This RES process becomes the
   user\(aqs task.
.sp 2
The \fBlimits\fR keyword can be defined together with the
\fBsession\fR keyword.
.sp 2
If LSF limits are more restrictive than PAM limits, LSF limits
are used, otherwise PAM limits are used. PAM limits are system
resource limits defined in the limits.conf file.
.sp 2
For parallel jobs, PAM sessions are only launched on the first
execution host if \fBUSE_PAM_CREDS=y\fR or
\fBUSE_PAM_CREDS=limits\fR is defined. PAM sessions are launched
on all tasks if \fBUSE_PAM_CREDS=session\fR or
\fBUSE_PAM_CREDS=limits session\fR is defined.
.sp 2
\fBNote: \fRWhen configuring Linux PAM to be used with LSF, you
must configure Linux PAM so that it does not ask users for their
passwords because jobs are not usually interactive.
.sp 2
Depending on the \fBUSE_PAM_CREDS\fR parameter setting, LSF
assumes that the following Linux PAM services are created:
.sp 2
*  If \fBUSE_PAM_CREDS\fR is set to \fRy\fR or \fRlimits\fR, LSF
   assumes that the Linux PAM service "lsf" is created.
.sp 2
*  If \fBUSE_PAM_CREDS\fR is set to \fRsession\fR, LSF assumes
   that the Linux PAM service "lsf-\fR\fI<clustername>\fR\fR" is
   created.
.sp 2
*  If \fBUSE_PAM_CREDS\fR is set to \fRlimits session\fR, LSF
   assumes that the Linux PAM services "lsf" and
   "lsf-\fR\fI<clustername>\fR\fR" are created.
.sp 2
It is also assumed that the "lsf" service is used in conjunction
with the /etc/security/limits.conf file.
.sp 2
The job sbatchd daemon checks the lsf service, and the job or
task RES daemon checks the lsf-\fR\fI<clustername>\fR\fR service.
.sp 2
Overrides \fBMEMLIMIT_TYPE=Process\fR.
.sp 2
Overridden (for CPU limit only) by \fBLSB_JOB_CPULIMIT=y\fR.
.sp 2
Overridden (for memory limits only) by \fBLSB_JOB_MEMLIMIT=y\fR.
.sp 2
The \fBUSE_PAM_CREDS\fR value in lsb.applications overrides the
\fBUSE_PAM_CREDS\fR value in lsb.queues.
.SH Default

.sp 2
n. \fBUSE_PAM_CREDS\fR is disabled.
.sp 2

.ce 1000
\fBUSE_PRIORITY_IN_POOL\fR
.ce 0

.sp 2
Queue-based fairshare only. After job scheduling occurs for each
queue, this parameter enables LSF to dispatch jobs to any
remaining slots in the pool in first-come first-served order
across queues.
.sp 2

.SH Syntax

.sp 2
\fRUSE_PRIORITY_IN_POOL= y | Y | n | N\fR
.sp 2
\fBNote: \fRThis parameter is deprecated and might be removed in
a future version of LSF.
.SH Default

.sp 2
N
.sp 2

.ce 1000
\fBUSERS\fR
.ce 0

.sp 2
Specifies a space-separated list of user names or user groups
that can submit jobs to the queue.
.sp 2

.SH Syntax

.sp 2
\fRUSERS=all\fR [\fR~\fR\fIuser_name\fR ...]
[\fR~\fR\fIuser_group\fR ...] | [\fIuser_name\fR ...]
[\fIuser_group\fR [\fR~\fR\fIuser_group\fR ...] ...]
.SH Description

.sp 2
LSF cluster administrators are automatically included in the list
of users. LSF cluster administrators can submit jobs to this
queue, or switch (bswitch) any user’s jobs into this queue.
.sp 2
If user groups are specified, each user in the group can submit
jobs to this queue. If FAIRSHARE is also defined in this queue,
only users defined by both parameters can submit jobs, so LSF
administrators cannot use the queue if they are not included in
the share assignments.
.sp 2
User names must be valid login names. To specify a Windows user
account, include the domain name in uppercase letters
(\fIDOMAIN_NAME\fR\\\fIuser_name\fR).
.sp 2
User group names can be LSF user groups or UNIX and Windows user
groups. To specify a Windows user group, include the domain name
in uppercase letters (\fIDOMAIN_NAME\fR\\\fIuser_group\fR).
.sp 2
Use the keyword all to specify all users or user groups in a
cluster.
.sp 2
Use the not operator (~) to exclude users from the all
specification or from user groups. This is useful if you have a
large number of users but only want to exclude a few users or
groups from the queue definition.
.sp 2
The not operator (~) can only be used with the all keyword or to
exclude users from user groups.
.sp 2
\fBCAUTION: \fRThe not operator (~) does not exclude LSF
administrators from the queue definition.
.SH Default

.sp 2
all (all users can submit jobs to the queue)
.SH Examples

.sp 2
*  \fRUSERS=user1 user2\fR
.sp 2
*  \fRUSERS=all ~user1 ~user2\fR
.sp 2
*  \fRUSERS=all ~ugroup1\fR
.sp 2
*  \fRUSERS=groupA ~user3 ~user4\fR
.sp 2

.ce 1000
\fBAutomatic time-based configuration\fR
.ce 0

.sp 2
Variable configuration is used to automatically change LSF
configuration based on time windows.
.sp 2
You define automatic configuration changes in lsb.queues by using
if-else constructs and time expressions. After you change the
files, reconfigure the cluster with the badmin reconfig command.
.sp 2
The expressions are evaluated by LSF every 10 minutes based on
mbatchd start time. When an expression evaluates true, LSF
dynamically changes the configuration based on the associated
configuration statements. Reconfiguration is done in real time
without restarting mbatchd, providing continuous system
availability.
.SH Example

.sp 2
Begin Queue
.br
 ... 
.br
#if time(8:30-18:30 EDT)   
.br
INTERACTIVE  = ONLY  # interactive only during day shift #endif
.br
#endif
.br
 ... 
.br
End Queue
.sp 2
Specifying the time zone is optional. If you do not specify a
time zone, LSF uses the local system time zone. LSF supports all
standard time zone abbreviations.